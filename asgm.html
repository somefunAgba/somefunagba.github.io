<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>AutoSGM: Unifying Momentum Methods for Better Learning | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Signal processing meets deep learning optimization." /> <meta property="og:description" content="Signal processing meets deep learning optimization." /> <link rel="canonical" href="http://localhost:4000/asgm.html" /> <meta property="og:url" content="http://localhost:4000/asgm.html" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"Signal processing meets deep learning optimization.","headline":"AutoSGM: Unifying Momentum Methods for Better Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm.html"},"url":"http://localhost:4000/asgm.html"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing meets deep learning optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods for Better Learning</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> ¬© 2025. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="autosgm"> <a href="#autosgm" class="anchor-heading" aria-labelledby="autosgm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> AutoSGM </h1> <p class="fs-6 fw-300">Connecting the dots ‚Ä¶ HB, NAG, Adam.</p><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#autosgm" id="markdown-toc-autosgm">AutoSGM</a> <ol> <li><a href="#-the-core-update-rule" id="markdown-toc--the-core-update-rule">üîç The Core Update Rule</a></li> <li><a href="#-optimal-learning-rate" id="markdown-toc--optimal-learning-rate">üìê Optimal Learning Rate</a> <ol> <li><a href="#practical-approximation" id="markdown-toc-practical-approximation">Practical Approximation</a></li> <li><a href="#ema-realizations" id="markdown-toc-ema-realizations">EMA Realizations</a></li> <li><a href="#partial-correlation-noise-robust-variant" id="markdown-toc-partial-correlation-noise-robust-variant">Partial-correlation: Noise-robust variant</a> <ol> <li><a href="#1-input-clipping" id="markdown-toc-1-input-clipping">1. Input Clipping</a></li> <li><a href="#2-output-clipping-and-max-normalization" id="markdown-toc-2-output-clipping-and-max-normalization">2. Output Clipping and Max-Normalization</a></li> <li><a href="#3-layer-wise-smoothing" id="markdown-toc-3-layer-wise-smoothing">3. Layer-wise smoothing</a></li> </ol> </li> <li><a href="#partial-correlation-relaxed-upper-bound-variant" id="markdown-toc-partial-correlation-relaxed-upper-bound-variant">Partial-correlation: Relaxed Upper-bound variant</a></li> </ol> </li> <li><a href="#-unifying-phb-nag-and-adam" id="markdown-toc--unifying-phb-nag-and-adam">üß© Unifying PHB, NAG, and Adam</a></li> <li><a href="#-lowpass-regularization" id="markdown-toc--lowpass-regularization">üéØ Lowpass Regularization</a></li> <li><a href="#-key-empirical-findings" id="markdown-toc--key-empirical-findings">üìä Key Empirical Findings</a></li> <li><a href="#-conclusion" id="markdown-toc--conclusion">üèÅ Conclusion</a></li> </ol> </li> </ol> </details><hr /> <p>Momentum-based stochastic gradient methods such as <strong>Polyak‚Äôs Heavy Ball (PHB)</strong>, <strong>Nesterov‚Äôs Accelerated Gradient (NAG)</strong>, and <strong>Adam</strong> dominate deep learning optimization.</p> <p>They are often treated as separate algorithms, but in our recent work, we show they are all <strong>special cases</strong> of a single signal-processing (DSP) structure: the <strong>Automatic Stochastic Gradient Method (AutoSGM)</strong>.</p> <p>AutoSGM reframes these optimizers through the lens of a <strong>first-order lowpass filter</strong> applied to the stochastic gradient, revealing:</p> <ul> <li>A principled way to <strong>tune momentum</strong> via the filter‚Äôs pole and zero.</li> <li>An <strong>optimal, iteration-dependent learning rate</strong> that Adam approximates.</li> <li>Momentum as a natural form of <strong>lowpass regularization</strong> that smooths the loss surface.</li> </ul> <blockquote> <p><em>All algebraic operations are sample-by-sample (<strong>elementwise</strong>) unless otherwise stated.</em> The shorthand notation \((t,i)\) denotes the \(i\)-th element of a vector at iteration \(t\).</p> </blockquote><hr /> <h2 id="-the-core-update-rule"> <a href="#-the-core-update-rule" class="anchor-heading" aria-labelledby="-the-core-update-rule"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üîç The Core Update Rule </h2> <p>The classic stochastic gradient method (SGM) updates parameters as:</p> \[\mathbf{w}(t+1,i) = \mathbf{w}(t,i) - \alpha(t,i) \, \mathbf{g}(t,i)\] <p>where:</p> <ul> <li>\(\mathbf{g}(t,i) = \nabla f(\mathbf{w}(t,i))\) is an unbiased stochastic gradient component,</li> <li>\(\alpha(t,i)\) denotes the learning rate at iteration \(t\), determined via a selected oracle function.</li> </ul> <p>In <strong>AutoSGM</strong>, we replace the stochastic gradient with a <strong>smoothed</strong> version:</p> \[\mathbf{w}(t+1,i) = \mathbf{w}(t,i) - \alpha(t,i) \, H_{\beta,\gamma}(\mathbf{g}(t,i))\] <p>Here, \(H_{\beta,\gamma}\) is a <strong>first-order filter</strong> with transfer function:</p> \[H(z) = \eta \, \frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}}, \quad 0 &lt; \beta &lt; 1, \ \gamma &lt; \beta\] <p>The time-domain realization is:</p> \[\mathbf{v}(t,i) = \beta\,\mathbf{v}(t-1,i) + \eta\,(\mathbf{g}(t,i) - \gamma\,\mathbf{g}(t-1,i))\]<hr /> <h2 id="-optimal-learning-rate"> <a href="#-optimal-learning-rate" class="anchor-heading" aria-labelledby="-optimal-learning-rate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üìê Optimal Learning Rate </h2> <p>We assume that both the objective function and its gradient are Lipschitz continuous <a class="citation" href="#bottouOptimizationMethodsLargescale2018">(Bottou et al., 2018)</a>. Let \(\mathbb{E}\) denote expectation over the randomness in the data samples. For a log-likelihood objective function, by the score-function identity, the expected gradient is zero for all parameter values, not only at the optimum <a class="citation" href="#moonMathematicalMethodsAlgorithms2000">(Moon &amp; Stirling, 2000; Van Trees et al., 2013)</a>, and so \(\mathbb{E}[\mathbf{g}(t,i)] = \mathbf{0}\).</p> <p>By minimizing the expected squared error \(\mathbb{E}[\mathbf{e}(t,i)^2]\), at iteration \(t\), we derive an <strong>iteration-dependent optimal learning rate</strong></p> \[\alpha(t,i)^\star = \frac{\mathbb{E}[\mathbf{w}(t,i) \,\mathbf{g}(t,i)]}{\mathbb{E}[\mathbf{g}(t,i)^2]},\] <p>where \(\mathbf{e}(t,i) = \mathbf{w}(t,i) - {\mathbf{w}(i)}^\star\) is the parameter error vector, the gap between current weights and a local optimum.</p><hr /> <h3 id="practical-approximation"> <a href="#practical-approximation" class="anchor-heading" aria-labelledby="practical-approximation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Approximation </h3> <p>In real training:</p> <ol> <li>Expectations are estimated with <strong>exponential moving averages (EMA)</strong>.</li> <li>For numerical stability, we use the normalized gradient form.</li> <li>As a safety margin, the iteration-dependent locally-optimal learning rate estimate is modulated with a small \(\mu\digamma(t)\) which acts as a trust-region against the effect of noisy gradient scales.</li> </ol> <p>Let \(0 \le \mu\digamma(t) \le 1\), where \(\mu &gt; 0\), \(0\le \digamma(t) \le 1\) is a learning-rate schedule. Define</p> \[\bar{\mathbf{g}}(t,i) = \frac{\mathbf{g}(t,i)}{\sqrt{\mathbb{E}[\mathbf{g}(t,i)^2]}},\] <p>where \(\bar{\mathbf{g}}(t,i)\) is the normalized gradient scaled to its unit root-mean-square (RMS) value. The learning rate becomes</p> \[\alpha(t,i) = \mu \digamma(t)\, \frac{\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)]}{\sqrt{\mathbb{E}[\mathbf{g}(t,i)^2]}}.\]<hr /> <h3 id="ema-realizations"> <a href="#ema-realizations" class="anchor-heading" aria-labelledby="ema-realizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> EMA Realizations </h3> <p>Track the second moment of the gradient:</p> \[\mathbf{b}(t,i) = \beta_b \,\mathbf{b}(t-1,i) + \mu (1 - \beta_b) \,\mathbf{g}(t,i)^2\] <p>‚Üí moment estimation.</p> <p>Define the RMS-normalizer:</p> \[\mathbf{d}(t,i) = \sqrt{\frac{\mathbf{b}(t,i)}{1 - \beta_b^t}} + \epsilon\] <p>‚Üí bias-corrected RMS with small \(\epsilon\) to prevent division by zero.</p> <p>Track the numerator term:</p> \[\mathbf{a}(t,i) = \beta_a \,\mathbf{a}(t-1,i) + \mu \,\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)\] <p>‚Üí running estimate of the weight‚Äìgradient correlation.</p> <p>Finally:</p> \[\alpha(t,i) = \digamma(t)\,\frac{\mathbf{a}(t,i)}{\mathbf{d}(t,i)}.\] <blockquote> <p><strong>Note:</strong> This reduces to <strong>Adam</strong> when \(\mathbf{a}(t,i)\) is replaced by a fixed constant \(\mu\).</p> </blockquote><hr /> <h3 id="partial-correlation-noise-robust-variant"> <a href="#partial-correlation-noise-robust-variant" class="anchor-heading" aria-labelledby="partial-correlation-noise-robust-variant"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Partial-correlation: Noise-robust variant </h3> <p>The learning-rate‚Äôs numerator is a partial-correlation estimate and is prone to heavy‚Äëtailed stochastic weight-gradient noisy correlations, sign flips and occasional magnitude spikes.</p> <p>As an empirical safeguard, before dividing the numerator by the denominator, we want to ensure that the numerator remains positive, well‚Äëbounded, and free from overshoot due to noisy, heavy-tailed gradient statistics. In other words, we want to limit rare spikes in the instantaneous partial correlation term without distorting the bulk of the signal.</p> \[\mathbf{u}(t,i) = \mathbf{w}(t,i) \, \bar{\mathbf{g}}(t,i)\]<hr /> <h4 id="1-input-clipping"> <a href="#1-input-clipping" class="anchor-heading" aria-labelledby="1-input-clipping"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Input Clipping </h4> <p>Heavy-tailed gradient noise statistics induce erroneous spikes that can dominate the EMA‚Äôs estimate over many iterations and increase its bias from the true mean estimate. Since, we do not know the probability distribution, <strong>Markov‚Äôs inequality</strong> gives a rationale for how often large such values can occur. The inequality</p> \[\mathbb{Pr}[|u| \ge c\,\mathbb{E}[|u|] ] \le \frac{1}{c},\] <p>relates how large the magnitude of \(u\) can be relative to its expected magnitude. <strong>Huberisation</strong> <a class="citation" href="#Menon2020GradientClipping">(Menon et al., 2020)</a> is a practical way to robustly mitigate such heavy-tailed values. The Huber clipping function bounds the most extreme outliers (\(c \times\) the expected scale) before they enter the EMA, while avoiding dead zones and allowing moderate estimates to pass through untouched relative to its expected scale.</p> \[\psi_{c}(u) = \begin{cases} u, &amp; |u| \le c\,\mathbb{E}[|u|] \\ \mathrm{sign}(u) \cdot c\,\mathbb{E}[|u|], &amp; |u| &gt; c\,\mathbb{E}[|u|], \end{cases}\] <p>where \(u\) is the input, and typically \(c\) is the scale multiplier used to clip only extreme outliers (\(c\) times) relative to the expected scale. For example, \(c=4\) corresponds to the prior that the probability \(p\) of the magnitude being at least \(4\) times its mean is never more than \(25\%\), and so the probability of the magnitude being less than \(4\) times its mean is at least \(1-p=75\%\) which represent the quantiles of the distribution. This allows us to have a robust EMA estimator less sensitive to the extreme values induced by heavy-tailed noise.</p> <p>For the instantaneous partial correlation \(\mathbf{u}(t,i)\), we can adaptively estimate the expected scale, via the EMA estimate</p> \[\hat{\mathbf{u}}(t,i) = \beta_a \, \hat{\mathbf{u}}(t-1,i) + (1 - \beta_a)\,|{\mathbf{u}}(t,i)|,\] <p>where \(\hat{\mathbf{u}}(t,i)\) adapts to the typical scale of \(\mathbf{u}(t,i)\) in each layer.</p><hr /> <h4 id="2-output-clipping-and-max-normalization"> <a href="#2-output-clipping-and-max-normalization" class="anchor-heading" aria-labelledby="2-output-clipping-and-max-normalization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Output Clipping and Max-Normalization </h4> <p>Numerically, we want to ensure the estimate for the partial-correlation numerator stays within a predictable, and reasonable range, while ensuring \(\alpha(t,i) \ge 0\).</p> <p>From the inequality \(0 \le (\mathbf{w}(t,i)-\bar{\mathbf{g}}(t,i))^2\), we have that \(\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) \ \le\ \frac{1}{2}\,\big(\mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\big),\) and so obtain the max-bound</p> \[\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)] \le \,\mathbb{E}[\mathbf{w}(t,i)^2] + \mathbb{E}[\bar{\mathbf{g}}(t,i)^2] = \mathbb{E}[\mathbf{w}(t,i)^2] + 1.\] <p>\(\mathbb{E}[\mathbf{w}(t,i)^2]\) can be realized by maintaining an EMA estimate</p> \[\mathbf{s}(t,i) = \beta_a \,\mathbf{s}(t-1,i) + (1 - \beta_a) \,\mathbf{w}(t,i)^2,\] <p>and the max-normalization scaler is \(\bar{\mathbf{s}}(t,i) = (1 + \mathbf{s}(t,i))^{-1}\).</p> <p>Estimation noise can flip signs of the numerator estimate or inflate or deflate the learning rate ratio, leading to unstable or zeroed steps. When the sign flips negative. The default clipping the minimum to zero stalls learning. Since learning-rate constant \(\mu\) already acts as a safety margin (trust-region), a safer trust-region approach is to take the magnitude and clip the estimated learning-rate around \(\mu\) using \(\bar{\mathbf{s}}(t,i)\) and \(\mathbf{d}(t,i)\).</p><hr /> <p>Taken together, these practical clipping techniques, helps the partial correlation estimate to remain within a predictable dynamic range (via an elementwise min. and max. operation) and helps to avoid a transient spike dominated EMA estimate (via Huber clipping), leading to</p> \[\tilde{\mathbf{a}}(t,i) = \beta_a \, \tilde{\mathbf{a}}(t-1,i) + \mu\, \bar{\mathbf{s}}(t,i)\cdot{\psi_{c} (\mathbf{u}(t,i))}\] \[\mathbf{a}(t,i) = \max\bigl(\mu\,\bar{\mathbf{s}}(t,i)\cdot\mathbf{d}(t,i),\, \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu \bigr) \bigr).\] <h4 id="3-layer-wise-smoothing"> <a href="#3-layer-wise-smoothing" class="anchor-heading" aria-labelledby="3-layer-wise-smoothing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Layer-wise smoothing </h4> <p>To account for intra-layer variability in neural networks, we replace the raw estimates with their layerwise mean, ensuring smoother parameter adaptation within each layer. Specifically, for a given layer \(\ell\), with parameter size \(n_\ell\), the numerator update is averaged as</p> \[\mathbf{a}(t,i) = {n_\ell}^{-1} \sum_{i=1}^{n_\ell} \mathbf{a}(t,i).\]<hr /> <h3 id="partial-correlation-relaxed-upper-bound-variant"> <a href="#partial-correlation-relaxed-upper-bound-variant" class="anchor-heading" aria-labelledby="partial-correlation-relaxed-upper-bound-variant"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Partial-correlation: Relaxed Upper-bound variant </h3> <p>Since \(\mathbf{u}(t,i) = \mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) \ \le\ \frac{1}{2}\,\big(\mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\big),\)</p> <p>we may replace \(\mathbf{u}(t,i)\) with a relaxed form of the upper-bound, that is a symmetric term \(\tilde{\mathbf{u}}(t,i) = \mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\), weighted by \(\mu &lt; \frac{1}{2}\). Then a simpler estimate than the previous variant becomes</p> \[\tilde{\mathbf{a}}(t,i) = \beta_a \,\tilde{\mathbf{a}}(t-1,i) + (1 - \beta_a) \,\mu\,\tilde{\mathbf{u}}(t,i).\] \[\mathbf{a}(t,i) = \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu \bigr).\] <p>Layer wise smoothing can be applied next.</p><hr /> <h2 id="-unifying-phb-nag-and-adam"> <a href="#-unifying-phb-nag-and-adam" class="anchor-heading" aria-labelledby="-unifying-phb-nag-and-adam"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üß© Unifying PHB, NAG, and Adam </h2> <p>By choosing \(\beta, \gamma, \alpha(t,i)\) appropriately, AutoSGM recovers</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center">Algorithm</th> <th style="text-align: center">\(\gamma\)</th> <th style="text-align: center">\(\eta\)</th> <th style="text-align: center">\(\alpha(t,i)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">PHB</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1\)</td> <td style="text-align: center">\(\mu\)</td> </tr> <tr> <td style="text-align: center">NAG</td> <td style="text-align: center">\({\beta}/{(1+\beta)}\)</td> <td style="text-align: center">\((1+\beta)\)</td> <td style="text-align: center">\(\mu\)</td> </tr> <tr> <td style="text-align: center">Adam</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1-\beta\)</td> <td style="text-align: center">\({\mu}/{\mathbf{d}(t,i)}\)</td> </tr> </tbody> </table></div> <!-- $$ \begin{array}{l|c} \text{Algorithm} & \gamma \\ \hline \text{PHB} & 0 \\ \text{NAG} & \tfrac{\beta}{1+\beta} \\ \text{Adam} & 0 \\ \end{array} $$ --><hr /> <h2 id="-lowpass-regularization"> <a href="#-lowpass-regularization" class="anchor-heading" aria-labelledby="-lowpass-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üéØ Lowpass Regularization </h2> <p>Incorporating momentum is known to practically help stabilize learning dynamics and avoid <strong>shallow local minima</strong> <a class="citation" href="#haykinNeuralNetworksLearning2008">(Haykin, 2008)</a>.<br /> In the paper, we use the impulse response of the filter to show that smoothing the gradient (also called momentum) is <strong>approximately equivalent</strong> to smoothing the loss surface:</p> <p>This <strong>Lowpass regularization</strong> due to smoothing the gradient reflects the stabilized training effect of:</p> <ul> <li>reduced noise in the gradient updates,</li> <li>improved convergence to <strong>flatter local minima</strong>,</li> </ul> <p>often observed.</p><hr /> <h2 id="-key-empirical-findings"> <a href="#-key-empirical-findings" class="anchor-heading" aria-labelledby="-key-empirical-findings"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üìä Key Empirical Findings </h2> <p>Using Adam as a fixed-rate baseline, we tested our AutoSGM realization on <strong>CIFAR-10</strong> (ViT, ResNet) and <strong>language modeling</strong> (GPT-2 on WikiText and Shakespeare):</p> <ul> <li><strong>Tuning the filter‚Äôs zero</strong> \(\gamma\) away from 0 improved performance in most cases.</li> <li><strong>Iteration-dependent learning rates</strong> outperformed fixed rates.</li> <li>Best results often occurred for \(\gamma \in [0.31, 0.55]\) in the tested range.</li> </ul> <p>Example:</p> <ul> <li>ViT on CIFAR-10: <strong>+3% accuracy</strong> over fixed-rate baseline.</li> <li>GPT-2 on Shakespeare: <strong>~32% lower test loss</strong> over fixed-rate baseline.</li> </ul><hr /> <h2 id="-conclusion"> <a href="#-conclusion" class="anchor-heading" aria-labelledby="-conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üèÅ Conclusion </h2> <p>AutoSGM offers a <strong>unified, interpretable, and tunable</strong> framework for momentum-based optimization.<br /> By viewing PHB, NAG, and Adam as points in the <strong>AutoSGM parameter space</strong>, we can:</p> <ul> <li>Design <strong>new stochastic gradient optimizers</strong> with principled stability and error bounds.</li> <li>Achieve <strong>better generalization</strong> through lowpass regularization.</li> <li>Improve <strong>learning rate algorithms</strong>.</li> </ul><hr /> <p>üí° <em>Takeaway:</em> If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. <strong>AutoSGM gives you the structure or map</strong>.</p><hr /> <ol class="bibliography"><li><span id="bottouOptimizationMethodsLargescale2018">Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. <i>SIAM Review</i>, <i>60</i>(2), 223‚Äì311.</span></li> <li><span id="moonMathematicalMethodsAlgorithms2000">Moon, T. K., &amp; Stirling, W. C. (2000). <i>Mathematical Methods and Algorithms for Signal Processing</i>. Prentice Hall.</span></li> <li><span id="vantreesDetectionEstimationModulation2013">Van Trees, H. L., Bell, K. L., &amp; Tian, Z. (2013). <i>Detection Estimation and Modulation Theory, Detection, Estimation, and Filtering Theory, Part I</i> (2nd ed.). Wiley.</span></li> <li><span id="Menon2020GradientClipping">Menon, A. K., Rawat, A. S., Reddi, S. J., &amp; Kumar, S. (2020). Can Gradient Clipping Mitigate Label Noise? <i>Proceedings of the 8th International Conference on Learning Representations</i>.</span></li> <li><span id="haykinNeuralNetworksLearning2008">Haykin, S. (2008). <i>Neural Networks and Learning Machines</i> (3rd edition). Pearson.</span></li></ol> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
