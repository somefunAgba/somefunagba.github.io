<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>AutoSGM: Unifying Momentum Methods for Better Learning | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="AutoSGM Connecting the dots … HB, NAG, Adam." /> <meta property="og:description" content="AutoSGM Connecting the dots … HB, NAG, Adam." /> <link rel="canonical" href="http://localhost:4000/asgm.html" /> <meta property="og:url" content="http://localhost:4000/asgm.html" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"AutoSGM Connecting the dots … HB, NAG, Adam.","headline":"AutoSGM: Unifying Momentum Methods for Better Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm.html"},"url":"http://localhost:4000/asgm.html"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing meets deep learning optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods for Better Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods for Better Learning</a><ul class="nav-list"><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Learning Dynamics</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/lrwinds.html" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2025. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="autosgm"> <a href="#autosgm" class="anchor-heading" aria-labelledby="autosgm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> AutoSGM </h1> <p class="fs-6 fw-300">Connecting the dots … HB, NAG, Adam.</p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2">Page created: Sep 11 2025 at 12:00 AM</p> </div><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#autosgm" id="markdown-toc-autosgm">AutoSGM</a> <ol> <li><a href="#-the-core-update-rule" id="markdown-toc--the-core-update-rule">🌀 The Core Update Rule</a></li> <li><a href="#-optimal-learning-rate" id="markdown-toc--optimal-learning-rate">📐 Optimal Learning Rate</a> <ol> <li><a href="#practical-approximation" id="markdown-toc-practical-approximation">Practical Approximation</a></li> <li><a href="#ema-realizations" id="markdown-toc-ema-realizations">EMA Realizations</a></li> <li><a href="#partial-correlation-robust-ema-estimate" id="markdown-toc-partial-correlation-robust-ema-estimate">Partial-correlation: Robust EMA estimate</a> <ol> <li><a href="#1-input-clipping" id="markdown-toc-1-input-clipping">1. Input Clipping</a></li> <li><a href="#2-output-clipping-and-max-normalization" id="markdown-toc-2-output-clipping-and-max-normalization">2. Output Clipping and Max-Normalization</a></li> <li><a href="#3-layer-wise-smoothing" id="markdown-toc-3-layer-wise-smoothing">3. Layer-wise smoothing</a></li> </ol> </li> <li><a href="#alternatives-relaxed-upper-bound-variant" id="markdown-toc-alternatives-relaxed-upper-bound-variant">Alternatives: Relaxed Upper-bound variant</a></li> </ol> </li> <li><a href="#-unifying-phb-nag-and-adam" id="markdown-toc--unifying-phb-nag-and-adam">🧩 Unifying PHB, NAG, and Adam</a></li> <li><a href="#-lowpass-regularization" id="markdown-toc--lowpass-regularization">🎯 Lowpass Regularization</a></li> <li><a href="#-key-empirical-findings" id="markdown-toc--key-empirical-findings">📊 Key Empirical Findings</a> <ol> <li><a href="#1-gpt-2-on-shakespeare-char-32-lower-test-loss-over-fixed-numerator-baseline" id="markdown-toc-1-gpt-2-on-shakespeare-char-32-lower-test-loss-over-fixed-numerator-baseline">1. GPT-2 on Shakespeare-char: <strong>~32% lower test loss</strong> over fixed-numerator baseline.</a></li> <li><a href="#2-vit-on-cifar10" id="markdown-toc-2-vit-on-cifar10">2. VIT on CIFAR10.</a></li> <li><a href="#3-resnet18-on-cifar10" id="markdown-toc-3-resnet18-on-cifar10">3. ResNet18 on CIFAR10.</a></li> <li><a href="#4-gpt-2-on-wikitext-103" id="markdown-toc-4-gpt-2-on-wikitext-103">4. GPT-2 on WikiText-103.</a></li> </ol> </li> <li><a href="#-conclusion" id="markdown-toc--conclusion">🏁 Conclusion</a></li> </ol> </li> </ol> </details><hr /> <p>Momentum-based stochastic gradient methods such as <strong>Polyak’s Heavy Ball (PHB)</strong>, <strong>Nesterov’s Accelerated Gradient (NAG)</strong>, and <strong>Adam</strong> dominate deep learning optimization.</p> <p>They are often treated as separate algorithms, but in our recent work, we show they are all <strong>special cases</strong> of a single signal-processing (DSP) structure: the <strong>Automatic Stochastic Gradient Method (AutoSGM)</strong>.</p> <p>AutoSGM reframes these optimizers through the lens of a <strong>first-order lowpass filter</strong> applied to the stochastic gradient, revealing:</p> <ul> <li>A principled way to <strong>tune momentum</strong> via the filter’s pole and zero.</li> <li>An <strong>optimal, iteration-dependent learning rate</strong> that Adam approximates.</li> <li>Momentum as a natural form of <strong>lowpass regularization</strong> that smooths the loss surface.</li> </ul> <blockquote> <p><em>All algebraic operations are sample-by-sample (<strong>elementwise</strong>) unless otherwise stated.</em> The shorthand notation \((t,i)\) denotes the \(i\)-th element of a vector at iteration \(t\).</p> </blockquote><hr /> <h2 id="-the-core-update-rule"> <a href="#-the-core-update-rule" class="anchor-heading" aria-labelledby="-the-core-update-rule"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🌀 The Core Update Rule </h2> <p>The classic stochastic gradient method (SGM) updates parameters as:</p> \[\mathbf{w}(t+1,i) = \mathbf{w}(t,i) - \alpha(t,i) \, \mathbf{g}(t,i)\] <p>where:</p> <ul> <li>\(\mathbf{g}(t,i) = \nabla f(\mathbf{w}(t,i))\) is an unbiased stochastic gradient component,</li> <li>\(\alpha(t,i)\) denotes the learning rate at iteration \(t\), determined via a selected oracle function.</li> </ul> <p>In <strong>AutoSGM</strong>, we replace the stochastic gradient with a <strong>smoothed</strong> version:</p> \[\mathbf{w}(t+1,i) = \mathbf{w}(t,i) - \alpha(t,i) \, H_{\beta,\gamma}(\mathbf{g}(t,i))\] <p>Here, \(H_{\beta,\gamma}\) is a <strong>first-order filter</strong> with transfer function:</p> \[H(z) = \eta \, \frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}}, \quad 0 \le \beta &lt; 1, \ \gamma &lt; \beta\] <p>The time (iteration)-domain realization is:</p> \[\mathbf{v}(t,i) = \beta\,\mathbf{v}(t-1,i) + \eta\,(\mathbf{g}(t,i) - \gamma\,\mathbf{g}(t-1,i))\] <p><strong>See</strong> the <a href="/learning_dynamics">learning dynamics</a> of the stochastic gradient update.</p> <p>For a quadratic loss function, a <strong>simulation</strong> of the LTI learning behavior can be found here <a href="/quad_sim" target="_blank">quad_sim</a>.</p><hr /> <h2 id="-optimal-learning-rate"> <a href="#-optimal-learning-rate" class="anchor-heading" aria-labelledby="-optimal-learning-rate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📐 Optimal Learning Rate </h2> <p>We assume that both the objective function and its gradient are Lipschitz continuous <a class="citation" href="#bottouOptimizationMethodsLargescale2018">(Bottou et al., 2018)</a>. Let \(\mathbb{E}\) denote expectation with respect to the model distribution \(p(\mathbf{w})\) parameterized by \(\mathbf{w}\). For a log-likelihood objective \(f=\ln p(\mathbf{w})\), by the score-function identity, the expected gradient is zero for all \(\mathbf{w}\), not only at the optimum <a class="citation" href="#moonMathematicalMethodsAlgorithms2000">(Moon &amp; Stirling, 2000; Van Trees et al., 2013)</a>, so that \(\mathbb{E}[\mathbf{g}(t,i)] = 0\).</p> <p>By minimizing the expected squared error \(\mathbb{E}[\mathbf{e}(t,i)^2]\), at iteration \(t\), we derive an <strong>iteration-dependent optimal learning rate</strong></p> \[\alpha(t,i)^\star = \frac{\mathbb{E}[\mathbf{w}(t,i) \,\mathbf{g}(t,i)]}{\mathbb{E}[\mathbf{g}(t,i)^2]},\] <p>where \(\mathbf{e}(t,i) = \mathbf{w}(t,i) - {\mathbf{w}(i)}^\star\) is the parameter error vector, the gap between current weights and a local optimum.</p><hr /> <h3 id="practical-approximation"> <a href="#practical-approximation" class="anchor-heading" aria-labelledby="practical-approximation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Approximation </h3> <p>In practice, the iteration-dependent optimal learning‑rate is realized using standard adaptive‑filtering techniques <a class="citation" href="#dinizAdaptiveFilteringAlgorithms2020">(Diniz, 2020; Haykin, 2014)</a>, which involve the following steps:</p> <ol> <li>Expectations are estimated with <strong>exponential moving averages (EMA)</strong>.</li> <li>For numerical stability, we use the normalized gradient form.</li> <li>As a safety margin, the iteration-dependent locally-optimal learning rate estimate is modulated with a small \(\mu\digamma(t)\) which acts as a trust-region against the effect of noisy gradient scales.</li> </ol> <p>Let \(0 \le \mu\digamma(t) \le 1\), where \(\mu &gt; 0\), \(0\le \digamma(t) \le 1\) is a learning-rate schedule. Define</p> \[\bar{\mathbf{g}}(t,i) = \frac{\mathbf{g}(t,i)}{\sqrt{\mathbb{E}[\mathbf{g}(t,i)^2]}},\] <p>where \(\bar{\mathbf{g}}(t,i)\) is the normalized gradient scaled to its unit root-mean-square (RMS) value. The learning rate becomes</p> \[\alpha(t,i) = \mu \digamma(t)\, \frac{\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)]}{\sqrt{\mathbb{E}[\mathbf{g}(t,i)^2]}}.\]<hr /> <h3 id="ema-realizations"> <a href="#ema-realizations" class="anchor-heading" aria-labelledby="ema-realizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> EMA Realizations </h3> <p>Track the second moment of the gradient:</p> \[\mathbf{b}(t,i) = \beta_b \,\mathbf{b}(t-1,i) + \mu (1 - \beta_b) \,\mathbf{g}(t,i)^2\] <p>→ moment estimation.</p> <p>Define the RMS-normalizer:</p> \[\mathbf{d}(t,i) = \sqrt{\frac{\mathbf{b}(t,i)}{1 - \beta_b^t}} + \epsilon\] <p>→ bias-corrected RMS with small \(\epsilon\) <a class="citation" href="#honigAdaptiveFiltersStructures1984">(Honig &amp; Messerschmitt, 1984)</a> to prevent division by zero.</p> <p>Track the numerator term:</p> \[\mathbf{a}(t,i) = \beta_a \,\mathbf{a}(t-1,i) + \mu \,\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)\] <p>→ a naive running estimate of the weight–gradient correlation.</p> <p>Finally:</p> \[\alpha(t,i) = \digamma(t)\,\frac{\mathbf{a}(t,i)}{\mathbf{d}(t,i)}.\] <blockquote> <p><strong>Note:</strong> This reduces to only <strong>adaptive moment estimation</strong> when \(\mathbf{a}(t,i)\) is replaced by a fixed constant \(1\).</p> </blockquote> \[\alpha(t,i) = \mu\digamma(t)\,\frac{1}{\mathbf{d}(t,i)}.\]<hr /> <h3 id="partial-correlation-robust-ema-estimate"> <a href="#partial-correlation-robust-ema-estimate" class="anchor-heading" aria-labelledby="partial-correlation-robust-ema-estimate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Partial-correlation: Robust EMA estimate </h3> <p>The learning-rate’s EMA numerator estimates a partial-correlation. EMAs assume a well-behaved noise model <a class="citation" href="#huberRobustEstimationLocation1992">(Huber, 1992)</a>. <strong>Heavy‑tailed stochastic correlations, noisy sign flips and occasional magnitude spikes can break this assumption</strong> <a class="citation" href="#zoubirRobustStatisticsSignal2018a">(Zoubir et al., 2018)</a>.</p> <p>As an empirical safeguard, we want the numerator estimate to remain positive, well‑bounded, and avoid corruptions due to noisy, heavy-tailed inputs. In other words, we want to robustify the partial correlation estimate from the EMA without distorting the bulk of the signal observed via its input \(\mathbf{u}(t,i) = \mathbf{w}(t,i) \, \bar{\mathbf{g}}(t,i)\)</p> <h4 id="1-input-clipping"> <a href="#1-input-clipping" class="anchor-heading" aria-labelledby="1-input-clipping"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Input Clipping </h4> <p>Heavy-tailed gradient noise statistics induce misleading spikes that can dominate the EMA’s estimate over many iterations and increase its bias from the true mean estimate. Since, we do not know the probability distribution, <strong>Markov’s inequality</strong> gives a rationale for how often large such values can occur. The inequality</p> \[\mathbb{Pr}[|u| \ge c\,\mathbb{E}[|u|] ] \le \frac{1}{c},\] <p>relates how large the magnitude of \(u\) can be relative to its expected magnitude. <strong>Huberisation</strong> <a class="citation" href="#zoubirRobustEstimationSignal2012">(Zoubir et al., 2012; Menon et al., 2020)</a> is a practical way to robustly mitigate such heavy-tailed values. The Huber clipping function bounds the most extreme outliers (\(c \times\) the expected scale) before they enter the EMA, while avoiding dead zones and allowing moderate estimates to pass through untouched relative to its expected scale.</p> \[\psi_{c}(u) = \begin{cases} u, &amp; |u| \le c\,\mathbb{E}[|u|] \\ \mathrm{sign}(u) \cdot c\,\mathbb{E}[|u|], &amp; |u| &gt; c\,\mathbb{E}[|u|]. \end{cases}\] <p>Here, \(u\) denotes the instantaneous input, and \(c\) is a scale multiplier used to clip only extreme outliers relative to the expected scale. For example, setting \(c=4\) corresponds to the prior that the probability \(p\) of its magnitude \(|u|\) exceeding four times its mean is no more than \(25\%\). Equivalently, the probability that \(|u|\) remains below this threshold is at least \(1-p=75\%\). Therefore, the interval defined by the 25–75% quantiles captures the bulk of the distribution, while the clipping function suppresses only the most extreme values. This yields a more <strong>robust EMA estimator</strong> that is less sensitive to heavy‑tailed noise and spurious magnitude spikes.</p> <!-- , and so the probability of the magnitude being less than $$4$$ times its mean is at least $$1-p=75\%$$. T --> <p>Using the instantaneous partial correlation \(\mathbf{u}(t,i)\), we can adaptively estimate the expected scale, via the EMA estimate</p> \[\hat{\mathbf{u}}(t,i) = \beta_a \, \hat{\mathbf{u}}(t-1,i) + (1 - \beta_a)\,|{\mathbf{u}}(t,i)|,\] <p>where \(\hat{\mathbf{u}}(t,i)\) adapts to the typical scale of \(\mathbf{u}(t,i)\) in each layer.</p> <h4 id="2-output-clipping-and-max-normalization"> <a href="#2-output-clipping-and-max-normalization" class="anchor-heading" aria-labelledby="2-output-clipping-and-max-normalization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Output Clipping and Max-Normalization </h4> <p>Estimation noise can flip signs of the numerator estimate, artificially inflating or deflating the learning‑rate ratio and producing unstable or vanishing steps.</p> <p>The input‑clipping strategy does not account for sign flips that slip through the estimator’s input. When the estimate turns negative, clipping to zero stalls progress entirely. Since the global learning‑rate constant \(\mu\) already serves as a safety margin (a trust‑region), a more principled approach is to design a trust‑region safeguard around \(\mu\) that preserves sign information while bounding magnitude, rather than zeroing negative values outright.</p> <p>Thus, we want to ensure the numerical estimate for the partial-correlation stays within a predictable, and reasonable range, while ensuring \(\alpha(t,i) \ge 0\). From the inequality \(0 \le (\mathbf{w}(t,i)-\bar{\mathbf{g}}(t,i))^2\), we have that \(\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) \ \le\ \frac{1}{2}\,\big(\mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\big),\) and so obtain the max-bound</p> \[\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)] \le \,\mathbb{E}[\mathbf{w}(t,i)^2] + \mathbb{E}[\bar{\mathbf{g}}(t,i)^2] = \mathbb{E}[\mathbf{w}(t,i)^2] + 1.\] <p>\(\mathbb{E}[\mathbf{w}(t,i)^2]\) can be realized by maintaining an EMA estimate</p> \[\mathbf{s}(t,i) = \beta_a \,\mathbf{s}(t-1,i) + (1 - \beta_a) \,\mathbf{w}(t,i)^2,\] <p>and the max-normalizer is \(\bar{\mathbf{s}}(t,i) = 1 + \mathbf{s}(t,i)\).</p> <p>Taken together, using \(\bar{\mathbf{s}}(t,i)\) and \(\mathbf{d}(t,i)\), these practical clipping techniques, help the partial correlation estimate from the EMA to remain within a predictable dynamic range, leading to</p> \[\tilde{\mathbf{a}}(t,i) = \beta_a \, \tilde{\mathbf{a}}(t-1,i) + \mu\, \bar{\mathbf{s}}(t,i)^{-1}\cdot{\psi_{c} (\mathbf{u}(t,i))}\] <!-- $$ \mathbf{a}(t,i) = \max\bigl(\mu\,\bar{\mathbf{s}}(t,i)^{-1}\cdot\mathbf{d}(t,i),\, \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu \bigr) \bigr). $$ --> \[\mathbf{a}(t,i) = \max\bigl(0,\, \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu\,\bar{\mathbf{s}}(t,i) \bigr) \bigr).\] <h4 id="3-layer-wise-smoothing"> <a href="#3-layer-wise-smoothing" class="anchor-heading" aria-labelledby="3-layer-wise-smoothing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Layer-wise smoothing </h4> <p>To account for intra-layer structure and variability in neural networks, we observed that replacing the raw estimates with their layerwise mean, ensured more numerically stable parameter adaptation within each layer. Specifically, for a given layer \(\ell\), with parameter size \(n_\ell\), the numerator update is averaged as</p> \[\mathbf{a}(t,i) ← \frac{1}{n_\ell} \sum_{i=1}^{n_\ell} \mathbf{a}(t,i).\]<hr /> <h3 id="alternatives-relaxed-upper-bound-variant"> <a href="#alternatives-relaxed-upper-bound-variant" class="anchor-heading" aria-labelledby="alternatives-relaxed-upper-bound-variant"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Alternatives: Relaxed Upper-bound variant </h3> <p>Since \(\mathbf{u}(t,i) = \mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) \ \le\ \frac{1}{2}\,\big(\mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\big),\) we may replace \(\mathbf{u}(t,i)\) with a relaxed form of the symmetric upper-bound, \(\tilde{\mathbf{u}}(t,i) = \mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\), weighted by \(\mu &lt; \frac{1}{2}\). A proxy estimate of the partial-correlation then becomes</p> \[\tilde{\mathbf{a}}(t,i) = \beta_a \,\tilde{\mathbf{a}}(t-1,i) + (1 - \beta_a) \,\mu\,\tilde{\mathbf{u}}(t,i).\] \[\mathbf{a}(t,i) = \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu \bigr).\] <p>Layer-wise smoothing can be applied next.</p> <!-- --- --> <h2 id="-unifying-phb-nag-and-adam"> <a href="#-unifying-phb-nag-and-adam" class="anchor-heading" aria-labelledby="-unifying-phb-nag-and-adam"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🧩 Unifying PHB, NAG, and Adam </h2> <p>By choosing \(\beta, \gamma, \alpha(t,i)\) appropriately, AutoSGM recovers</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center">Algorithm</th> <th style="text-align: center">\(\beta\)</th> <th style="text-align: center">\(\gamma\)</th> <th style="text-align: center">\(\eta\)</th> <th style="text-align: center">\(\alpha(t,i)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Basic</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(\mu \digamma(t)\)</td> </tr> <tr> <td style="text-align: center">PHB</td> <td style="text-align: center">\(✓\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1\)</td> <td style="text-align: center">\(\mu \digamma(t)\)</td> </tr> <tr> <td style="text-align: center">NAG</td> <td style="text-align: center">\(✓\)</td> <td style="text-align: center">\({\beta}/{(1+\beta)}\)</td> <td style="text-align: center">\((1+\beta)\)</td> <td style="text-align: center">\(\mu \digamma(t)\)</td> </tr> <tr> <td style="text-align: center">Adam</td> <td style="text-align: center">\(✓\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1-\beta\)</td> <td style="text-align: center">\({\mu} \digamma(t) \cdot{\mathbf{d}(t,i)}^{-1}\)</td> </tr> </tbody> </table></div> <!-- $$ \begin{array}{l|c} \text{Algorithm} & \gamma \\ \hline \text{PHB} & 0 \\ \text{NAG} & \tfrac{\beta}{1+\beta} \\ \text{Adam} & 0 \\ \end{array} $$ --><hr /> <h2 id="-lowpass-regularization"> <a href="#-lowpass-regularization" class="anchor-heading" aria-labelledby="-lowpass-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🎯 Lowpass Regularization </h2> <p>Incorporating momentum is known to practically help stabilize learning dynamics and avoid <strong>shallow local minima</strong> <a class="citation" href="#haykinNeuralNetworksLearning2008">(Haykin, 2008)</a>.<br /> In the paper, we use the impulse response of the filter to show that smoothing the gradient (also called momentum) is <strong>approximately equivalent</strong> to smoothing the loss surface:</p> <p>This <strong>Lowpass regularization</strong> due to smoothing the gradient reflects the stabilized training effect of:</p> <ul> <li>reduced noise in the gradient updates,</li> <li>improved convergence to <strong>flatter local minima</strong>,</li> </ul> <p>often observed.</p><hr /> <h2 id="-key-empirical-findings"> <a href="#-key-empirical-findings" class="anchor-heading" aria-labelledby="-key-empirical-findings"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📊 Key Empirical Findings </h2> <p>Using Adam as a fixed-numerator baseline for the learning rate, we tested the AutoSGM framework using our iteration-dependent learning-rate realization on <strong>CIFAR-10</strong> (ViT, ResNet) and <strong>language modeling</strong> (GPT-2 on WikiText and Shakespeare):</p> <ul> <li><strong>Tuning the filter’s zero</strong> \(\gamma\) improved performance in most cases.</li> <li><strong>Iteration-dependent learning rate numerator</strong> (circled dots) outperformed fixed numerator (squared dots).</li> </ul> <h3 id="1-gpt-2-on-shakespeare-char-32-lower-test-loss-over-fixed-numerator-baseline"> <a href="#1-gpt-2-on-shakespeare-char-32-lower-test-loss-over-fixed-numerator-baseline" class="anchor-heading" aria-labelledby="1-gpt-2-on-shakespeare-char-32-lower-test-loss-over-fixed-numerator-baseline"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. GPT-2 on Shakespeare-char: <strong>~32% lower test loss</strong> over fixed-numerator baseline. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="assets/asgm/gpt2_30M_shake_tr.png" width="300" /></td> <td><img src="/assets/asgm/gpt2_30M_shake_tr.png" width="300" /></td> </tr> </tbody> </table></div> <h3 id="2-vit-on-cifar10"> <a href="#2-vit-on-cifar10" class="anchor-heading" aria-labelledby="2-vit-on-cifar10"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. VIT on CIFAR10. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="./assets/asgm/vit_cifar10_tr.png" width="300" /></td> <td><img src="./assets/asgm/vit_cifar10_tt.png" width="300" /></td> </tr> </tbody> </table></div> <h3 id="3-resnet18-on-cifar10"> <a href="#3-resnet18-on-cifar10" class="anchor-heading" aria-labelledby="3-resnet18-on-cifar10"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. ResNet18 on CIFAR10. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="./assets/asgm/resnet_cifar10_tr.png" width="300" /></td> <td><img src="./assets/asgm/resnet_cifar10_tt.png" width="300" /></td> </tr> </tbody> </table></div> <h3 id="4-gpt-2-on-wikitext-103"> <a href="#4-gpt-2-on-wikitext-103" class="anchor-heading" aria-labelledby="4-gpt-2-on-wikitext-103"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. GPT-2 on WikiText-103. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="./assets/asgm/gpt2_124M_wiki_tr.png" width="300" /></td> <td><img src="./assets/asgm/gpt2_124M_wiki_tt.png" width="300" /></td> </tr> </tbody> </table></div><hr /> <h2 id="-conclusion"> <a href="#-conclusion" class="anchor-heading" aria-labelledby="-conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🏁 Conclusion </h2> <p>AutoSGM offers a <strong>unified, interpretable, and tunable</strong> framework for momentum-based optimization.<br /> By viewing PHB, NAG, and Adam as points in the <strong>AutoSGM parameter space</strong>, we can:</p> <ul> <li>Design <strong>new stochastic gradient optimizers</strong> with principled stability and error bounds.</li> <li>Achieve <strong>better generalization</strong> through lowpass regularization.</li> <li>Improve <strong>learning rate algorithms</strong>.</li> </ul><hr /> <p>💡 <em>Takeaway:</em> If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. <strong>AutoSGM gives you the structure or map</strong>.</p><hr /> <ol class="bibliography"><li><span id="bottouOptimizationMethodsLargescale2018">Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. <i>SIAM Review</i>, <i>60</i>(2), 223–311.</span></li> <li><span id="moonMathematicalMethodsAlgorithms2000">Moon, T. K., &amp; Stirling, W. C. (2000). <i>Mathematical Methods and Algorithms for Signal Processing</i>. Prentice Hall.</span></li> <li><span id="vantreesDetectionEstimationModulation2013">Van Trees, H. L., Bell, K. L., &amp; Tian, Z. (2013). <i>Detection Estimation and Modulation Theory, Detection, Estimation, and Filtering Theory, Part I</i> (2nd ed.). Wiley.</span></li> <li><span id="dinizAdaptiveFilteringAlgorithms2020">Diniz, P. S. R. (2020). <i>Adaptive Filtering: Algorithms and Practical Implementation</i> (5th edition). Springer International Publishing. https://doi.org/10.1007/978-3-030-29057-3</span></li> <li><span id="haykinAdaptiveFilterTheory2014">Haykin, S. (2014). <i>Adaptive Filter Theory</i> (5th, intern.). Pearson.</span></li> <li><span id="honigAdaptiveFiltersStructures1984">Honig, M. L., &amp; Messerschmitt, D. G. (1984). <i>Adaptive Filters: Structures, Algorithms and Applications</i>. Kluwer Academic Publishers.</span></li> <li><span id="huberRobustEstimationLocation1992">Huber, P. J. (1992). Robust Estimation of a Location Parameter. In S. Kotz &amp; N. L. Johnson (Eds.), <i>Breakthroughs in Statistics: Methodology and Distribution</i> (pp. 492–518). Springer. https://doi.org/10.1007/978-1-4612-4380-9_35</span></li> <li><span id="zoubirRobustStatisticsSignal2018a">Zoubir, A. M., Koivunen, V., Ollila, E., &amp; Muma, M. (2018). <i>Robust Statistics for Signal Processing</i> (1st ed.). Cambridge University Press. https://doi.org/10.1017/9781139084291</span></li> <li><span id="zoubirRobustEstimationSignal2012">Zoubir, A. M., Koivunen, V., Chakhchoukh, Y., &amp; Muma, M. (2012). Robust Estimation in Signal Processing: A Tutorial-Style Treatment of Fundamental Concepts. <i>IEEE Signal Processing Magazine</i>, <i>29</i>(4), 61–80.</span></li> <li><span id="Menon2020GradientClipping">Menon, A. K., Rawat, A. S., Reddi, S. J., &amp; Kumar, S. (2020). Can Gradient Clipping Mitigate Label Noise? <i>Proceedings of the 8th International Conference on Learning Representations</i>.</span></li> <li><span id="haykinNeuralNetworksLearning2008">Haykin, S. (2008). <i>Neural Networks and Learning Machines</i> (3rd edition). Pearson.</span></li></ol> <hr> <h2 class="text-delta">Table of contents</h2> <ul> <li> <a href="/learning_dynamics">Learning Dynamics</a> </li> </ul> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2"> Page last modified: <span class="d-inline-block">Oct 9 2025 at 12:00 AM</span>. </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
