<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>AutoSGM: Unifying Momentum Methods for Better Learning | Research Notes</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Signal processing meets deep learning optimization." /> <meta property="og:description" content="Signal processing meets deep learning optimization." /> <link rel="canonical" href="http://localhost:4000/asgm.html" /> <meta property="og:url" content="http://localhost:4000/asgm.html" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"Signal processing meets deep learning optimization.","headline":"AutoSGM: Unifying Momentum Methods for Better Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm.html"},"url":"http://localhost:4000/asgm.html"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing meets deep learning optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods for Better Learning</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2025. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="autosgm"> <a href="#autosgm" class="anchor-heading" aria-labelledby="autosgm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> AutoSGM </h1> <p class="fs-6 fw-300">Connecting the dots … HB, NAG, Adam.</p><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#autosgm" id="markdown-toc-autosgm">AutoSGM</a> <ol> <li><a href="#-the-core-update-rule" id="markdown-toc--the-core-update-rule">🔍 The Core Update Rule</a></li> <li><a href="#-optimal-learning-rate" id="markdown-toc--optimal-learning-rate">📐 Optimal Learning Rate</a> <ol> <li><a href="#practical-approximation" id="markdown-toc-practical-approximation">Practical Approximation</a></li> <li><a href="#ema-realizations" id="markdown-toc-ema-realizations">EMA Realizations</a></li> <li><a href="#-making-the-numerator-ema-update-robust-to-noise" id="markdown-toc--making-the-numerator-ema-update-robust-to-noise">🔧 Making the Numerator EMA Update Robust to Noise</a> <ol> <li><a href="#huber-clip-function-elementwise" id="markdown-toc-huber-clip-function-elementwise">Huber Clip function (Elementwise)</a></li> <li><a href="#adaptive-huber-threshold" id="markdown-toc-adaptive-huber-threshold">Adaptive Huber Threshold</a></li> <li><a href="#normalize-by-a-tighter-upper-bound" id="markdown-toc-normalize-by-a-tighter-upper-bound">Normalize by a Tighter Upper Bound</a></li> <li><a href="#modified-learning-rate-numerator" id="markdown-toc-modified-learning-rate-numerator">Modified Learning-rate Numerator</a></li> <li><a href="#positivity-and-overshoot-safeguards" id="markdown-toc-positivity-and-overshoot-safeguards">Positivity and Overshoot Safeguards</a></li> <li><a href="#layer-wise-smoothing" id="markdown-toc-layer-wise-smoothing">Layer-wise smoothing</a></li> </ol> </li> <li><a href="#alternative-variants" id="markdown-toc-alternative-variants">Alternative Variants</a> <ol> <li><a href="#relaxed-upper-bound" id="markdown-toc-relaxed-upper-bound">Relaxed Upper-bound</a></li> </ol> </li> </ol> </li> <li><a href="#-unifying-phb-nag-and-adam" id="markdown-toc--unifying-phb-nag-and-adam">🧩 Unifying PHB, NAG, and Adam</a></li> <li><a href="#-lowpass-regularization" id="markdown-toc--lowpass-regularization">🎯 Lowpass Regularization</a></li> <li><a href="#-key-empirical-findings" id="markdown-toc--key-empirical-findings">📊 Key Empirical Findings</a></li> <li><a href="#-conclusion" id="markdown-toc--conclusion">🏁 Conclusion</a></li> </ol> </li> </ol> </details><hr /> <p>Momentum-based stochastic gradient methods such as <strong>Polyak’s Heavy Ball (PHB)</strong>, <strong>Nesterov’s Accelerated Gradient (NAG)</strong>, and <strong>Adam</strong> dominate deep learning optimization.</p> <p>They are often treated as separate algorithms, but in our recent work, we show they are all <strong>special cases</strong> of a single signal-processing (DSP) structure: the <strong>Automatic Stochastic Gradient Method (AutoSGM)</strong>.</p> <p>AutoSGM reframes these optimizers through the lens of a <strong>first-order lowpass filter</strong> applied to the stochastic gradient, revealing:</p> <ul> <li>A principled way to <strong>tune momentum</strong> via the filter’s pole and zero.</li> <li>An <strong>optimal, iteration-dependent learning rate</strong> that Adam approximates.</li> <li>Momentum as a natural form of <strong>lowpass regularization</strong> that smooths the loss surface.</li> </ul> <blockquote> <p><em>All algebraic operations are sample-by-sample (<strong>elementwise</strong>) unless otherwise stated.</em></p> </blockquote><hr /> <h2 id="-the-core-update-rule"> <a href="#-the-core-update-rule" class="anchor-heading" aria-labelledby="-the-core-update-rule"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔍 The Core Update Rule </h2> <p>The classic stochastic gradient method (SGM) updates parameters as:</p> \[\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha_t \, \mathbf{g}_t\] <p>where:</p> <ul> <li>\(\mathbf{g}_t = \nabla f(\mathbf{w}_t)\) is the stochastic gradient,</li> <li>\(\alpha_t\) is the learning rate.</li> </ul> <p>In <strong>AutoSGM</strong>, we replace the stochastic gradient with a <strong>smoothed</strong> version:</p> \[\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha_t \, H_{\beta,\gamma}(\mathbf{g}_t)\] <p>Here, \(H_{\beta,\gamma}\) is a <strong>first-order filter</strong> with transfer function:</p> \[H(z) = \eta \, \frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}}, \quad 0 &lt; \beta &lt; 1, \ \gamma &lt; \beta\] <p>The time-domain realization is:</p> \[\mathbf{v}_t = \beta \mathbf{v}_{t-1} + \eta (\mathbf{g}_t - \gamma \mathbf{g}_{t-1})\]<hr /> <h2 id="-optimal-learning-rate"> <a href="#-optimal-learning-rate" class="anchor-heading" aria-labelledby="-optimal-learning-rate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📐 Optimal Learning Rate </h2> <p>We derive an <strong>iteration-dependent optimal learning rate</strong></p> \[\alpha_t^\star = \frac{\mathbb{E}[\mathbf{e}_t \,\mathbf{g}_t]}{\mathbb{E}[\mathbf{g}_t^2]},\] <p>by minimizing the expected squared error \(\mathbb{E}[\mathbf{e}_t^2]\), where \(\mathbf{e}_t = \mathbf{w}_t - \mathbf{w}^\star\) is the parameter error vector, the gap between current weights and a local optimum.</p><hr /> <h3 id="practical-approximation"> <a href="#practical-approximation" class="anchor-heading" aria-labelledby="practical-approximation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Approximation </h3> <p>In real training:</p> <ol> <li>\(\mathbf{w}^\star\) is unknown, so we replace \(\mathbf{e}_t\) with \(\mathbf{w}_t\).</li> <li>Expectations are estimated with <strong>exponential moving averages (EMA)</strong>.</li> <li>For numerical stability, we use the normalized gradient form.</li> <li>The numerator is modulated with a small \(\mu\) to reduce the effect of noisy gradient scales.</li> </ol> <p>Let \(0 &lt; \mu \le 1\), and define</p> \[\bar{\mathbf{g}}_t = \frac{\mathbf{g}_t}{\sqrt{\mathbb{E}[\mathbf{g}_t^2]}},\] <p>where \(\bar{\mathbf{g}}_t\) is the normalized gradient scaled to its unit root-mean-square (RMS) value. The learning rate becomes</p> \[\alpha_t = \mu \, \frac{\mathbb{E}[\mathbf{w}_t \,\bar{\mathbf{g}}_t]}{\sqrt{\mathbb{E}[\mathbf{g}_t^2]}}.\]<hr /> <h3 id="ema-realizations"> <a href="#ema-realizations" class="anchor-heading" aria-labelledby="ema-realizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> EMA Realizations </h3> <p>Track the second moment of the gradient:</p> \[\mathbf{b}_t = \beta_b \,\mathbf{b}_{t-1} + \mu (1 - \beta_b) \,\mathbf{g}_t^2\] <p>→ smooths out noisy gradient magnitudes.</p> <p>Define the RMS-normalizer:</p> \[\mathbf{d}_t = \sqrt{\frac{\mathbf{b}_t}{1 - \beta_b^t}} + \epsilon\] <p>→ bias-corrected RMS with small \(\epsilon\) to prevent division by zero.</p> <p>Track the numerator term:</p> \[\mathbf{a}_t = \beta_a \,\mathbf{a}_{t-1} + \mu \,\mathbf{w}_t \,\bar{\mathbf{g}}_t\] <p>→ running estimate of the weight–gradient correlation.</p> <p>Finally:</p> \[\alpha_t = \frac{\mathbf{a}_t}{\mathbf{d}_t}.\] <blockquote> <p><strong>Note:</strong> This reduces to <strong>Adam</strong> when \(\mathbf{a}_t\) is replaced by a fixed constant \(\mu\).</p> </blockquote><hr /> <h3 id="-making-the-numerator-ema-update-robust-to-noise"> <a href="#-making-the-numerator-ema-update-robust-to-noise" class="anchor-heading" aria-labelledby="-making-the-numerator-ema-update-robust-to-noise"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔧 Making the Numerator EMA Update Robust to Noise </h3> <p><strong>Goal</strong> Robustness to heavy‑tailed gradient correlations and spikes.</p> <p>Before dividing the numerator by the denominator, we want to ensure that the learning rate remains positive, well‑bounded, and free from overshoot due to noisy, heavy-tailed gradient statistics.</p> <p>In other words, we want to bound rare spikes in the instantaneous partial correlation term</p> \[\mathbf{u}_t = \mathbf{w}_t \, \bar{\mathbf{g}}_t\] <p>without distorting the bulk of the signal. This can be achieved via Huberization (Menon et al. (2020)) and Normalization of the numerator EMA’s input.</p> <p>Noisy, heavy-tailed gradient statistics are prone to rare, erroneous spikes that can dominate the EMA’s running average over many iterations.</p> <p>Techniques such as <strong>Huberization</strong> and <strong>Max-Normalization</strong> ensure such outliers are bounded before they enter the EMA, while avoiding dead zones and allowing smaller moderate values to pass through untouched. Using the <strong>adaptive threshold</strong> ensures each parameter has its threshold tuned to its running scale.</p><hr /> <h4 id="huber-clip-function-elementwise"> <a href="#huber-clip-function-elementwise" class="anchor-heading" aria-labelledby="huber-clip-function-elementwise"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Huber Clip function (Elementwise) </h4> <p>\(\psi_{v}(u) = \begin{cases} u, &amp; |u| \le c\,v \\ \mathrm{sign}(u) \cdot c\,v, &amp; |u| &gt; c\,v \end{cases}\)</p> <p>where \(u\) is the input and \(v\) is the Huber threshold, and typically \(c\in [3,5]\) is a scale multiplier to clip only extreme outliers relative to recent scale, and a smaller value means more aggressive clipping.</p> <h4 id="adaptive-huber-threshold"> <a href="#adaptive-huber-threshold" class="anchor-heading" aria-labelledby="adaptive-huber-threshold"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Adaptive Huber Threshold </h4> <p>We can adaptively estimate a threshold for each parameter. Maintain an EMA estimate of \(\mathbf{u}_t\) as</p> \[\hat{\mathbf{u}}_t = \beta_a \, \hat{\mathbf{u}}_{t-1} + (1 - \beta_a)\,|{\mathbf{u}}_t|,\] <p>where \(\hat{\mathbf{u}}_t\) adapts to the typical scale of \(\mathbf{u}_t\) in each layer.</p><hr /> <h4 id="normalize-by-a-tighter-upper-bound"> <a href="#normalize-by-a-tighter-upper-bound" class="anchor-heading" aria-labelledby="normalize-by-a-tighter-upper-bound"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Normalize by a Tighter Upper Bound </h4> <!-- To ensure the numerator stays within a predictable range, we use the inequality --> <p>From:</p> \[\mathbb{E}[\mathbf{w}_t \,\bar{\mathbf{g}}_t] \le \,\mathbb{E}[\mathbf{w}_t^2] + \mathbb{E}[\bar{\mathbf{g}}_t^2] = \mathbb{E}[\mathbf{w}_t^2] + 1.\] <p>Maintain an EMA estimate of \(\mathbb{E}[\mathbf{w}_t^2]\) as \(\mathbf{s}_t = \beta_a \,\mathbf{s}_{t-1} + (1 - \beta_a) \,\mathbf{w}_t^2.\)</p><hr /> <h4 id="modified-learning-rate-numerator"> <a href="#modified-learning-rate-numerator" class="anchor-heading" aria-labelledby="modified-learning-rate-numerator"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Modified Learning-rate Numerator </h4> <p>The modified learning rate numerator becomes:</p> \[\mathbf{a}_t = \beta_a \, \mathbf{a}_{t-1} + \mu\, \frac{\psi_{\hat{\mathbf{u}}_t} (\mathbf{u}_t)}{1 + \mathbf{s}_t}\]<hr /> <h4 id="positivity-and-overshoot-safeguards"> <a href="#positivity-and-overshoot-safeguards" class="anchor-heading" aria-labelledby="positivity-and-overshoot-safeguards"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Positivity and Overshoot Safeguards </h4> <p>Avoid <strong>sign flips</strong> to preserve adaptation while guaranteeing \(\alpha_t \ge 0\).</p> \[\mathbf{a}_t = \min\bigl( |\mathbf{a}_t|, \, \mu \bigr).\] <p>This caps the maximum partial correlation estimate and avoids transient spikes.</p> <h4 id="layer-wise-smoothing"> <a href="#layer-wise-smoothing" class="anchor-heading" aria-labelledby="layer-wise-smoothing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Layer-wise smoothing </h4> <p>To ensure smoother adaptation at the cost of per‑parameter detail. For each layer \(\ell\), with parameter size \(n_\ell\), replace the resulting \(\mathbf{a}_t\) values with their layerwise mean:</p> \[\bar{a}_t = {n_\ell}^{-1} \sum_{i=1}^{n_\ell} \mathbf{a}_{t,i}, \quad \mathbf{a}_{t,i} \leftarrow \bar{a}_t \ \ \forall i \in \ell.\]<hr /> <h3 id="alternative-variants"> <a href="#alternative-variants" class="anchor-heading" aria-labelledby="alternative-variants"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Alternative Variants </h3> <h4 id="relaxed-upper-bound"> <a href="#relaxed-upper-bound" class="anchor-heading" aria-labelledby="relaxed-upper-bound"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Relaxed Upper-bound </h4> <p>From the inequality \(0 \le (\mathbf{w}_t-\bar{\mathbf{g}}_t)^2\), we have that</p> \[\mathbf{u}_t = \mathbf{w}_t \,\bar{\mathbf{g}}_t \ \le\ \frac{1}{2}\,\big(\mathbf{w}_t^2 + \bar{\mathbf{g}}_t^2\big).\] <p>Replace \(\mathbf{u}_t\) with \(\tilde{\mathbf{u}}_t = \mathbf{w}_t^2 + \bar{\mathbf{g}}_t^2\), let \(\mu &lt; \frac{1}{2}\), the EMA becomes</p> \[\mathbf{a}_t = \beta_a \,\mathbf{a}_{t-1} + (1 - \beta_a) \,\mu\,\tilde{\mathbf{u}}_t.\] <p>Overshoot safeguards and layer wise smoothing can be applied next.</p> <p>This input is <strong>symmetric</strong> (both terms treated equally) and <strong>relaxed</strong> (looser than the tightest possible bound, but easier to compute).</p><hr /> <h2 id="-unifying-phb-nag-and-adam"> <a href="#-unifying-phb-nag-and-adam" class="anchor-heading" aria-labelledby="-unifying-phb-nag-and-adam"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🧩 Unifying PHB, NAG, and Adam </h2> <p>By choosing \(\beta, \gamma, \alpha_t\) appropriately, AutoSGM recovers:</p> <div class="table-wrapper"><table> <thead> <tr> <th>Algorithm</th> <th>Parameters</th> </tr> </thead> <tbody> <tr> <td>PHB</td> <td>\(\gamma = 0, \ \eta = 1, \ \alpha_t = \mu\)</td> </tr> <tr> <td>NAG</td> <td>\(\gamma = \frac{\beta}{1+\beta}, \ \eta = 1+\beta, \ \alpha_t = \mu\)</td> </tr> <tr> <td>Adam</td> <td>\(\gamma = 0, \ \eta = 1 - \beta, \ \alpha_t = \frac{\mu}{\mathbf{d}_t}\)</td> </tr> </tbody> </table></div><hr /> <h2 id="-lowpass-regularization"> <a href="#-lowpass-regularization" class="anchor-heading" aria-labelledby="-lowpass-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🎯 Lowpass Regularization </h2> <p>In the paper, we also show that smoothing the gradient is <strong>approximately equivalent</strong> to smoothing the loss surface:</p> <p><strong>Lowpass regularization</strong> helps to:</p> <ul> <li>Reduce noise in updates.</li> <li>Encourage convergence to <strong>flatter minima</strong>.</li> <li>Improve generalization.</li> </ul><hr /> <h2 id="-key-empirical-findings"> <a href="#-key-empirical-findings" class="anchor-heading" aria-labelledby="-key-empirical-findings"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📊 Key Empirical Findings </h2> <p>We tested AutoSGM on <strong>CIFAR-10</strong> (ViT, ResNet) and <strong>language modeling</strong> (GPT-2 on WikiText and Shakespeare):</p> <ul> <li><strong>Tuning the filter’s zero</strong> \(\gamma\) away from 0 improved performance in most cases.</li> <li><strong>Iteration-dependent learning rates</strong> outperformed fixed rates.</li> <li>Best results often occurred for \(\gamma \in [0.31, 0.55]\).</li> </ul> <p>Example:</p> <ul> <li>ViT on CIFAR-10: <strong>+3% accuracy</strong> over fixed-rate baseline.</li> <li>GPT-2 on Shakespeare: <strong>~32% lower test loss</strong>.</li> </ul><hr /> <h2 id="-conclusion"> <a href="#-conclusion" class="anchor-heading" aria-labelledby="-conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🏁 Conclusion </h2> <p>AutoSGM offers a <strong>unified, interpretable, and tunable</strong> framework for momentum-based optimization.<br /> By viewing PHB, NAG, and Adam as points in the <strong>AutoSGM parameter space</strong>, we can:</p> <ul> <li>Design <strong>new optimizers</strong> with principled stability and error bounds.</li> <li>Achieve <strong>better generalization</strong> through lowpass regularization.</li> <li>Improve <strong>learning rate tuning</strong>.</li> </ul><hr /> <p>💡 <strong>Takeaway:</strong><br /> If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. AutoSGM gives you the map.</p> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
