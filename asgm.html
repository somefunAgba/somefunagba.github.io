<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>AutoSGM: Unifying Momentum Methods | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="AutoSGM: Unifying Momentum Methods" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="AutoSGM Connecting the dots ‚Ä¶ HB, NAG, Adam." /> <meta property="og:description" content="AutoSGM Connecting the dots ‚Ä¶ HB, NAG, Adam." /> <link rel="canonical" href="http://localhost:4000/asgm.html" /> <meta property="og:url" content="http://localhost:4000/asgm.html" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="AutoSGM: Unifying Momentum Methods" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"AutoSGM Connecting the dots ‚Ä¶ HB, NAG, Adam.","headline":"AutoSGM: Unifying Momentum Methods","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm.html"},"url":"http://localhost:4000/asgm.html"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing, and control in learning and optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods</a><ul class="nav-list"><li class="nav-list-item"><a href="/lpf_not_ema" class="nav-list-link">Momentum is not an EMA</a></li><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Smooth Learning Dynamics</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Trust-region Optimal Learning rates</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Conjugated Directions</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/lrwinds.html" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> ¬© 2026. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="autosgm"> <a href="#autosgm" class="anchor-heading" aria-labelledby="autosgm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> AutoSGM </h1> <p class="fs-6 fw-300">Connecting the dots ‚Ä¶ HB, NAG, Adam.</p> <it>Preliminary versions of this work was presented at ICASSP 2024.</it> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2">Page created: Sep 11 2025 at 12:00 AM</p> </div><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#autosgm" id="markdown-toc-autosgm">AutoSGM</a> <ol> <li><a href="#-the-core-update-rule" id="markdown-toc--the-core-update-rule">üåÄ The Core Update Rule</a></li> <li><a href="#-unifying-phb-nag-and-adam" id="markdown-toc--unifying-phb-nag-and-adam">üß© Unifying PHB, NAG, and Adam</a></li> <li><a href="#-lowpass-regularization" id="markdown-toc--lowpass-regularization">üéØ Lowpass Regularization</a></li> <li><a href="#-key-empirical-findings" id="markdown-toc--key-empirical-findings">üìä Key Empirical Findings</a> <ol> <li><a href="#1-vit-on-cifar10" id="markdown-toc-1-vit-on-cifar10">1. VIT on CIFAR10.</a></li> <li><a href="#2-resnet18-on-cifar10" id="markdown-toc-2-resnet18-on-cifar10">2. ResNet18 on CIFAR10.</a></li> <li><a href="#3-gpt-2-on-shakespeare-char" id="markdown-toc-3-gpt-2-on-shakespeare-char">3. GPT-2 on Shakespeare-char:</a></li> <li><a href="#4-gpt-2-on-wikitext-103" id="markdown-toc-4-gpt-2-on-wikitext-103">4. GPT-2 on WikiText-103.</a></li> </ol> </li> <li><a href="#-conclusion" id="markdown-toc--conclusion">üèÅ Conclusion</a></li> </ol> </li> </ol> </details><hr /> <p>Momentum-based stochastic gradient methods such as <strong>Polyak‚Äôs Heavy Ball (PHB)</strong>, <strong>Nesterov‚Äôs Accelerated Gradient (NAG)</strong>, and <strong>Adam</strong> dominate deep learning optimization.</p> <p>They are often treated as separate algorithms, but in our recent work, we show they are all <strong>special cases</strong> of a single signal-processing (DSP) structure. The framework that allows us to do this is called the <strong>Automatic Stochastic Gradient Method (AutoSGM)</strong> framework.</p> <p>AutoSGM reframes these stochastic gradient optimizers through the lens of a <strong>first-order lowpass filter</strong> applied to the stochastic gradient, and the existence of an optimal iteration-dependent learning rate choice. The AutoSGM framework reveals:</p> <ul> <li>the <strong>first-order filtering mechanics</strong> behind what has been called <em>momentum</em>.</li> <li>that we can derive an <strong>optimal, iteration-dependent learning rate</strong> choice that involves <em>moment estimation</em>.</li> <li>that the smoothing effect of the first-order filter is a <strong>lowpass regularization</strong> of the loss surface.</li> </ul> <blockquote> <p><em>All algebraic operations are sample-by-sample (<strong>elementwise</strong>) unless otherwise stated.</em> The shorthand notation \((t,i)\) denotes the \(i\)-th element of a vector at iteration \(t\).</p> </blockquote><hr /> <h2 id="-the-core-update-rule"> <a href="#-the-core-update-rule" class="anchor-heading" aria-labelledby="-the-core-update-rule"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üåÄ The Core Update Rule </h2> <p>The classic stochastic gradient method (SGM) updates parameters as:</p> \[\mathbf{w}(t+1,i) = \mathbf{w}(t,i) - \alpha(t,i) \, \mathbf{g}(t,i)\] <p>where:</p> <ul> <li>\(\mathbf{g}(t,i) = \nabla f(\mathbf{w}(t,i))\) is an unbiased stochastic gradient component,</li> <li>\(\alpha(t,i)\) denotes the learning rate at iteration \(t\), determined via a selected oracle function.</li> </ul> <p>In <strong>AutoSGM</strong>, we replace the stochastic gradient with a <strong>smoothed</strong> version:</p> \[\mathbf{w}(t+1,i) = \mathbf{w}(t,i) - \alpha(t,i) \, H_{\beta,\gamma}(\mathbf{g}(t,i))\] <p>Here, \(H_{\beta,\gamma}\) is a <strong>first-order filter</strong> with transfer function:</p> \[H(z) = \eta \, \frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}}, \quad 0 \le \beta &lt; 1, \ \gamma &lt; \beta\] <p>The time (iteration)-domain realization is:</p> \[\mathbf{v}(t,i) = \beta\,\mathbf{v}(t-1,i) + \eta\,(\mathbf{g}(t,i) - \gamma\,\mathbf{g}(t-1,i))\] <p><strong>See</strong> the <a href="/learning_dynamics">learning dynamics</a> of the stochastic gradient update in this framework. <strong>Also, see</strong> <a href="/lpf_not_ema">smoothing is not averaging</a> for how the filter called <em>momentum</em> is generally not an exponential moving average, EMA.</p> <p>Using an extremely simple problem setup, we show in <a href="/asgm_qsim" target="_blank">Momentum: a principled signal-processing framework</a> that the first-order smoothing filter commonly called <em>momentum</em> is a principled signal processing operation‚úÖ, and in particular derive Nesterov‚Äôs Accelerated Gradient (NAG) from first principles as <strong>a point in the filter design space</strong> where we set \(\gamma=\tfrac{\beta}{1+\beta}\).</p><hr /> <h2 id="-unifying-phb-nag-and-adam"> <a href="#-unifying-phb-nag-and-adam" class="anchor-heading" aria-labelledby="-unifying-phb-nag-and-adam"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üß© Unifying PHB, NAG, and Adam </h2> <p>Choosing \(\beta, \gamma, \alpha(t,i)\) appropriately, the AutoSGM framework recovers</p> <div class="table-wrapper"><table> <thead> <tr> <th style="text-align: center">Algorithm</th> <th style="text-align: center">\(\beta\)</th> <th style="text-align: center">\(\gamma\)</th> <th style="text-align: center">\(\eta\)</th> <th style="text-align: center">\(\alpha(t,i)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Basic</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(\mu \digamma(t)\)</td> </tr> <tr> <td style="text-align: center">PHB</td> <td style="text-align: center">\(‚úì\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1\)</td> <td style="text-align: center">\(\mu \digamma(t)\)</td> </tr> <tr> <td style="text-align: center">NAG</td> <td style="text-align: center">\(‚úì\)</td> <td style="text-align: center">\({\beta}/{(1+\beta)}\)</td> <td style="text-align: center">\((1+\beta)\)</td> <td style="text-align: center">\(\mu \digamma(t)\)</td> </tr> <tr> <td style="text-align: center">Adam</td> <td style="text-align: center">\(‚úì\)</td> <td style="text-align: center">\(0\)</td> <td style="text-align: center">\(1-\beta\)</td> <td style="text-align: center">\({\mu} \digamma(t) \cdot{\mathbf{d}(t,i)}^{-1}\)</td> </tr> </tbody> </table></div> <!-- $$ \begin{array}{l|c} \text{Algorithm} & \gamma \\ \hline \text{PHB} & 0 \\ \text{NAG} & \tfrac{\beta}{1+\beta} \\ \text{Adam} & 0 \\ \end{array} $$ --><hr /> <h2 id="-lowpass-regularization"> <a href="#-lowpass-regularization" class="anchor-heading" aria-labelledby="-lowpass-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üéØ Lowpass Regularization </h2> <p>Incorporating momentum is known to practically help stabilize learning dynamics and avoid <strong>shallow local minima</strong> <a class="citation" href="#haykinNeuralNetworksLearning2008">(Haykin, 2008)</a>.<br /> In the paper, we use the impulse response of the filter to show that smoothing the gradient (also called momentum) is <strong>approximately equivalent</strong> to smoothing the loss surface:</p> <p>This <strong>Lowpass regularization</strong> due to smoothing the gradient reflects the stabilized training effect of:</p> <ul> <li>reduced noise in the gradient updates,</li> <li>improved convergence to <strong>flatter local minima</strong>,</li> </ul> <p>often observed.</p><hr /> <h2 id="-key-empirical-findings"> <a href="#-key-empirical-findings" class="anchor-heading" aria-labelledby="-key-empirical-findings"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üìä Key Empirical Findings </h2> <p>Using Adam as a <strong>fixed-numerator baseline</strong> for the learning rate, we tested the AutoSGM framework using a derived iteration-dependent <strong>partial correlation numerator</strong> alternative on <strong>CIFAR-10</strong> image classification (ViT, ResNet) and <strong>language modeling</strong> (GPT-2 on WikiText and Shakespeare):</p> <ul> <li><strong>Tuning the filter‚Äôs zero</strong> \(\gamma\) improved performance in most cases.</li> <li><strong>Iteration-dependent learning rate numerator</strong> (circled dots) outperformed <strong>fixed numerator</strong> (squared dots).</li> </ul> <h3 id="1-vit-on-cifar10"> <a href="#1-vit-on-cifar10" class="anchor-heading" aria-labelledby="1-vit-on-cifar10"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. VIT on CIFAR10. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="./assets/asgm/vit_cifar10_tr.png" width="300" /></td> <td><img src="./assets/asgm/vit_cifar10_tt.png" width="300" /></td> </tr> </tbody> </table></div> <h3 id="2-resnet18-on-cifar10"> <a href="#2-resnet18-on-cifar10" class="anchor-heading" aria-labelledby="2-resnet18-on-cifar10"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. ResNet18 on CIFAR10. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="./assets/asgm/resnet_cifar10_tr.png" width="300" /></td> <td><img src="./assets/asgm/resnet_cifar10_tt.png" width="300" /></td> </tr> </tbody> </table></div> <h3 id="3-gpt-2-on-shakespeare-char"> <a href="#3-gpt-2-on-shakespeare-char" class="anchor-heading" aria-labelledby="3-gpt-2-on-shakespeare-char"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. GPT-2 on Shakespeare-char: </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="assets/asgm/gpt2_30M_shake_tr.png" width="300" /></td> <td><img src="/assets/asgm/gpt2_30M_shake_tt.png" width="300" /></td> </tr> </tbody> </table></div> <h3 id="4-gpt-2-on-wikitext-103"> <a href="#4-gpt-2-on-wikitext-103" class="anchor-heading" aria-labelledby="4-gpt-2-on-wikitext-103"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 4. GPT-2 on WikiText-103. </h3> <div class="table-wrapper"><table> <tbody> <tr> <td><img src="./assets/asgm/gpt2_124M_wiki_tr.png" width="300" /></td> <td><img src="./assets/asgm/gpt2_124M_wiki_tt.png" width="300" /></td> </tr> </tbody> </table></div><hr /> <h2 id="-conclusion"> <a href="#-conclusion" class="anchor-heading" aria-labelledby="-conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üèÅ Conclusion </h2> <p>AutoSGM offers a <strong>unified, interpretable, and tunable</strong> framework for what has traditionally been referred to as momentum-based optimization.</p> <p>We can operate PHB, NAG, and Adam as points in the <strong>AutoSGM parameter space</strong>.</p> <p>Overall AutoSGM is a foundational framework for studying stochastic gradient algorithms, enabling systematic separation of filter design, automatic learning-rate function choices and the non-unique implementations present in current methods.</p> <!-- - Design **new stochastic gradient optimizers** with principled stability and error bounds. - Achieve **better generalization** through lowpass regularization. - Improve **learning rate algorithms**. --><hr /> <p>üí° <em>Takeaway:</em> If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. <strong>AutoSGM gives you the structure or map</strong>.</p><hr /> <ol class="bibliography"><li><span id="haykinNeuralNetworksLearning2008">Haykin, S. (2008). <i>Neural Networks and Learning Machines</i> (3rd edition). Pearson.</span></li></ol> <hr> <h2 class="text-delta">Table of contents</h2> <ul> <li> <a href="/lpf_not_ema">Momentum is not an EMA</a> </li> <li> <a href="/learning_dynamics">Smooth Learning Dynamics</a> </li> <li> <a href="/asgm_cjg">Trust-region Optimal Learning rates</a> </li> <li> <a href="/asgm_cjg">Conjugated Directions</a> </li> </ul> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2"> Page last modified: <span class="d-inline-block">Oct 9 2025 at 12:00 AM</span>. </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
