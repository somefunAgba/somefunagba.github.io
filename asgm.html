<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>AutoSGM: Unifying Momentum Methods for Better Learning | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Signal processing meets deep learning optimization." /> <meta property="og:description" content="Signal processing meets deep learning optimization." /> <link rel="canonical" href="http://localhost:4000/asgm.html" /> <meta property="og:url" content="http://localhost:4000/asgm.html" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="AutoSGM: Unifying Momentum Methods for Better Learning" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"Signal processing meets deep learning optimization.","headline":"AutoSGM: Unifying Momentum Methods for Better Learning","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm.html"},"url":"http://localhost:4000/asgm.html"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing meets deep learning optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods for Better Learning</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2025. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="autosgm"> <a href="#autosgm" class="anchor-heading" aria-labelledby="autosgm"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> AutoSGM </h1> <p class="fs-6 fw-300">Connecting the dots … HB, NAG, Adam.</p><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#autosgm" id="markdown-toc-autosgm">AutoSGM</a> <ol> <li><a href="#-the-core-update-rule" id="markdown-toc--the-core-update-rule">🔍 The Core Update Rule</a></li> <li><a href="#-optimal-learning-rate" id="markdown-toc--optimal-learning-rate">📐 Optimal Learning Rate</a> <ol> <li><a href="#practical-approximation" id="markdown-toc-practical-approximation">Practical Approximation</a></li> <li><a href="#ema-realizations" id="markdown-toc-ema-realizations">EMA Realizations</a></li> <li><a href="#partial-correlation-noise-robust-variant" id="markdown-toc-partial-correlation-noise-robust-variant">Partial-correlation: Noise-robust variant</a> <ol> <li><a href="#1-huber-clipping" id="markdown-toc-1-huber-clipping">1. Huber Clipping</a></li> <li><a href="#2-min-clipping-and-max-normalization" id="markdown-toc-2-min-clipping-and-max-normalization">2. Min-Clipping and Max-Normalization</a></li> <li><a href="#3-layer-wise-smoothing" id="markdown-toc-3-layer-wise-smoothing">3. Layer-wise smoothing</a></li> </ol> </li> <li><a href="#partial-correlation-relaxed-upper-bound-variant" id="markdown-toc-partial-correlation-relaxed-upper-bound-variant">Partial-correlation: Relaxed Upper-bound variant</a></li> </ol> </li> <li><a href="#-unifying-phb-nag-and-adam" id="markdown-toc--unifying-phb-nag-and-adam">🧩 Unifying PHB, NAG, and Adam</a></li> <li><a href="#-lowpass-regularization" id="markdown-toc--lowpass-regularization">🎯 Lowpass Regularization</a></li> <li><a href="#-key-empirical-findings" id="markdown-toc--key-empirical-findings">📊 Key Empirical Findings</a></li> <li><a href="#-conclusion" id="markdown-toc--conclusion">🏁 Conclusion</a></li> </ol> </li> </ol> </details><hr /> <p>Momentum-based stochastic gradient methods such as <strong>Polyak’s Heavy Ball (PHB)</strong>, <strong>Nesterov’s Accelerated Gradient (NAG)</strong>, and <strong>Adam</strong> dominate deep learning optimization.</p> <p>They are often treated as separate algorithms, but in our recent work, we show they are all <strong>special cases</strong> of a single signal-processing (DSP) structure: the <strong>Automatic Stochastic Gradient Method (AutoSGM)</strong>.</p> <p>AutoSGM reframes these optimizers through the lens of a <strong>first-order lowpass filter</strong> applied to the stochastic gradient, revealing:</p> <ul> <li>A principled way to <strong>tune momentum</strong> via the filter’s pole and zero.</li> <li>An <strong>optimal, iteration-dependent learning rate</strong> that Adam approximates.</li> <li>Momentum as a natural form of <strong>lowpass regularization</strong> that smooths the loss surface.</li> </ul> <blockquote> <p><em>All algebraic operations are sample-by-sample (<strong>elementwise</strong>) unless otherwise stated.</em></p> </blockquote><hr /> <h2 id="-the-core-update-rule"> <a href="#-the-core-update-rule" class="anchor-heading" aria-labelledby="-the-core-update-rule"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🔍 The Core Update Rule </h2> <p>The classic stochastic gradient method (SGM) updates parameters as:</p> \[\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha_t \, \mathbf{g}_t\] <p>where:</p> <ul> <li>\(\mathbf{g}_t = \nabla f(\mathbf{w}_t)\) is the stochastic gradient,</li> <li>\(\alpha_t\) denotes the learning rate at iteration \(t\), determined via a selected oracle function.</li> </ul> <p>In <strong>AutoSGM</strong>, we replace the stochastic gradient with a <strong>smoothed</strong> version:</p> \[\mathbf{w}_{t+1} = \mathbf{w}_t - \alpha_t \, H_{\beta,\gamma}(\mathbf{g}_t)\] <p>Here, \(H_{\beta,\gamma}\) is a <strong>first-order filter</strong> with transfer function:</p> \[H(z) = \eta \, \frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}}, \quad 0 &lt; \beta &lt; 1, \ \gamma &lt; \beta\] <p>The time-domain realization is:</p> \[\mathbf{v}_t = \beta \mathbf{v}_{t-1} + \eta (\mathbf{g}_t - \gamma \mathbf{g}_{t-1})\]<hr /> <h2 id="-optimal-learning-rate"> <a href="#-optimal-learning-rate" class="anchor-heading" aria-labelledby="-optimal-learning-rate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📐 Optimal Learning Rate </h2> <p>We derive an <strong>iteration-dependent optimal learning rate</strong></p> \[\alpha_t^\star = \frac{\mathbb{E}[\mathbf{w}_t \,\mathbf{g}_t]}{\mathbb{E}[\mathbf{g}_t^2]},\] <p>by minimizing the expected squared error \(\mathbb{E}[\mathbf{e}_t^2]\), where \(\mathbf{e}_t = \mathbf{w}_t - \mathbf{w}^\star\) is the parameter error vector, the gap between current weights and a local optimum.</p><hr /> <h3 id="practical-approximation"> <a href="#practical-approximation" class="anchor-heading" aria-labelledby="practical-approximation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Approximation </h3> <p>In real training:</p> <ol> <li>Expectations are estimated with <strong>exponential moving averages (EMA)</strong>.</li> <li>For numerical stability, we use the normalized gradient form.</li> <li>The numerator is modulated with a small \(\mu\digamma(t)\) to reduce the effect of noisy gradient scales.</li> </ol> <p>Let \(0 \le \mu\digamma(t) \le 1\), where \(\mu &gt; 0\), \(0\le \digamma(t) \le 1\) is a learning-rate schedule. Define</p> \[\bar{\mathbf{g}}_t = \frac{\mathbf{g}_t}{\sqrt{\mathbb{E}[\mathbf{g}_t^2]}},\] <p>where \(\bar{\mathbf{g}}_t\) is the normalized gradient scaled to its unit root-mean-square (RMS) value. The learning rate becomes</p> \[\alpha_t = \mu \digamma(t)\, \frac{\mathbb{E}[\mathbf{w}_t \,\bar{\mathbf{g}}_t]}{\sqrt{\mathbb{E}[\mathbf{g}_t^2]}}.\]<hr /> <h3 id="ema-realizations"> <a href="#ema-realizations" class="anchor-heading" aria-labelledby="ema-realizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> EMA Realizations </h3> <p>Track the second moment of the gradient:</p> \[\mathbf{b}_t = \beta_b \,\mathbf{b}_{t-1} + \mu (1 - \beta_b) \,\mathbf{g}_t^2\] <p>→ smooths out noisy gradient magnitudes.</p> <p>Define the RMS-normalizer:</p> \[\mathbf{d}_t = \sqrt{\frac{\mathbf{b}_t}{1 - \beta_b^t}} + \epsilon\] <p>→ bias-corrected RMS with small \(\epsilon\) to prevent division by zero.</p> <p>Track the numerator term:</p> \[\mathbf{a}_t = \beta_a \,\mathbf{a}_{t-1} + \mu \,\mathbf{w}_t \,\bar{\mathbf{g}}_t\] <p>→ running estimate of the weight–gradient correlation.</p> <p>Finally:</p> \[\alpha_t = \digamma(t)\,\frac{\mathbf{a}_t}{\mathbf{d}_t}.\] <blockquote> <p><strong>Note:</strong> This reduces to <strong>Adam</strong> when \(\mathbf{a}_t\) is replaced by a fixed constant \(\mu\).</p> </blockquote><hr /> <h3 id="partial-correlation-noise-robust-variant"> <a href="#partial-correlation-noise-robust-variant" class="anchor-heading" aria-labelledby="partial-correlation-noise-robust-variant"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Partial-correlation: Noise-robust variant </h3> <p>The learning-rate’s numerator is a partial-correlation estimate that is prone to heavy‑tailed stochastic gradient correlations and occasional magnitude spikes.</p> <p>As a design prior (\(\alpha_t \ge 0\)), before dividing the numerator by the denominator, we want to ensure that the numerator remains positive, well‑bounded, and free from overshoot due to noisy, heavy-tailed gradient statistics. In other words, we want to limit rare spikes in the instantaneous partial correlation term without distorting the bulk of the signal.</p> \[\mathbf{u}_t = \mathbf{w}_t \, \bar{\mathbf{g}}_t\]<hr /> <h4 id="1-huber-clipping"> <a href="#1-huber-clipping" class="anchor-heading" aria-labelledby="1-huber-clipping"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Huber Clipping </h4> <p>Noisy, heavy-tailed gradient statistics are prone to rare, erroneous spikes that can dominate the EMA’s running average over many iterations. A practically robust way to mitigate this is to clip the instantaneous partial correlation via <strong>Huberisation</strong> <a class="citation" href="#Menon2020GradientClipping">(Menon et al., 2020)</a> such that outliers are bounded before they enter the EMA, while avoiding dead zones and allowing moderate estimates to pass through untouched relative to its running scale. The Huber clip function (Elementwise)</p> \[\psi_{c}(u) = \begin{cases} u, &amp; |u| \le c\,\mathbb{E}[|u|] \\ \mathrm{sign}(u) \cdot c\,\mathbb{E}[|u|], &amp; |u| &gt; c\,\mathbb{E}[|u|], \end{cases}\] <p>where \(u\) is the input, and typically \(c\in [3,5]\) is a scale multiplier to clip only extreme outliers relative to the expected scale. A smaller value means a more aggressive clipping.</p> <p>For \(\mathbf{u}_t\), we can adaptively estimate the expected threshold scale for each parameter, via the EMA estimate</p> \[\hat{\mathbf{u}}_t = \beta_a \, \hat{\mathbf{u}}_{t-1} + (1 - \beta_a)\,|{\mathbf{u}}_t|,\] <p>where \(\hat{\mathbf{u}}_t\) adapts to the typical scale of \(\mathbf{u}_t\) in each layer.</p><hr /> <h4 id="2-min-clipping-and-max-normalization"> <a href="#2-min-clipping-and-max-normalization" class="anchor-heading" aria-labelledby="2-min-clipping-and-max-normalization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Min-Clipping and Max-Normalization </h4> <p>Numerically, we want to ensure the estimate for the partial-correlation numerator stays within a predictable, and reasonable range, while ensuring \(\alpha_t \ge 0\). This gives us a min-bound</p> \[\mathbb{E}[\mathbf{w}_t \,\bar{\mathbf{g}}_t] \ge \sqrt{\mathbb{E}[{\mathbf{g}}_t^2]}.\] <p>Also, from the inequality \(0 \le (\mathbf{w}_t-\bar{\mathbf{g}}_t)^2\), we have that</p> <p>\(\mathbf{w}_t \,\bar{\mathbf{g}}_t \ \le\ \frac{1}{2}\,\big(\mathbf{w}_t^2 + \bar{\mathbf{g}}_t^2\big),\) and so obtain the max-bound</p> \[\mathbb{E}[\mathbf{w}_t \,\bar{\mathbf{g}}_t] \le \,\mathbb{E}[\mathbf{w}_t^2] + \mathbb{E}[\bar{\mathbf{g}}_t^2] = \mathbb{E}[\mathbf{w}_t^2] + 1.\] <p>\(\mathbb{E}[\mathbf{w}_t^2]\) can be realized by maintaining an EMA estimate \(\mathbf{s}_t = \beta_a \,\mathbf{s}_{t-1} + (1 - \beta_a) \,\mathbf{w}_t^2,\) and define \(\bar{\mathbf{s}}_t = (1 + \mathbf{s}_t)^{-1}\).</p><hr /> <p>Taken together, these practical techniques, helps the partial correlation estimate to remain within a predictable dynamic range (via an elementwise min. and max. operation) and helps to avoid a transient spike dominated EMA estimate (via Huber clipping), leading to</p> \[\tilde{\mathbf{a}}_t = \beta_a \, \tilde{\mathbf{a}}_{t-1} + \mu\, \bar{\mathbf{s}}_t\cdot{\psi_{c} (\mathbf{u}_t)}\] \[\mathbf{a}_t = \max\bigl(\mu\,\bar{\mathbf{s}}_t\cdot\mathbf{d}_t,\, \min\bigl( |\tilde{\mathbf{a}}_t|, \, \mu \bigr) \bigr).\] <h4 id="3-layer-wise-smoothing"> <a href="#3-layer-wise-smoothing" class="anchor-heading" aria-labelledby="3-layer-wise-smoothing"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Layer-wise smoothing </h4> <p>In addition, by replacing the resulting \(\mathbf{a}_t\) estimates with their layerwise mean, we can ensure smoother layer-wise adaptation of the parameters in each layer. For each layer \(\ell\), with parameter size \(n_\ell\),</p> \[\bar{a}_t = {n_\ell}^{-1} \sum_{i=1}^{n_\ell} \mathbf{a}_{t,i}, \quad \mathbf{a}_{t,i} \leftarrow \bar{a}_t \ \ \forall i \in \ell.\]<hr /> <h3 id="partial-correlation-relaxed-upper-bound-variant"> <a href="#partial-correlation-relaxed-upper-bound-variant" class="anchor-heading" aria-labelledby="partial-correlation-relaxed-upper-bound-variant"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Partial-correlation: Relaxed Upper-bound variant </h3> <p>Since \(\mathbf{u}_t = \mathbf{w}_t \,\bar{\mathbf{g}}_t \ \le\ \frac{1}{2}\,\big(\mathbf{w}_t^2 + \bar{\mathbf{g}}_t^2\big),\)</p> <p>we may replace \(\mathbf{u}_t\) with a relaxed form of the upper-bound, that is a symmetric term \(\tilde{\mathbf{u}}_t = \mathbf{w}_t^2 + \bar{\mathbf{g}}_t^2\), weighted by \(\mu &lt; \frac{1}{2}\). Then a simpler estimate than the previous variant becomes</p> \[\tilde{\mathbf{a}}_t = \beta_a \,\tilde{\mathbf{a}}_{t-1} + (1 - \beta_a) \,\mu\,\tilde{\mathbf{u}}_t.\] \[\mathbf{a}_t = \min\bigl( |\tilde{\mathbf{a}}_t|, \, \mu \bigr).\] <p>Layer wise smoothing can be applied next.</p><hr /> <h2 id="-unifying-phb-nag-and-adam"> <a href="#-unifying-phb-nag-and-adam" class="anchor-heading" aria-labelledby="-unifying-phb-nag-and-adam"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🧩 Unifying PHB, NAG, and Adam </h2> <p>By choosing \(\beta, \gamma, \alpha_t\) appropriately, AutoSGM recovers:</p> <div class="table-wrapper"><table> <thead> <tr> <th>Algorithm</th> <th>Parameters</th> </tr> </thead> <tbody> <tr> <td>PHB</td> <td>\(\gamma = 0, \ \eta = 1, \ \alpha_t = \mu\)</td> </tr> <tr> <td>NAG</td> <td>\(\gamma = \frac{\beta}{1+\beta}, \ \eta = 1+\beta, \ \alpha_t = \mu\)</td> </tr> <tr> <td>Adam</td> <td>\(\gamma = 0, \ \eta = 1 - \beta, \ \alpha_t = \frac{\mu}{\mathbf{d}_t}\)</td> </tr> </tbody> </table></div><hr /> <h2 id="-lowpass-regularization"> <a href="#-lowpass-regularization" class="anchor-heading" aria-labelledby="-lowpass-regularization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🎯 Lowpass Regularization </h2> <p>Incorporating momentum is known to practically help stabilize learning dynamics and avoid <strong>shallow local minima</strong> <a class="citation" href="#haykinNeuralNetworksLearning2008">(Haykin, 2008)</a>.<br /> In the paper, we use the impulse response of the filter to show that smoothing the gradient (also called momentum) is <strong>approximately equivalent</strong> to smoothing the loss surface:</p> <p>This <strong>Lowpass regularization</strong> due to smoothing the gradient reflects the stabilized training effect of:</p> <ul> <li>reduced noise in the gradient updates,</li> <li>improved convergence to <strong>flatter local minima</strong>,</li> </ul> <p>often observed.</p><hr /> <h2 id="-key-empirical-findings"> <a href="#-key-empirical-findings" class="anchor-heading" aria-labelledby="-key-empirical-findings"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📊 Key Empirical Findings </h2> <p>Using Adam as a fixed-rate baseline, we tested our AutoSGM realization on <strong>CIFAR-10</strong> (ViT, ResNet) and <strong>language modeling</strong> (GPT-2 on WikiText and Shakespeare):</p> <ul> <li><strong>Tuning the filter’s zero</strong> \(\gamma\) away from 0 improved performance in most cases.</li> <li><strong>Iteration-dependent learning rates</strong> outperformed fixed rates.</li> <li>Best results often occurred for \(\gamma \in [0.31, 0.55]\) in the tested range.</li> </ul> <p>Example:</p> <ul> <li>ViT on CIFAR-10: <strong>+3% accuracy</strong> over fixed-rate baseline.</li> <li>GPT-2 on Shakespeare: <strong>~32% lower test loss</strong> over fixed-rate baseline.</li> </ul><hr /> <h2 id="-conclusion"> <a href="#-conclusion" class="anchor-heading" aria-labelledby="-conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🏁 Conclusion </h2> <p>AutoSGM offers a <strong>unified, interpretable, and tunable</strong> framework for momentum-based optimization.<br /> By viewing PHB, NAG, and Adam as points in the <strong>AutoSGM parameter space</strong>, we can:</p> <ul> <li>Design <strong>new stochastic gradient optimizers</strong> with principled stability and error bounds.</li> <li>Achieve <strong>better generalization</strong> through lowpass regularization.</li> <li>Improve <strong>learning rate algorithms</strong>.</li> </ul><hr /> <p>💡 <em>Takeaway:</em> If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. <strong>AutoSGM gives you the structure or map</strong>.</p><hr /> <ol class="bibliography"><li><span id="Menon2020GradientClipping">Menon, A. K., Rawat, A. S., Reddi, S. J., &amp; Kumar, S. (2020). Can Gradient Clipping Mitigate Label Noise? <i>Proceedings of the 8th International Conference on Learning Representations</i>.</span></li> <li><span id="haykinNeuralNetworksLearning2008">Haykin, S. (2008). <i>Neural Networks and Learning Machines</i> (3rd edition). Pearson.</span></li></ol> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
