<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(3)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(3) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(3) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(3) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Trust-region Optimal Learning rates | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Trust-region Optimal Learning rates" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Optimal Learning Rate Functions Minimizing Trust-region Subproblems." /> <meta property="og:description" content="Optimal Learning Rate Functions Minimizing Trust-region Subproblems." /> <link rel="canonical" href="http://localhost:4000/asgm_cjg" /> <meta property="og:url" content="http://localhost:4000/asgm_cjg" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Trust-region Optimal Learning rates" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"Optimal Learning Rate Functions Minimizing Trust-region Subproblems.","headline":"Trust-region Optimal Learning rates","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm_cjg"},"url":"http://localhost:4000/asgm_cjg"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing, and control in learning and optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods</a><ul class="nav-list"><li class="nav-list-item"><a href="/lpf_not_ema" class="nav-list-link">Momentum is not an EMA</a></li><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Smooth Learning Dynamics</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Trust-region Optimal Learning rates</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Conjugated Directions</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/lrwinds.html" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> ¬© 2026. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/asgm.html">AutoSGM: Unifying Momentum Methods</a></li> <li class="breadcrumb-nav-list-item"><span>Trust-region Optimal Learning rates</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="optimal-learning-rate-functions"> <a href="#optimal-learning-rate-functions" class="anchor-heading" aria-labelledby="optimal-learning-rate-functions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimal Learning Rate Functions </h1> <p class="fs-6 fw-300">Minimizing Trust-region Subproblems.</p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2">Page created: Sep 11 2025 at 12:00 AM</p> </div> <blockquote> <p>We show here that the automatic learning rate perspective in the AutoSGM framework captures several variants of the stochastic gradient learning that have been viewed as preconditioning methods.</p> </blockquote> <p>Recall, that AutoSGM reframes stochastic gradient optimizers through the lens of a <strong>first-order lowpass filter</strong> applied to a stochastic gradient, and the <strong>existence</strong> of an <strong>optimal</strong> iteration-dependent <strong>learning rate</strong> choice.</p><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#optimal-learning-rate-functions" id="markdown-toc-optimal-learning-rate-functions">Optimal Learning Rate Functions</a> <ol> <li><a href="#an-iteration-dependent-learning-rate-oracle" id="markdown-toc-an-iteration-dependent-learning-rate-oracle">An Iteration-dependent Learning-rate Oracle</a></li> <li><a href="#-an-optimal-learning-rate" id="markdown-toc--an-optimal-learning-rate">üìê An Optimal Learning Rate</a> <ol> <li><a href="#practical-approximation" id="markdown-toc-practical-approximation">Practical Approximation</a> <ol> <li><a href="#ema-realizations" id="markdown-toc-ema-realizations">EMA Realizations</a></li> </ol> </li> <li><a href="#robust-ema-estimation" id="markdown-toc-robust-ema-estimation">Robust EMA estimation</a> <ol> <li><a href="#1-input-clipping" id="markdown-toc-1-input-clipping">1. Input Clipping</a></li> <li><a href="#2-output-clipping-and-max-normalization" id="markdown-toc-2-output-clipping-and-max-normalization">2. Output Clipping and Max-Normalization</a></li> <li><a href="#3-layer-wise-average" id="markdown-toc-3-layer-wise-average">3. Layer-wise Average</a></li> </ol> </li> </ol> </li> </ol> </li> </ol> </details><hr /> <h4 id="an-iteration-dependent-learning-rate-oracle"> <a href="#an-iteration-dependent-learning-rate-oracle" class="anchor-heading" aria-labelledby="an-iteration-dependent-learning-rate-oracle"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> An Iteration-dependent Learning-rate Oracle </h4> <p>The <em>iteration-dependent denominator</em> part \(\gain{\mathbf{d}[(t,i)]}\) of an optimal choice of learning rate is an <strong>adaptive moment estimator</strong> of</p> <p class="text-center">\(\sqrt{\mathbb{E}[\statez{\mathbf{g}[t,i]}^2]}\).</p> <p>The <em>iteration-dependent numerator</em> part \(\gain{\mathbf{a}[(t,i)]}\) of an optimal choice of learning rate is a <strong>partial correlation estimator</strong> of</p> <p class="text-center">\({\mathbb{E}[\statex{\mathbf{w}[t,i]}\statez{\bar{\mathbf{g}}[t,i]}]}\),</p> <p>where \(\statez{\bar{\mathbf{g}}[t,i]} = \statez{\mathbf{g}[t,i]}/\gain{\mathbf{d}[(t,i)]}\).</p><hr /> <h2 id="-an-optimal-learning-rate"> <a href="#-an-optimal-learning-rate" class="anchor-heading" aria-labelledby="-an-optimal-learning-rate"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> üìê An Optimal Learning Rate </h2> <p>Assuming that both the training objective function and its gradient are Lipschitz continuous <a class="citation" href="#bottouOptimizationMethodsLargescale2018">(Bottou et al., 2018)</a>, and that the objective function admits an underlying log-likelihood interpretation.</p> <p>To derive an optimal learning rate, let \(\mathbb{E}\) denote expectation with respect to a model distribution \(p(\mathbf{w})\) parameterized by \(\mathbf{w}\). For an explicitly defined log-likelihood objective \(f=\ln p(\mathbf{w})\), the <strong>score-function</strong> identity, tells us that the expected gradient is zero for all \(\mathbf{w}\), not only at the optimum <a class="citation" href="#moonMathematicalMethodsAlgorithms2000">(Moon &amp; Stirling, 2000; Van Trees et al., 2013)</a>. Formally, \(\mathbb{E}[\mathbf{g}(t,i)] = 0\).</p> <blockquote> <p>In practice, many widely used training objectives admit log‚Äëlikelihood interpretations but differ from this simplified model.</p> </blockquote> <p>Using this model, define \(\mathbf{e}(t,i) = \mathbf{w}(t,i) - {\mathbf{w}(i)}^\star\) as the parameter error, the gap between current weight and a local optimum. Minimizing the expected squared error \(\mathbb{E}[\mathbf{e}(t+1,i)^2]\), at iteration \(t\), yields a closed-form expression for an <strong>iteration-dependent optimal learning rate</strong></p> \[\alpha(t,i)^\star = \frac{\mathbb{E}[\mathbf{w}(t,i) \,\mathbf{g}(t,i)]}{\mathbb{E}[\mathbf{g}(t,i)^2]},\] <p>This learning rate is the ratio of two expectation functions:</p> <ul> <li>numerator term is the <strong>partial-correlation</strong> between the weight and gradient.</li> <li>denominator term is the <strong>second moment</strong> (variance) of the gradient.</li> </ul> <p>This learning rate choice is locally-optimal at each iteration. In general, for our actual training objective functions, these expectations are unknown. Nevertheless, the learning rate can be realized in practice by iteratively approximating the expectations in the numerator and denominator terms.</p><hr /> <h3 id="practical-approximation"> <a href="#practical-approximation" class="anchor-heading" aria-labelledby="practical-approximation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Approximation </h3> <p>The derived optimal learning‚Äërate function can be realized using standard adaptive‚Äëfiltering techniques <a class="citation" href="#dinizAdaptiveFilteringAlgorithms2020">(Diniz, 2020; Haykin, 2014)</a>, which involve the following steps:</p> <ol> <li>Expectations are estimated with <strong>exponential moving averages (EMA)</strong>.</li> <li>For numerical stability, we use the <strong>normalized gradient</strong> form.</li> <li>As a safety margin, the locally-optimal iteration-dependent learning rate estimate is modulated with a small \(\mu\digamma(t)\) which acts as its trust-region variable.</li> </ol> <p>Let \(0 \le \mu\digamma(t) \le 1\), where \(\mu &gt; 0\), \(0\le \digamma(t) \le 1\) is a learning-rate schedule. Define</p> \[\bar{\mathbf{g}}(t,i) = \frac{\mathbf{g}(t,i)}{\sqrt{\mathbb{E}[\mathbf{g}(t,i)^2]}},\] <p>where \(\bar{\mathbf{g}}(t,i)\) is the normalized gradient scaled to its unit root-mean-square (RMS) value. The learning rate becomes</p> \[\alpha(t,i) = \mu \digamma(t)\, \frac{\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)]}{\sqrt{\mathbb{E}[\mathbf{g}(t,i)^2]}}.\]<hr /> <h4 id="ema-realizations"> <a href="#ema-realizations" class="anchor-heading" aria-labelledby="ema-realizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> EMA Realizations </h4> <p>Track the denominator term (moment estimation):</p> \[\mathbf{b}(t,i) = \beta_b \,\mathbf{b}(t-1,i) + (1 - \beta_b) \,\mathbf{g}(t,i)^2,\] <p>and define the RMS-normalizer:</p> \[\mathbf{d}(t,i) = \sqrt{\frac{\mathbf{b}(t,i)}{1 - \beta_b^t}} + \epsilon\] <p>‚Üí bias-corrected RMS-norm with small \(\epsilon\) <a class="citation" href="#honigAdaptiveFiltersStructures1984">(Honig &amp; Messerschmitt, 1984)</a> to prevent division by zero.</p> <p>Track the numerator term:</p> \[\mathbf{a}(t,i) = \beta_a \,\mathbf{a}(t-1,i) + \mu \,\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)\] <p>‚Üí a naive running estimate of the weight‚Äìgradient correlation.</p> <p>Finally:</p> \[\alpha(t,i) = \digamma(t)\,\frac{\mathbf{a}(t,i)}{\mathbf{d}(t,i)}.\] <blockquote> <p><strong>Note:</strong> This learning rate function reduces to only <strong>adaptive moment estimation</strong> when \(\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)]\) is replaced by a fixed constant \(1\).</p> </blockquote> \[\alpha(t,i) = \mu\digamma(t)\,\frac{1}{\mathbf{d}(t,i)}.\]<hr /> <h3 id="robust-ema-estimation"> <a href="#robust-ema-estimation" class="anchor-heading" aria-labelledby="robust-ema-estimation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Robust EMA estimation </h3> <p>In practice, one of the essential properties of a statistical estimator is <strong>robustness</strong>. It describes the resistance property of the estimator against outliers. The robustness of an estimator can be expressed using its <strong>breakdown point</strong> (bigger is better). The breakdown point is the proportion of corrupted inputs that the estimator can handle before outputing an incorrect estimate, and it cannot exceed <strong>0.5</strong> <a class="citation" href="#zoubirRobustEstimationSignal2012">(Zoubir et al., 2012)</a>. The <strong>breakdown point</strong> of the <strong>EMA</strong> is 0 <a class="citation" href="#zoubirRobustEstimationSignal2012">(Zoubir et al., 2012)</a>. This implies only a single corrupt input sample-point is needed to significantly distort its estimate <a class="citation" href="#huberRobustEstimationLocation1992">(Huber, 1992)</a>.</p> <p>The <em>denominator EMA term</em> of the learning rate can be interpreted as a norm of the input gradient signal, serving as a measure of its energy or magnitude <a class="citation" href="#boydLinearControllerDesign1991b">(Boyd &amp; Barratt, 1991)</a>. By normalizing the update through division by this gradient norm, the learning rule becomes scale‚Äënormalized, always adjusted relative to the effective strength of the input. As a result, even when the squared gradient input to the denominator EMA is corrupted by heavy‚Äëtailed noise or occasional outliers, the normalization absorbs these effects. Extreme values in the gradient are proportionally scaled down, preventing instability and ensuring that the update remains bounded and robust.</p> <p>However, the same cannot be said for the <em>numerator EMA term</em>. Whereas the denominator term acts as a norm of the gradient signal and thus provides scale‚Äënormalized robustness, the numerator term directly involves the correlation between a weight and a gradient component. This correlation is inherently more sensitive to noise and outliers: if the weight-gradient product is corrupted, the output of the numerator EMA can be distorted in both magnitude and sign. Unlike the denominator, which absorbs extreme values through normalization, the numerator reflects them directly, potentially leading to erratic updates. In practice, this means that while the denominator stabilizes the learning rate by bounding its scale, the numerator remains the primary channel through which input variability and heavy‚Äëtailed disturbances distort the update step.</p> <p>Classic mean estimators like the EMA assume a well-behaved noise model <a class="citation" href="#huberRobustEstimationLocation1992">(Huber, 1992; Zoubir et al., 2012)</a>. <strong>Heavy‚Äëtailed stochastic correlations, noisy sign flips and occasional magnitude spikes can break this assumption</strong> <a class="citation" href="#zoubirRobustStatisticsSignal2018a">(Zoubir et al., 2018)</a> leading to breakdown. In other words, heavy-tailed gradient noise statistics induce misleading spikes that can dominate the EMA‚Äôs estimate over many iterations by increasing its bias from the true mean estimate.</p> <blockquote> <p>To handle such problems, a common approach in the robust estimation of location from data is to essentially apply <strong>concentration inequality</strong> techniques that <strong>detect</strong> if the input to the mean estimator is suspicious (an outlier), then <strong>replace</strong> (or <strong>clip</strong>) with an appropriate value using a measure of scale <a class="citation" href="#zoubirRobustEstimationSignal2012">(Zoubir et al., 2012; Zoubir et al., 2018)</a>.</p> </blockquote> <p>In this case, we want the estimate of the <strong>numerator EMA</strong> to remain positive, well‚Äëbounded, and avoid corruptions due to noisy, heavy-tailed inputs. Limiting the influence of outliers while preserving sensitivity to smaller signals. In other words, we want to robustify the estimate from the numerator EMA without distorting the bulk of the signal observed via its input \(\mathbf{u}(t,i) = \mathbf{w}(t,i) \, \bar{\mathbf{g}}(t,i)\)</p> <h4 id="1-input-clipping"> <a href="#1-input-clipping" class="anchor-heading" aria-labelledby="1-input-clipping"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Input Clipping </h4> <p>Since, we do not know the probability distribution, <strong>Markov‚Äôs inequality</strong> gives a rationale for how often large such values can occur. Let \(u\) denote an instantaneous input signal, and \(c &gt; 0\) be a scale constant. Markov‚Äôs inequality</p> \[\mathbb{Pr}[|u| \ge c\,\mathbb{E}[|u|] ] \le \frac{1}{c},\] <p>relates how large the magnitude of \(u\) can be relative to its expected magnitude \(m = \mathbb{E} \bigl[|u|\bigr]\).</p> <!-- **Soft limiting** is a practical way to robustly mitigate such heavy-tailed values. --> <p>By utilizing Markov‚Äôs inequality, instead of naively passing an input \(u\) though the EMA, the <strong>Huber clipping function</strong> \(\psi_{c}(u)\) <a class="citation" href="#zoubirRobustEstimationSignal2012">(Zoubir et al., 2012; Menon et al., 2020)</a> can be used to detect the most extreme outliers (\(&gt; c\times m\)) and replace with \(c \times m\) before they are processed by the EMA. This avoids signal dead-zones of zero and allows moderate estimates to pass through untouched relative to the expected magnitude.</p> \[\psi_{c}(u) = \begin{cases} u, &amp; |u| \le c\,\mathbb{E}[|u|] \\ \mathrm{sign}(u) \cdot c\,\mathbb{E}[|u|], &amp; |u| &gt; c\,\mathbb{E}[|u|]. \end{cases}\] <p>The scale constant \(c\) is used to clip the extreme outliers relative to \(m\) For example, \(c=4\) can be viewed as a <em>prior</em> that the probability \(p\) of the magnitude \(|u|\) exceeding four times its expectation is no more than \(25\%\). Equivalently, the probability that \(|u|\) remains below this threshold is at least \(1-p=75\%\). Therefore, the interval defined by the 25‚Äì75% quantiles capture a good percentage of the distribution, while the clipping function suppresses only the most extreme values. This yields a more <strong>robust EMA estimator</strong> that is less sensitive to heavy‚Äëtailed noise and spurious magnitude spikes.</p> <!-- , and so the probability of the magnitude being less than $$4$$ times its mean is at least $$1-p=75\%$$. T --> <p>In the case of \(\mathbf{u}(t,i)\), we can iteratively estimate its expected magnitude, via the EMA estimate</p> \[\hat{\mathbf{u}}(t,i) = \beta_a \, \hat{\mathbf{u}}(t-1,i) + (1 - \beta_a)\,|{\mathbf{u}}(t,i)|,\] <p>where \(\hat{\mathbf{u}}(t,i)\) adapts to the typical scale of \(\mathbf{u}(t,i)\) in each layer.</p> <h4 id="2-output-clipping-and-max-normalization"> <a href="#2-output-clipping-and-max-normalization" class="anchor-heading" aria-labelledby="2-output-clipping-and-max-normalization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Output Clipping and Max-Normalization </h4> <p>Furthermore, small noisy inputs that slip through the input-clipping function can flip the signs of numerator EMA‚Äôs estimate. These <strong>spurious sign flips</strong> distort the learning‚Äërate ratio, sometimes inflating it and other times collapsing it to near zero. A simple input‚Äëclipping strategy cannot prevent this problem: when the estimate spuriously turns negative, clipping to zero halts progress entirely.</p> <p>Since the global learning‚Äërate constant \(\mu\) already serves as a safety margin (a trust‚Äëregion) for the locally-optimal learning rate, a more robust limiting approach that utilizes \(\mu\) is needed. Drawing on robust filtering principles <a class="citation" href="#zoubirRobustStatisticsSignal2018a">(Zoubir et al., 2018)</a>, one approach is to <strong>preserve sign information</strong> only within the EMA update via the input clipping, while at the output stage, a <strong>magnitude-only bound</strong> is applied to produce the final estimate.</p> <p>To achieve this, let \(s\) be the upper-bound on a signal \(u\). We can define a second clipping function \(\psi_{\mu}(u)\) that allows small values of \(|u|\) to pass unchanged, but limits large values to the threshold \(s\) scaled by \(\mu\).</p> \[\psi_{\mu}(u) = \begin{cases} |u|, &amp; |u| \le \mu\,s \\ \mu\,\mathbb{E}[|u|], &amp; |u| &gt; \mu\,s. \end{cases}\] <p>In our case, for the inequality \(0 \le (\mathbf{w}(t,i)-\bar{\mathbf{g}}(t,i))^2\), we have that \(\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) \ \le\ \frac{1}{2}\,\big(\mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\big),\) and so obtain the upper-bound for \(\mathbf{u}(t,i)\)</p> \[\mathbb{E}[\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i)] \le \,\mathbb{E}[\mathbf{w}(t,i)^2] + \mathbb{E}[\bar{\mathbf{g}}(t,i)^2] = \mathbb{E}[\mathbf{w}(t,i)^2] + 1.\] <p>To realize a dynamic max-threshold, we can realize \(\mathbb{E}[\mathbf{w}(t,i)^2]\) by maintaining an EMA estimate</p> \[\mathbf{s}(t,i) = \beta_a \,\mathbf{s}(t-1,i) + (1 - \beta_a) \,\mathbf{w}(t,i)^2,\] <p>then define the max-normalizer as \(\bar{\mathbf{s}}(t,i) = 1 + \mathbf{s}(t,i)\).</p> <p>We can now realize a robust numerator EMA estimate as follows:</p> \[\tilde{\mathbf{a}}(t,i) = \beta_a \, \tilde{\mathbf{a}}(t-1,i) + \mu\, \bar{\mathbf{s}}(t,i)^{-1}\cdot{\psi_{c} (\mathbf{u}(t,i))},\] \[\mathbf{a}(t,i) = \psi_{\mu}\bigl(\tilde{\mathbf{a}}(t,i)\bigr).\] <!-- $$ \mathbf{a}(t,i) = \max\bigl(\mu\,\bar{\mathbf{s}}(t,i)^{-1}\cdot\mathbf{d}(t,i),\, \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu \bigr) \bigr). $$ --> <p>Taken together, the small \(\epsilon\), the normalizers \(\bar{\mathbf{s}}(t,i)\) and \(\mathbf{d}(t,i)\) and robust input-output clipping functions help us realize a learning rate estimate that remains within a predictable dynamic range, while preventing large values that can lead to <strong>breakdown</strong>.</p> <blockquote> <p>In terms of implementation, note that \(\psi_{c}\bigl(u\bigr) = \mathrm{sign}(u) \cdot \min\bigl( |u|, \, c\,m\bigr),\) and \(\psi_{\mu}\bigl(u\bigr) = \min\bigl( |u|, \, \mu\,s\bigr).\)</p> </blockquote> <h4 id="3-layer-wise-average"> <a href="#3-layer-wise-average" class="anchor-heading" aria-labelledby="3-layer-wise-average"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Layer-wise Average </h4> <p>In addition, to account for intra-layer structure and variability in deep neural networks, we observed that replacing the raw \(\mathbf{a}(t,i)\) estimates with their layerwise mean further ensured more numerically stable and uniform parameter adaptation within each layer. Specifically, for a given layer \(\ell\), with parameter size \(n_\ell\), the numerator estimates are averaged to yield a uniform estimate:</p> \[\mathbf{a}(t,i) ‚Üê \frac{1}{n_\ell} \sum_{i=1}^{n_\ell} \mathbf{a}(t,i).\] <!-- ### Alternatives: Relaxed Upper-bound variant Since $$ \mathbf{u}(t,i) = \mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) \ \le\ \frac{1}{2}\,\big(\mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2\big), $$ we may replace $$\mathbf{u}(t,i)$$ with a relaxed form of the symmetric upper-bound, $$ \tilde{\mathbf{u}}(t,i) = \mathbf{w}(t,i)^2 + \bar{\mathbf{g}}(t,i)^2$$, weighted by $$ \mu < \frac{1}{2} $$. A proxy estimate of the partial-correlation then becomes $$ \tilde{\mathbf{a}}(t,i) = \beta_a \,\tilde{\mathbf{a}}(t-1,i) + (1 - \beta_a) \,\mu\,\tilde{\mathbf{u}}(t,i). $$ $$ \mathbf{a}(t,i) = \min\bigl( |\tilde{\mathbf{a}}(t,i)|, \, \mu \bigr). $$ Layer-wise smoothing can be applied next. --> <!-- --- --><hr /> <ol class="bibliography"><li><span id="bottouOptimizationMethodsLargescale2018">Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. <i>SIAM Review</i>, <i>60</i>(2), 223‚Äì311.</span></li> <li><span id="moonMathematicalMethodsAlgorithms2000">Moon, T. K., &amp; Stirling, W. C. (2000). <i>Mathematical Methods and Algorithms for Signal Processing</i>. Prentice Hall.</span></li> <li><span id="vantreesDetectionEstimationModulation2013">Van Trees, H. L., Bell, K. L., &amp; Tian, Z. (2013). <i>Detection Estimation and Modulation Theory, Detection, Estimation, and Filtering Theory, Part I</i> (2nd ed.). Wiley.</span></li> <li><span id="dinizAdaptiveFilteringAlgorithms2020">Diniz, P. S. R. (2020). <i>Adaptive Filtering: Algorithms and Practical Implementation</i> (5th edition). Springer International Publishing. https://doi.org/10.1007/978-3-030-29057-3</span></li> <li><span id="haykinAdaptiveFilterTheory2014">Haykin, S. (2014). <i>Adaptive Filter Theory</i> (5th, intern.). Pearson.</span></li> <li><span id="honigAdaptiveFiltersStructures1984">Honig, M. L., &amp; Messerschmitt, D. G. (1984). <i>Adaptive Filters: Structures, Algorithms and Applications</i>. Kluwer Academic Publishers.</span></li> <li><span id="zoubirRobustEstimationSignal2012">Zoubir, A. M., Koivunen, V., Chakhchoukh, Y., &amp; Muma, M. (2012). Robust Estimation in Signal Processing: A Tutorial-Style Treatment of Fundamental Concepts. <i>IEEE Signal Processing Magazine</i>, <i>29</i>(4), 61‚Äì80.</span></li> <li><span id="huberRobustEstimationLocation1992">Huber, P. J. (1992). Robust Estimation of a Location Parameter. In S. Kotz &amp; N. L. Johnson (Eds.), <i>Breakthroughs in Statistics: Methodology and Distribution</i> (pp. 492‚Äì518). Springer. https://doi.org/10.1007/978-1-4612-4380-9_35</span></li> <li><span id="boydLinearControllerDesign1991b">Boyd, S., &amp; Barratt, C. (1991). <i>Linear Controller Design: Limits of Performance</i>. Prentice Hall.</span></li> <li><span id="zoubirRobustStatisticsSignal2018a">Zoubir, A. M., Koivunen, V., Ollila, E., &amp; Muma, M. (2018). <i>Robust Statistics for Signal Processing</i> (1st ed.). Cambridge University Press. https://doi.org/10.1017/9781139084291</span></li> <li><span id="Menon2020GradientClipping">Menon, A. K., Rawat, A. S., Reddi, S. J., &amp; Kumar, S. (2020). Can Gradient Clipping Mitigate Label Noise? <i>Proceedings of the 8th International Conference on Learning Representations</i>.</span></li></ol><hr /> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2"> Page last modified: <span class="d-inline-block">Dec 24 2025 at 12:00 AM</span>. </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
