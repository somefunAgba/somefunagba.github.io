<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(1)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(1) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Smooth Learning Dynamics | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Smooth Learning Dynamics" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Stochastic Gradient Learning Dynamics Linear Time (Iteration) Varying System." /> <meta property="og:description" content="Stochastic Gradient Learning Dynamics Linear Time (Iteration) Varying System." /> <link rel="canonical" href="http://localhost:4000/learning_dynamics" /> <meta property="og:url" content="http://localhost:4000/learning_dynamics" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Smooth Learning Dynamics" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Stochastic Gradient Learning Dynamics Linear Time (Iteration) Varying System.","headline":"Smooth Learning Dynamics","url":"http://localhost:4000/learning_dynamics"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ <!-- TROLRS --> $$ \newcommand{\trobjsca}{\mathcal{D}[t,i]} \newcommand{\trobjmat}{\mathcal{D}[t]} \newcommand{\Tr}{\mathrm{Tr}} \newcommand{\step}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{g}[t,i]} \newcommand{\stepv}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{v}[t,i]} \newcommand{\matstep}{\Delta[t+1] = -\alpha[t]\,\mathbf{g}[t]} \newcommand{\matstepv}{\Delta[t+1] = -\alpha[t]\,\mathbf{v}[t]} \newcommand{\ngrad}{\bar{\mathbf{g}}[t,i]} \newcommand{\ngradv}{\bar{\mathbf{v}}[t,i]} \newcommand{\ngradsq}{\bar{\mathbf{g}}^2[t,i]} \newcommand{\ngradvsq}{\bar{\mathbf{v}}^2[t,i]} \newcommand{\nmatgrad}{\bar{\mathbf{g}}[t]} \newcommand{\nmatgradv}{\bar{\mathbf{v}}[t]} \newcommand{\fof}{\mathbb{H}_{\beta,\,\gamma}} \newcommand{\expg}{\mathbb{E}\big[\mathbf{g}[t,i]\big]} \newcommand{\expv}{\mathbb{E}\big[\mathbf{v}[t,i]\big]} $$ $$ \newcommand{\stepmom}{\mathbb{E}\big[{\Delta}^2[t+1, i]\big]} \newcommand{\matstepmom}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\Delta[t+1]\big]} \newcommand{\gmatstepmom}{\mathbb{E}\big[\Tr\big(\left<{\Delta[t+1],\Delta[t+1]}\right>\big)\big]} $$ $$ \newcommand{\stepcorrng}{\mathbb{E}\big[\Delta[t+1, i]\,\ngrad\big]} \newcommand{\stepcorrngv}{\mathbb{E}\big[\Delta[t+1, i]\,\ngradv\big]} \newcommand{\matstepcorrng}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgrad \big) \big]} \newcommand{\matstepcorrngv}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgradv \big) \big]} \newcommand{\gmatstepcorrng}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgrad}\right\rangle \big)\big]} \newcommand{\gmatstepcorrngv}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgradv}\right\rangle \big)\big]} $$ $$ \newcommand{\stepcorru}{\mathbb{E}\big[\Delta[t+1, i]\,\mathbf{u}[t,i]\big]} \newcommand{\matstepcorru}{\mathbb{E}\big[\Delta^\intercal[t+1]\,\mathbf{u}[t]\big]} $$ $$ \newcommand{\numngradcorr}{\mathbb{E}\big[\ngradsq\big]} \newcommand{\numwgcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngrad\big]} \newcommand{\numngradvcorr}{\mathbb{E}\big[\ngradvsq\big]} \newcommand{\numwvcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngradv\big]} \newcommand{\dengmom}{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]} \newcommand{\dengmomv}{\mathbb{E}\big[\mathbf{v}^2[t,i]\big]} \newcommand{\matdengmom}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]} \newcommand{\matdengmomv}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]} \newcommand{\dengmomsqrt}{\sqrt{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]}} \newcommand{\matdengmomsqrt}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matdenvmomsqrt}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matngradcorr}{\mathbb{E}\big[\nmatgrad\nmatgrad^{\intercal}\big]} \newcommand{\matngradvcorr}{\mathbb{E}\big[\nmatgradv\nmatgradv^{\intercal}\big]} \newcommand{\matwngradcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgrad^{\intercal}\big]} \newcommand{\matwngradvcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgradv^{\intercal}\big]} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing, and control in learning and optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods</a><ul class="nav-list"><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Smooth Learning Dynamics</a></li><li class="nav-list-item"><a href="/asgm_trolrs" class="nav-list-link">Trust-region Optimal Learning rates</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Conjugated Directions</a></li><li class="nav-list-item"><a href="/lpf_not_ema" class="nav-list-link">Momentum is not an EMA</a></li><li class="nav-list-item"><a href="/asgm_lrwinds" class="nav-list-link">Learning-Rate Annealing</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2026. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/asgm.html">AutoSGM: Unifying Momentum Methods</a></li> <li class="breadcrumb-nav-list-item"><span>Smooth Learning Dynamics</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="stochastic-gradient-learning-dynamics"> <a href="#stochastic-gradient-learning-dynamics" class="anchor-heading" aria-labelledby="stochastic-gradient-learning-dynamics"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Stochastic Gradient Learning Dynamics </h1> <p class="fs-6 fw-300">Linear Time (Iteration) Varying System.</p> <blockquote> <p><em>Oluwasegun Somefun</em>. “<a href="https://somefunagba.github.io/learning_dynamics.html">Smooth Learning Dynamics</a>.” <em>AutoSGM Framework</em>, 2025.</p> </blockquote> <blockquote> <p><strong>Please cite this page</strong> if you use information from these notes for your work, research, or anything that requires academic or formal citation.</p> </blockquote><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#stochastic-gradient-learning-dynamics" id="markdown-toc-stochastic-gradient-learning-dynamics">Stochastic Gradient Learning Dynamics</a> <ol> <li><a href="#weight-decay" id="markdown-toc-weight-decay">Weight decay</a></li> <li><a href="#a-proximal-subproblem" id="markdown-toc-a-proximal-subproblem">A Proximal Subproblem</a></li> <li><a href="#stability-conditions-and-transient-behavior" id="markdown-toc-stability-conditions-and-transient-behavior">Stability Conditions and Transient Behavior</a></li> </ol> </li> </ol> </details><hr /> <blockquote> <p>The AutoSGM framework exposes the exact update trajectory of each trainable parameter in a gradient-generating system like a deep neural network via the stochastic gradient algorithm under a <strong>lowpass filter</strong> (momentum) and <strong>iteration-dependent learning-rate</strong> oracle as the dynamics of a <strong>first-order linear time (iteration) varying (LTV) filter</strong>.</p> </blockquote> <p>This LTV description makes it possible to apply linear systems, control and signal‑processing tools to reason about stability, transient response, noise attenuation and steady-state convergence tradeoffs.</p><hr /> <p>Formally, at each iteration \(t\), a single parameter \(\mathbf{w}[t, i]\) update via its gradient component \(\mathbf{g}[t, i]\) follows the first-order linear filter trajectory</p> <div class="text-center">\[\boxed{\state{\Delta\mathbf{w}[t+1, i]} = \filter{\beta}\,\gain{r[t, i]}\cdot\state{\Delta\mathbf{w}[t,i]} + \filter{\eta}\,\gain{\alpha[t,i]}\cdot \input{\mathbf{e}[t,i]}},\] </div><hr /> <h4 id="weight-decay"> <a href="#weight-decay" class="anchor-heading" aria-labelledby="weight-decay"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Weight decay </h4> <p>Depending on the weight-decay mechanism, the generated input trajectory for what has been called <strong>decoupled weight-decay</strong></p> <div class="text-center">\[\input{ \mathbf{e}[t, i]} = \bigl(\filter{\gamma}\,\statez{\mathbf{g}[t-1, i]} - \statez{\mathbf{g}[t, i]} \bigr) + \gainx{\rho}\,\filter{\eta^{-1}}\bigl(\filter{\beta} \,\statex{\tilde{\mathbf{w}}[t-1, i]} - \statex{\tilde{\mathbf{w}}[t, i]}\bigr),\] </div> <p>is exactly equivalent to the <strong>standard weight-decay</strong></p> <div class="text-center">\[\input{\mathbf{e}[t, i]} = \bigl(\filter{\gamma}\,\statez{\mathbf{g}[t-1, i]} - \statez{\mathbf{g}[t, i]} \bigr) + \gainx{\rho^{\prime}}\bigl(\filter{\gamma} \,\statex{\mathbf{w}[t-1, i]} - \statex{\mathbf{w}[t, i]} \bigr),\] </div> <p>when \(\gainx{\rho^\prime} = \gainx{\rho}\,\filter{\eta^{-1}}\), \(\filter{\beta}=\filter{\gamma}\), and \(\statex{\tilde{\mathbf{w}}[t, i]} = \statex{\mathbf{w}[t, i]}\).</p><hr /> <p>Finally, by integrating the LTV filter, the actual parameter update is recovered</p> <p class="text-center">\(\boxed{\statex{\mathbf{w}[t+1, i]} = \statex{\mathbf{w}[t,i]} + \state{\Delta \mathbf{w}[t+1,i]}}\),</p> <p>where</p> <ul> <li>\(\gain{\alpha[t,i]} = {\gain{\mu}}\,\gain{\digamma[t}] \cdot \frac{\gain{\mathbf{a}[t,i]}}{\gain{\mathbf{d}[t,i]}}\) is an iteration-dependent learning rate oracle, essentially composed of a trust-region constant \(0 &lt; \gain{\mu} &lt; 1\), a window function \(0 \le \gain{\digamma[t]} \le 1\) as the learning-rate schedule, together with the oracle’s numerator and denominator functions denoted respectively as \(\gain{\mathbf{a}[t,i]}\), \(\gain{\mathbf{d}[t,i]}\).</li> <li>\(\gain{r[t,i]} = \gain{\alpha[t,i]}/\gain{\alpha[t-1,i]}\) is the learning rate ratio,</li> <li>\(\filter{\beta}\) is the lowpass filter’s pole parameter selected for stability \(0 \le \filter{\beta} &lt; 1\),</li> <li>\(\filter{\gamma}\) is the lowpass filter’s zero parameter, selected such that \(\filter{\gamma} &lt; \filter{\beta}\),</li> <li>\(\filter{\eta}\) is a constant selected as \((1-\filter{\beta})/(1-\filter{\gamma})\) such the steady-state (DC) gain of the lowpass filter is unity,</li> <li>\(\gainx{\rho, \rho^\prime} \ge 0\) denote small weight-decay constants, can be selected relative to \(\eta\),</li> <li>\(\statex{\tilde{\mathbf{w}}[t, i]} = \gain{\mathbf{d}[t,i]}\,\statex{\mathbf{w}[t, i]}\) is the scaled parameter via the learning-rate’s denominator.</li> </ul> <p>Under the mild assumptions of local smoothness of the loss function generating the gradient, and bounded gradient moments, <em>the trajectory input \(\input{\mathbf{e}[t, i]}\) is bounded.</em></p> <h3 id="a-proximal-subproblem"> <a href="#a-proximal-subproblem" class="anchor-heading" aria-labelledby="a-proximal-subproblem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Proximal Subproblem </h3> <!-- $$ \boxed{ \Delta\mathbf{w}[t+1, i] = \beta\, r[t, i] \cdot \Delta\mathbf{w}[t,i] + \eta\, \alpha[t,i] \cdot \mathbf{e}[t,i] } $$ --> <p>Any first‑order linear filter like</p> \[\boxed{\state{\Delta\mathbf{w}[t+1, i]} = \filter{\beta}\,\gain{r[t, i]}\cdot\state{\Delta\mathbf{w}[t,i]} + \filter{\eta}\,\gain{\alpha[t,i]}\cdot \input{\mathbf{e}[t,i]}}\] <p>can be written as the solution of a proximal subproblem.</p> <p>This first‑order linear filter is the explicit <strong>closed‑form optimal solution</strong> to a regularized weighted least squares <strong>objective</strong>: that balances the next trajectory step \(\Delta\mathbf{w}[t+1,i]\) between aligned with \(\mathbf{e}[t,i]\), and being close to a weighted version of the previous trajectory step \(\Delta\mathbf{w}[t,i]\) .</p> \[\Delta\mathbf{w}[t+1,i] = \arg\min_{\Delta} Q\bigl(\Delta\bigr)\] \[Q\bigl(\Delta\bigr)= \;-\; \eta\,\Delta \cdot \mathbf{e}[t,i] \;+\; \frac{1}{2\,\alpha[t,i]}\, \Bigl(\Delta - \beta\, r[t,i]\cdot\Delta\mathbf{w}[t,i]\Bigr)^2\] <p>Rewriting as a proximal operator,</p> \[\Delta\mathbf{w}[t+1,i] = \mathbf{prox}_{\gain{\alpha[t,i]},\:\eta\,\Delta \cdot\mathbf{e}[t,i]}\big(\beta\, r[t,i]\cdot\Delta\mathbf{w}[t,i]\big)\] <blockquote> <p>Despite the <strong>underlying deep learning loss being non‑convex</strong>, this per-iteration subproblem is <strong>strongly convex</strong> in \(\Delta\mathbf{w}[t,i]\) with modulus \(\alpha[t,i]^{-1}\), and driven by \(\mathbf{e}[t,i]\).</p> </blockquote> <p>Apart from unification, this LTV filter and its resulting convex quadratic subproblem formulation, concretely highlights</p> <ul> <li> <p><strong>Lowpass regularization:</strong> The smooth path of parameter changes empirically enhances the robustness and generalization of solutions.</p> </li> <li> <p><strong>Stability:</strong> Each parameter-change is the solution of a strongly convex quadratic subproblem. Provided the learning rate at each iteration is finite, this lowpass filter ensures bounded, well‑defined and stable parameter changes.</p> </li> </ul> <p>This makes the design of such practical stochastic graient learning algorithms, more principled, analyzable, and unifying across several methods.</p><hr /> <h2 id="stability-conditions-and-transient-behavior"> <a href="#stability-conditions-and-transient-behavior" class="anchor-heading" aria-labelledby="stability-conditions-and-transient-behavior"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Stability Conditions and Transient Behavior </h2> <p>As stated, the overall filtered SGM dynamics is \(\boxed{\state{\Delta\mathbf{w}[t+1, i]} = \filter{\beta}\,\gain{r[t, i]}\cdot\state{\Delta\mathbf{w}[t,i]} + \filter{\eta}\,\gain{\alpha[t,i]}\cdot \input{\mathbf{e}[t,i]}}\)</p> <ul> <li>The lowpass pole \(\filter{\beta}\) and learning-rate ratio \(r[t,i]\) shape both the the low-frequency properties of the filter (response to slow-changing inputs), and exponential stability margin of the system \(\lvert \filter{\beta^{-1}}\gain{ r[t,i]} \rvert &lt; 1\).</li> </ul> <p>Together, \(\filter{\beta}\) and \(\gain{\alpha[t,i]}\) shape how quickly and smoothly the learning dynamics settle into steady-state.<br /> If selected properly, they ensure bounded and convergent behavior over time.</p> <p>For <strong>BIBO stability</strong>, and uniform <strong>exponential stability</strong> (boundedness and asymptotic behavior of the trajectory solution): the necessary and sufficient conditions are: \(0 &lt; \filter{\beta} &lt; 1\), and \(\sup_t \gain{\alpha[t,i]} &lt; \infty\). Ensures a bounded input \(\input{\mathbf{e}[t,i]}\) leads to a bounded trajectory output \(\state{\Delta\mathbf{w}[t+1, i]}\).</p> <p>In addition, if \(0 &lt; \filter{\beta} &lt; 1\) to improve exponential stability, we need \(\gain{\digamma[t]} \to 0\) as \(t \to \infty\), hich imply both \(\gain{r[t]} \to 0\) and \(\gain{\alpha[t]} \to 0\). <strong>This explains why learning-rate annealing is important</strong>.</p><hr /> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
