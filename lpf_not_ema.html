<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(4)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(4) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(4) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(4) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Momentum is not an EMA | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Momentum is not an EMA" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Lowpass filtering versus Mean estimation via EMAs How Gradient smoothing called Momentum is not an EMA" /> <meta property="og:description" content="Lowpass filtering versus Mean estimation via EMAs How Gradient smoothing called Momentum is not an EMA" /> <link rel="canonical" href="http://localhost:4000/lpf_not_ema" /> <meta property="og:url" content="http://localhost:4000/lpf_not_ema" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Momentum is not an EMA" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Lowpass filtering versus Mean estimation via EMAs How Gradient smoothing called Momentum is not an EMA","headline":"Momentum is not an EMA","url":"http://localhost:4000/lpf_not_ema"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ <!-- TROLRS --> $$ \newcommand{\trobjsca}{\mathcal{D}[t,i]} \newcommand{\trobjmat}{\mathcal{D}[t]} \newcommand{\Tr}{\mathrm{Tr}} \newcommand{\step}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{g}[t,i]} \newcommand{\stepv}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{v}[t,i]} \newcommand{\matstep}{\Delta[t+1] = -\alpha[t]\,\mathbf{g}[t]} \newcommand{\matstepv}{\Delta[t+1] = -\alpha[t]\,\mathbf{v}[t]} \newcommand{\ngrad}{\bar{\mathbf{g}}[t,i]} \newcommand{\ngradv}{\bar{\mathbf{v}}[t,i]} \newcommand{\ngradsq}{\bar{\mathbf{g}}^2[t,i]} \newcommand{\ngradvsq}{\bar{\mathbf{v}}^2[t,i]} \newcommand{\nmatgrad}{\bar{\mathbf{g}}[t]} \newcommand{\nmatgradv}{\bar{\mathbf{v}}[t]} \newcommand{\fof}{\mathbb{H}_{\beta,\,\gamma}} \newcommand{\expg}{\mathbb{E}\big[\mathbf{g}[t,i]\big]} \newcommand{\expv}{\mathbb{E}\big[\mathbf{v}[t,i]\big]} $$ $$ \newcommand{\stepmom}{\mathbb{E}\big[{\Delta}^2[t+1, i]\big]} \newcommand{\matstepmom}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\Delta[t+1]\big]} \newcommand{\gmatstepmom}{\mathbb{E}\big[\Tr\big(\left<{\Delta[t+1],\Delta[t+1]}\right>\big)\big]} $$ $$ \newcommand{\stepcorrng}{\mathbb{E}\big[\Delta[t+1, i]\,\ngrad\big]} \newcommand{\stepcorrngv}{\mathbb{E}\big[\Delta[t+1, i]\,\ngradv\big]} \newcommand{\matstepcorrng}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgrad \big) \big]} \newcommand{\matstepcorrngv}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgradv \big) \big]} \newcommand{\gmatstepcorrng}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgrad}\right\rangle \big)\big]} \newcommand{\gmatstepcorrngv}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgradv}\right\rangle \big)\big]} $$ $$ \newcommand{\stepcorru}{\mathbb{E}\big[\Delta[t+1, i]\,\mathbf{u}[t,i]\big]} \newcommand{\matstepcorru}{\mathbb{E}\big[\Delta^\intercal[t+1]\,\mathbf{u}[t]\big]} $$ $$ \newcommand{\numngradcorr}{\mathbb{E}\big[\ngradsq\big]} \newcommand{\numwgcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngrad\big]} \newcommand{\numngradvcorr}{\mathbb{E}\big[\ngradvsq\big]} \newcommand{\numwvcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngradv\big]} \newcommand{\dengmom}{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]} \newcommand{\dengmomv}{\mathbb{E}\big[\mathbf{v}^2[t,i]\big]} \newcommand{\matdengmom}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]} \newcommand{\matdengmomv}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]} \newcommand{\dengmomsqrt}{\sqrt{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]}} \newcommand{\matdengmomsqrt}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matdenvmomsqrt}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matngradcorr}{\mathbb{E}\big[\nmatgrad\nmatgrad^{\intercal}\big]} \newcommand{\matngradvcorr}{\mathbb{E}\big[\nmatgradv\nmatgradv^{\intercal}\big]} \newcommand{\matwngradcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgrad^{\intercal}\big]} \newcommand{\matwngradvcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgradv^{\intercal}\big]} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing, and control in learning and optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods</a><ul class="nav-list"><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Smooth Learning Dynamics</a></li><li class="nav-list-item"><a href="/asgm_trolrs" class="nav-list-link">Trust-region Optimal Learning rates</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Conjugated Directions</a></li><li class="nav-list-item"><a href="/lpf_not_ema" class="nav-list-link">Momentum is not an EMA</a></li><li class="nav-list-item"><a href="/asgm_lrwinds" class="nav-list-link">Learning-Rate Annealing</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2026. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/asgm.html">AutoSGM: Unifying Momentum Methods</a></li> <li class="breadcrumb-nav-list-item"><span>Momentum is not an EMA</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="lowpass-filtering-versus-mean-estimation-via-emas"> <a href="#lowpass-filtering-versus-mean-estimation-via-emas" class="anchor-heading" aria-labelledby="lowpass-filtering-versus-mean-estimation-via-emas"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lowpass filtering versus Mean estimation via EMAs </h1> <p class="fs-6 fw-300">How Gradient smoothing called Momentum is not an EMA</p> <blockquote> <p><em>Oluwasegun Somefun</em>. “<a href="https://somefunagba.github.io/lpf_not_ema.html">Momentum is not an EMA</a>.” <em>AutoSGM Framework</em>, 2025.</p> </blockquote> <blockquote> <p><strong>Please cite this page</strong> if you use information from these notes for your work, research, or anything that requires academic or formal citation.</p> </blockquote><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#lowpass-filtering-versus-mean-estimation-via-emas" id="markdown-toc-lowpass-filtering-versus-mean-estimation-via-emas">Lowpass filtering versus Mean estimation via EMAs</a> <ol> <li><a href="#the-singlepole-lowpass-filter" id="markdown-toc-the-singlepole-lowpass-filter">The Single‑Pole Low‑Pass Filter</a> <ol> <li><a href="#ema-vs-typical-lowpass-filtering-regimes" id="markdown-toc-ema-vs-typical-lowpass-filtering-regimes">EMA vs. Typical Lowpass filtering Regimes</a></li> <li><a href="#frequencydomain-representation" id="markdown-toc-frequencydomain-representation">Frequency‑Domain Representation</a></li> </ol> </li> <li><a href="#why-a-zero-breaks-ema-behavior" id="markdown-toc-why-a-zero-breaks-ema-behavior">Why a Zero Breaks EMA Behavior</a> <ol> <li><a href="#non-monotone" id="markdown-toc-non-monotone">Non-monotone</a></li> </ol> </li> <li><a href="#takeaways" id="markdown-toc-takeaways">Takeaways</a></li> <li><a href="#references" id="markdown-toc-references">References</a></li> </ol> </li> </ol> </details><hr /> <p>It is tempting to conflate the general <em>lowpass filtering</em> operation of smoothing gradients, commonly called <em>momentum</em> with typical <em>exponential moving average (EMA)</em> of gradients, since both involve recursive exponential smoothing.</p> <p>However, the two mechanisms are fundamentally different <strong>modes of the same single-pole filter</strong>. Thinking in signal‑processing terms makes the distinction clear.</p> <blockquote> <p>An EMA is just an operating point in the single-pole lowpass filter, either as a long term average (typically, \(0.9 &lt; \beta \approx 1\)) or a short-term average (often, \(0 &lt; \beta &lt; 0.9\)).</p> </blockquote> \[H(z) = \eta\,\frac{1}{1 - \beta z^{-1}},\] <blockquote> <p>This is different from the general first-order lowpass filter (\(0 &lt; \beta &lt; 1, \gamma \ne 1\))</p> </blockquote> \[H(z) = \eta\,\frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}},\] <p>whose numerator zero \(\gamma\) fundamentally alters the impulse response. Because the filter is no longer a pure single‑pole integrator, it cannot behave as an EMA for any non-zero choice of parameters \(\beta, \gamma\). This first-order filter is the underlying mechanics commonly-called <em>momentum</em>.</p><hr /> <h2 id="the-singlepole-lowpass-filter"> <a href="#the-singlepole-lowpass-filter" class="anchor-heading" aria-labelledby="the-singlepole-lowpass-filter"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Single‑Pole Low‑Pass Filter </h2> <p>Consider a first-order filter with no zero, \(\gamma=0\), only a single pole \(\beta\). The filter reduces to a single‑pole IIR (infinite impulse recursive) filter recursion:</p> \[y_{t} = \beta y_{t-1} + (1-\beta) x_t,\] <p>where \(u_t\) is the input and \(x_t\) is the filtered output.</p> <ul> <li> <p>This is a <strong>causal linear filter</strong> with impulse response<br /> \(h[t] = (1-\beta)\beta^t, \quad t \geq 0,\) i.e. an exponentially decaying weighting of past inputs.</p> </li> <li> <p>The update direction is therefore the <strong>cumulative contribution of past inputs</strong>, exponentially weighted by \(\lvert \beta \rvert &lt; 1\).</p> </li> </ul><hr /> <h4 id="ema-vs-typical-lowpass-filtering-regimes"> <a href="#ema-vs-typical-lowpass-filtering-regimes" class="anchor-heading" aria-labelledby="ema-vs-typical-lowpass-filtering-regimes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> EMA vs. Typical Lowpass filtering Regimes </h4> <ul> <li> <p><strong>EMA regime:</strong><br /> The output behaves like a <em>long time‑average statistic</em> only if<br /> \(0.9 \ll \beta &lt; 1\) and \(\gamma=0.\)<br /> In this high \(\beta\) regime (<strong>extreme smoothing</strong>), the filter is said to have a long-time memory, a larger time-lag, and functions to approximate statistical expectations by mostly assuming ergodicity of the input to the filter. <strong>This is why in practice, even when ergodic assumptions do not hold, values like \(0.99, 0.999, 0.9999\) are effective</strong>.<br /> The output behaves like a <em>shorter time‑average statistic</em> if \(0 &lt; \beta \le 0.9\) (<strong>normal smoothing</strong>), and so the filter is an extremely poor estimator of expectation under ergodicity, but possess faster tracking behavior with respect to the input signal, due to shorter time-lag.</p> </li> <li> <p><strong>Typical Lowpass filtering regime:</strong><br /> In this regime, we are not interested in estimating expected values, but merely reducing the high-frequency noise content in a signal. It is simply returning its <strong>low‑pass filtered version</strong> of the input. This is the typical momentum. Since we want smoothing, \(0 &lt; \beta \le 0.9\) (<strong>normal smoothing</strong>), <strong>using \(0.9\) has long been used (before deep learning) as a good default value, when nothing is known about the frequency characteristics of the input signal</strong>.</p> </li> </ul><hr /> <h4 id="frequencydomain-representation"> <a href="#frequencydomain-representation" class="anchor-heading" aria-labelledby="frequencydomain-representation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Frequency‑Domain Representation </h4> <p>The transfer function of the single pole filter is</p> \[H(z) = (1-\beta)\,\frac{1}{1 - \beta z^{-1}},\] <p>on the unit circle \(z = e^{j\omega}\).</p> <ul> <li> <p><strong>Magnitude response:</strong><br /> Low frequencies (\(\omega\) nearer to \(0\)) pass with gain near 1.<br /> Both lower and higher frequencies are attenuated more strongly as \(\beta \to 1\).<br /> With high \(\beta\) acts as a <em>very narrow-band low‑pass filter</em>, and thus an EMA.</p> </li> <li> <p><strong>Phase response:</strong><br /> The filter introduces a delay (phase lag) that grows with \(\beta\).</p> </li> </ul><hr /> <h2 id="why-a-zero-breaks-ema-behavior"> <a href="#why-a-zero-breaks-ema-behavior" class="anchor-heading" aria-labelledby="why-a-zero-breaks-ema-behavior"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why a Zero Breaks EMA Behavior </h2> <p>For the filter</p> \[H(z)=\eta\,\frac{1-\gamma z^{-1}}{1-\beta z^{-1}}, \qquad 0&lt;\beta&lt;1,\] <p>By taking the inverse \(z\) transform of \(H(z)\) we get the filter’s unit impulse response signal as</p> \[\begin{equation} \label{eq:ImpulseResponse} h[t] = \eta \Bigl(\beta^t u[t] - \gamma \beta^{t-1} u[t-1]\Bigr), \end{equation}\] <p>where \(u[t]\) is the unit step signal that takes the value \(1\) for \(t \ge 0\) and \(0\) otherwise. Using the convolution property of all linear, iteration-invariant systems, it immediately follows that the smoothed output is given by \(\begin{equation} \label{eq:Convolution} y_{t} = \sum_{k = 0}^t h[k]\,x_{t-k}. \end{equation}\)</p> <p>The simplified impulse response becomes</p> \[h[0]=\eta,\qquad h[t]=\eta\,\beta^{t-1}(\beta-\gamma),\quad t\ge 1.\] <p>The filter is normalized if as \(t\to\infty\), \(\sum_{k=0}^{\infty} h[k] = 1\). To see this, take \(\sum_{k=0}^{\infty} h[k] = \eta + \eta(\beta-\gamma)\sum_{k=1}^{\infty}\beta^{k-1} = \eta\left(\frac{1-\gamma}{1-\beta}\right),\) which equals 1 only for a specific choice of \(\eta = \left(\frac{1-\beta}{1-\gamma}\right)\).</p> <p>This factorization makes the key point immediate:</p> <ul> <li>If \(\gamma &lt; \beta\), the weights are positive, but <em>not monotone</em>, so the filter cannot represent an exponential average.</li> <li>If \(\gamma &gt; \beta\), then \(h[k] &lt; 0\) for all \(k\ge 1\), so the filter assigns <em>negative weights</em>, which is impossible for an EMA.</li> <li>If \(\gamma = \beta\), the numerator cancels the pole and the filter becomes <em>non-integrating</em>, again incompatible with EMA behavior.</li> <li>Only when \(\gamma = 0\) does the zero vanish and the filter reduce to the pure single-pole form required for an EMA.</li> </ul> <p>Therefore, any numerator <strong>zero other than \(\gamma = 0\) destroys the positive, monotone, normalized, decaying impulse response structure required for an EMA</strong>. A first-order low-pass filter with a zero is therefore never an EMA unless the zero is removed.</p> <h4 id="non-monotone"> <a href="#non-monotone" class="anchor-heading" aria-labelledby="non-monotone"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Non-monotone </h4> <p>Recall that the typical single-pole version has geometrically decaying weights \(h[k] = (1-\beta)\beta^k,\) which satisfy \(\frac{h[k]}{h[k-1]} = \beta &lt; 1\), for \(k&gt;0\).</p> <p>For the complete first-order filter, when \(\gamma &lt; \beta\), the factor \((\beta-\gamma)\) is positive, so the weighting sequence \(h[k]\) is positive. When \(\gamma &gt; \beta\), the factor \((\beta-\gamma)\) is positive, so the weighting sequence \(h[k]\) is negative.</p> <p>To see that this sequence is not exponentially monotone, we find that the filter starts with an initial ratio \(\frac{h[1]}{h[0]} = \beta - \gamma\), which is not necessarily a decay factor like \(\beta\), therefore inducing a structural <em>jump</em> at \(k=1\), then geometric decay for \(k\ge 1\), with ratio \(\frac{h[k]}{h[k-1]} = \beta\). So this weighting sequence does not possess a pure exponential decay shape. Because an EMA requires a single, consistent exponential decay ratio, this sequence cannot represent an estimator of a statistical average, even though the weights are positive and eventually monotone.</p> <blockquote> <p>In particular, the EMA is the unique linear estimator of a mean whose weights form a normalized, non‑negative, pure exponential sequence. This structure is forced by the requirements of exponential forgetting, minimum‑variance estimation under i.i.d. noise, and the Markov sufficiency of the one‑step recursion. Any deviation from this exponential form, such as the introduction of a numerator zero in the filter structure, breaks the probabilistic interpretation entirely. A time average such as an EMA is therefore a specific statistical estimator of the mean whose weighting structure is uniquely determined by probability theory.</p> </blockquote><hr /> <h2 id="takeaways"> <a href="#takeaways" class="anchor-heading" aria-labelledby="takeaways"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Takeaways </h2> <p>A single-pole lowpass filter’s <strong>interpretation</strong> as an EMA (short-term or long-term) <strong>depends on its pole location</strong> (the value of \(\beta\)) and the intent behind its use:</p> <ul> <li><strong>High \(\beta\) regime:</strong><br /> The pole of the filter is very close to the unit circle (edge of stability often called <strong>marginal stability</strong>). <ul> <li>The filter has <strong>long memory</strong> and effectively integrates over a large window of past inputs.</li> <li>This makes the output track only the <strong>slowly‑varying mean</strong> of the input (under ergodic assumptions).</li> <li>High‑frequency content (rapid changes, oscillations, transient spikes) is heavily attenuated, i.e. <strong>a lot of information is lost</strong>.</li> <li>In estimation, this is the EMA regime: the output is treated as a proxy for a statistical expectation.</li> </ul> </li> <li><strong>Low–Moderate \(\beta\) regime:</strong><br /> The pole is further inside the unit circle. <ul> <li>The filter has <strong>shorter memory</strong> and responds more directly to the current input.</li> <li>The output is a <strong>smoothed version of the full input signal</strong>, not its mean estimate (under ergodic assumptions).</li> <li>High‑frequency noise is reduced, <strong>but the underlying variations are still preserved</strong>.</li> <li>In stochastic gradient learning, this is the <strong>momentum regime</strong>: the filter smooths and shapes the input trajectory rather than estimating a mean value. We call this <strong>lowpass regularization</strong>.</li> </ul> </li> </ul> <p><strong>Momentum is not an EMA</strong>. Conflating the two misses the point: it is the <em>same lowpass filter operating in a different regime</em>, with different intent: <strong>lowpass smoothing vs. mean estimation</strong>.</p> <p><strong>EMA operations</strong> are achieved via a <strong>single-pole lowpass filter</strong>. But <strong>typical smoothing operations</strong> can be achieved using simple to more-complicated filter configurations.</p><hr /> <h2 id="references"> <a href="#references" class="anchor-heading" aria-labelledby="references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> References </h2> <p>Given a normalized EMA: \(y_{t} = \beta y_{t-1} + (1-\beta) x_t\), or its un-normalized version \(y_{t} = \beta y_{t-1} + x_t\),</p> <p>The probabilistic foundations of this type of update, including its</p> <ul> <li>uniqueness as a linear estimator with normalized, non‑negative, exponentially decaying weights;</li> <li>minimum‑variance estimator property \(\beta \to 1\) under i.i.d. noise; and</li> <li>representation as a one‑step Markov‑sufficient recursion;</li> </ul> <p>are established in the classical literature on adaptive filtering, time‑series analysis, stochastic approximation, and Kalman filtering. Some of such works which provide standard derivations are:</p> <ul> <li> <p><strong>Winters, P. R. (1960).</strong> <em>Forecasting Sales by Exponentially Weighted Moving Averages.</em> Management Science, 6(3), 324–342.<br /> Establishes exponential smoothing as the unique minimum‑variance linear estimator under i.i.d. noise with exponential discounting. This is the earliest formal derivation of the EMA as a statistical estimator, not a filter.</p> </li> <li> <p><strong>Box, G. E. P., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015).</strong> <em>Time Series Analysis: Forecasting and Control</em> (5th ed.). Wiley.<br /> Provides the canonical derivation of exponential smoothing as the unique estimator whose weights form a normalized exponential sequence, and connects the EMA to ARIMA(0,1,1) structure.</p> </li> <li> <p><strong>Kalman, R. E. (1960).</strong> <em>A New Approach to Linear Filtering and Prediction Problems.</em> Journal of Basic Engineering, 82(1), 35–45.<br /> Shows that the scalar Kalman filter with constant gain reduces exactly to the EMA recursion, establishing its minimum‑variance and Markov‑sufficient properties. This provides the origin of recursive least squares (RLS) in adaptive filtering.</p> </li> <li> <p><strong>Robbins, H., &amp; Monro, S. (1951).</strong> <em>A Stochastic Approximation Method.</em> Annals of Mathematical Statistics, 22(3), 400–407.<br /> Demonstrates that the EMA arises as the unique stable linear estimator under constant‑step stochastic approximation.</p> </li> <li> <p><strong>Jazwinski, A. H. (1970).</strong> <em>Stochastic Processes and Filtering Theory.</em> Academic Press.<br /> The earliest rigorous derivation of constant‑gain recursive estimators from probability theory, establishing exponential forgetting, minimum‑variance estimation, and one‑step Markov sufficiency.</p> </li> <li> <p><strong>Tsypkin, Ya. Z. (1971).</strong> <em>Adaptation and Learning in Automatic Systems.</em> Academic Press.<br /> Provides one of the earliest unified treatments of adaptive estimation, exponential forgetting, and constant‑gain recursive filters, establishing the probabilistic structure underlying EMA‑type estimators.</p> </li> <li> <p><strong>Ljung, L., &amp; Söderström, T. (1983).</strong> <em>Theory and Practice of Recursive Identification.</em> MIT Press.<br /> Derives the EMA as the unique solution to an exponentially weighted least‑squares estimation and connects it to recursive identification theory.</p> </li> </ul> <p>Particularly, the EMA can be viewed as</p> <ul> <li>scalar Kalman filter with constant gain</li> <li>constant‑step Robbins–Monro estimator</li> <li>a unique exponentially‑weighted least‑squares estimator</li> </ul> <ol> <li> <p><strong>Kalman, R. E. (1960)</strong> show the EMA as the scalar Kalman filter with constant gain \(K=1−\beta\) produces \(y_{t} = \beta y_{t-1} + (1-\beta) x_t\). They show that this recursion is the optimal minimum‑variance estimator under Gaussian i.i.d. noise and exponential prior decay. This is the most rigorous probabilistic justification of the EMA.</p> </li> <li> <p><strong>Robbins, H., &amp; Monro, S. (1951)</strong> show that the update \(y_{t} = y_{t-1} + \alpha_t(x_t - y_{t-1})\) reduces to the EMA when \(\alpha_t\) is constant.</p> </li> <li> <p><strong>Tsypkin, Ya. Z. (1971)</strong>, <strong>Ljung, L., &amp; Söderström, T. (1983)</strong> show that for a constant mean \(m=y\), minimizing \(\sum_{k = 0}^\infty \beta^k\,(x_{t-k} - y)^2\) yields the EMA and that this is the only estimator consistent with exponential forgetting and recursive sufficiency.</p> </li> </ol> <blockquote> <p>Together, these works show that as an EMA, the single pole lowpass filter is not an arbitrary smoother. As an EMA, it is the <strong>unique</strong> linear estimator of a mean whose weighting structure is fixed by probability theory: normalized, non‑negative, and purely exponential. Any deviation from this structure via a general filter structure, such as introducing a numerator zero into the transfer-function, breaks the probabilistic interpretation entirely.</p> </blockquote> <!-- We acknowledge that CIFAR‑10 and GPT‑2 are established benchmarks. However, they remain widely used in optimizer research through 2025 (Wu, 2025; Keller, 2025; Vasilev, 2025), precisely because they provide reproducible baselines and allow controlled evaluation of optimizer behavior. Our focus is on theoretical contributions, which are model‑agnostic, and these benchmarks serve as illustrative testbeds rather than claims of state‑of‑the‑art performance. Powerful Design of Small Vision Transformer on CIFAR‑10 Investigating CNNs Performance on the CIFAR‑10 Dataset through Hyperparameter Tuning CIFAR‑10 AIRBench Performance Benchmarks Benchmarking CIFAR‑10 with Tsetlin Machines A Second‑Order‑Like Optimizer with Adaptive Gradient Scaling 94% on CIFAR‑10 in 3.29 Seconds on a Single GPU CIFAR‑10: Still a standard for testing optimizer variations because it’s lightweight, reproducible, and allows rapid ablation studies. Many optimizer papers in 2023–2024 (e.g., new SGD variants, adaptive methods) reported CIFAR‑10 results alongside larger datasets like ImageNet. GPT‑2: Remained a common NLP benchmark for optimizer studies because training GPT‑3/4‑scale models is prohibitively expensive. Researchers used GPT‑2 small/medium as a reproducible proxy to study optimizer dynamics in transformers. Papers in 2024 still benchmarked optimizers on GPT‑2 to demonstrate transferability to language modeling tasks. --> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
