
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>AutoSGM Quadratic Function Sim.</title>

  <script>
    window.MathJax = {
      loader: {load: ['[tex]/color']},
      tex: {          
        inlineMath: [['$', '$'], ['\\(', '\\)']],        
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: { '[+]': ['color']},
        tags: 'ams',        
        macros: {
          rw: "\\textrm{w}",
          rwt: "\\textcolor{OliveGreen}{\\rw_t}",
          rwtp: "\\textcolor{OliveGreen}{\\rw_{t+1}}",
          rmt: "\\textcolor{OrangeRed}{m_t}",
          rmtp: "\\textcolor{OrangeRed}{m_{t+1}}",
          rmtm: "\\textcolor{OrangeRed}{m_{t-1}}",
          rst: "\\textcolor{OrangeRed}{s_t}",
          rstp: "\\textcolor{OrangeRed}{s_{t+1}}",
          rstm: "\\textcolor{OrangeRed}{s_{t-1}}",
          rg: "\\textrm{g}",
          rv: "\\textrm{v}",
          rh: "\\textrm{h}",
          rH: "\\mathbf{H}",
          rx: "\\textrm{x}",
          rxt: "\\rx_t",
          rxtp: "\\rx_{t+1}",
          rxtm: "\\rx_{t-1}",
          rxi: "\\rx_0",
          rr: "\\textrm{r}",
          rrt: "\\rr_t",
          rA: "\\mathbf{A}",
          rB: "\\mathbf{B}",
          gradt: "\\textcolor{RoyalBlue}{\\rg_{t}}",
          gradtm: "\\textcolor{RoyalBlue}{\\rg_{t-1}}",          
          gradvt: "\\textcolor{RoyalBlue}{\\rv_{t}}",
          gradvtm: "\\textcolor{RoyalBlue}{\\rv_{t-1}}",
        },
      },
      options: {
        renderActions:  { addMenu: [], assitveMml: [] },
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      svg: {
          fontCache: 'none'
      }
    };
  </script>  
  <script id="MathJax-script" type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <script src="https://cdn.plot.ly/plotly-3.1.2.min.js" charset="utf-8"></script>

  <!-- CSS file in the same folder -->
  <link rel="stylesheet" href="quad_sim.css">

  <!-- <script
  type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/three@0.137.0/build/three.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/three@0.137.0/examples/js/controls/OrbitControls.js"
></script>
<script
  type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/mathbox@latest/build/bundle/mathbox.js"
></script> -->

<div style="display: none;">
$$
\newcommand{\errt}{\textcolor{RubineRed}{\varepsilon_t}}
\newcommand{\errtm}{\textcolor{RubineRed}{\varepsilon_{t-1}}}
\newcommand{\errtp}{\textcolor{RubineRed}{\varepsilon_{t+1}}}
\newcommand{\errti}{\textcolor{RubineRed}{\varepsilon_0}}
\newcommand{\derrtp}{\textcolor{RubineRed}{\Delta\varepsilon_{t+1}}}
\newcommand{\derrt}{\textcolor{RubineRed}{\Delta\varepsilon_{t}}}
\newcommand{\derrti}{\textcolor{RubineRed}{\Delta\varepsilon_{0}}}
$$
$$
\newcommand{\mean}[1]{\mathbb{E}[#1]}
\newcommand{\mgradt}{\mean{\gradt}}
\newcommand{\mgradtm}{\mean{\gradtm}}
\newcommand{\merrt}{\mean{\errt}}
\newcommand{\merrtm}{\mean{\errtm}}
\newcommand{\merrtp}{\mean{\errtp}}
\newcommand{\mderrtp}{\mean{\derrtp}}
\newcommand{\mderrt}{\mean{\derrt}}
$$ 
$$
\newcommand{\sgradt}{{\gradt^2}}
\newcommand{\sgradtm}{{\gradtm^2}}
\newcommand{\serrt}{{\errt^2}}
\newcommand{\serrtm}{{\errtm^2}}
\newcommand{\serrtp}{{\errtp^2}}
\newcommand{\sderrtp}{{\derrtp^2}}
\newcommand{\sderrt}{{\derrt^2}}
$$   
$$
\newcommand{\vgradt}{\mean{\gradt^2}}
\newcommand{\vgradtm}{\mean{\gradtm^2}}
\newcommand{\verrt}{\mean{\errt^2}}
\newcommand{\verrtm}{\mean{\errtm^2}}
\newcommand{\verrtp}{\mean{\errtp^2}}
\newcommand{\vderrtp}{\mean{\derrtp^2}}
\newcommand{\vderrt}{\mean{\derrt^2}}
$$   
</div>

</head>

<body>     

<div class="container">

    <div class="item-head">

      <div class="note-container" role="region">
        <div class="note-header">
          <div class="note-progress" aria-live="polite" id="note-progress">1 / 3</div>
        </div>
      
        <div class="note-viewport" aria-live="polite" aria-atomic="true">
          <div class="note-track" id="note-track">

            <section class="note">
              <h4 id="note-title-0">Problem setup</h4>
              Consider the simple but representative problem of minimizing, at each time-step or iteration $t>0$, a twice-differentiable, weighted quadratic loss function, 

              $$f(\rwt) = \tfrac{1}{2} u_t^2 \cdot \errt^2,$$

              where the model is a 1-dimensional parameter-vector $\rwt$, the error relative to an optimum value $\rw^*$ is $\errt = \rwt - \rw^* $, and the weighting variable $u_t$ is modeled as a wide-sense stationary, zero-mean Gaussian data process, statistically independent of the error. 
              
              <p>This implies $u_t$ has a mean $\mean{u_t} = 0$, a finite second-moment (or variance) $\mean{u_t^2} = λ$, with $0<\lambda<\infty$, and uncorrelated values across time $\mean{u_t\,u_k} = 0, \hbox{for } t\ne k$. Independence further implies that for any powers $i,j$, the joint moment $\mean{u_t^i \errt^j} = \mean{u_t^i}\mean{\errt^j}$.</p>

              Differentiating $f(\rwt)$ with respect to $\rwt$, we get the gradient $\gradt$ and Hessian $\rH_t$ as follows,
              $$ \begin{align}
              \gradt =  \nabla f(\rwt) =  u_t^2\cdot \varepsilon_t, & \quad \mean{\gradt} = \lambda\cdot \mean{\errt}\\
              \rH_t =  \nabla^2 f(\rwt) =  u_t^2,& \quad \mean{\rH_t} = \lambda
              \end{align}
              $$
              
              To minimize  $f(\rwt)$, an optimization algorithm of choice is known as the
              <strong>Stochastic Gradient Method</strong> (SGM). The simplest form of this algorithm is an iterative update rule of the form 
              $$ \rwtp = \rwt - \alpha_t \, \gradt.$$ 
              
              At each iteration, the update rule receives an $\alpha_t > 0$ from a <strong>learning rate</strong> oracle and $\gradt$ from a gradient-generating oracle. Although, the algorithm updates $\rwt$ by taking a step in the direction of the gradient, it does not strictly assume its input is either <strong>exact</strong> or <strong>stochastic</strong>, nor does it guarantee a  <strong>descent</strong> of $f(\rwt)$ per iteration. Nonetheless, the notions of stochasticity and descent have become commonly associated with the update rule, even if they are not required for it to function.

              $$ \errtp = \errt - \alpha_t \, \gradt.$$ 
              <p>
              We will further assume that the system dynamics are linear time-invariant (LTI). This means $\alpha_t = \alpha, \forall t$, a constant learning rate across iterations, and so
              $$ \errtp = \errt - \alpha \, \gradt.$$ 
              
              <strong>Goal</strong>. In this controlled and interpretable setting, we desire that the algorithm drives its error, starting from any initial non-zero value $\errti$, to converge to its optimum value of <strong>zero</strong> in approximately one or two iterations. 
              </p>

              <div class="teaser">
              As we move forward, we will discover that <b>ensuring convergence</b> in this setting is equivalent to certifying the <b>stability conditions</b> of LTI filters.
              </div>
              
              
              <!-- <details>
                <summary>Show derivation</summary>
                <p>For quadratic \( f(x) = \tfrac{1}{2}x^\top H x \), \( \nabla f(x) = Hx \), hence \( x_{t+1} = (I - \alpha H)x_t \).</p>
              </details>
              <blockquote>Tip: Slide the plot’s step size to see contraction change.</blockquote> -->
            </section>
      
            <section class="note" >
              <h4 id="note-title-1"> Error Dynamics (Mean Convergence)</h4>

              <p>
              Recall that $\gradt = u_t^2\cdot \errt$, we can write the error equation as $\errtp =  \errt +  \derrtp$, where $\derrtp = -\alpha \, \gradt$,
              from which we get $\gradt = u_t^2\cdot \errtm + u_t^2\cdot \derrt$, and the dynamics
              </p>

              $$
              \begin{equation}              
              \label{eq:smc1}
              \begin{aligned} 
              \derrtp &= -\alpha u_t^2 \cdot \derrt -\alpha u_t^2 \cdot \errtm \\ 
              \errt &= \derrt + \errtm.
              \end{aligned}
              \end{equation}

              $$

              Note that $\mean{u_t^2} = \lambda$. Taking expectation of the terms in ($\ref{eq:smc1}$), and denote $\rmt = \merrt$, $\rst = \mderrt$, $\beta_p = -\alpha \lambda$, and $k_p = -\alpha \lambda$, the expected error dynamics is

              $$
              \begin{equation}\label{eq:smeq}
              \boxed{
              \begin{aligned} 
              \rstp &= \beta_p\,\rst + k_p \, \rmtm \\ 
              \rmt &= \rst + \rmtm.
              \end{aligned}}
              \end{equation}
              $$ 


              Equation $(\ref{eq:smeq})$, tells us that the expected error $\rmt$ is the sum of the expected change-level error $\rst$, and that the expected change-level error dynamics is that of a single-pole LTI filter,
              $$\rstp = \beta_p\,\rst + k_p \, \rmtm$$ with input $\rmtm$, its transfer-function is 
              $$H_{s}(z) = \frac{k_p}{1-\beta_p\,z^{-1}}.$$

              The change-level system $H_{s}(z)$ is both asymptotically and exponentially stable if its single pole satisfies $\lvert \beta_p\rvert < 1$. Since $\alpha >0$, $\beta_p = -\alpha \lambda$ is negative, this translates to $-1 < -\alpha\lambda < 0$, that is $1 > \alpha\lambda$, and $\alpha\lambda > 0$, which imply

              $$\begin{equation}\label{eq:mewin}
              \boxed{0 < \alpha < \tfrac{1}{\lambda}.} 
              \end{equation}$$ 
              
              The overall dynamics of the expected error, written in matrix form is
              $$
              \begin{equation}
              \underbrace{
              \begin{bmatrix}
              \rstp \\
              \rmt
              \end{bmatrix}
              }_{\rxt}
              =
              \underbrace{
              \begin{bmatrix}
              \beta_p & k_p \\
              1 & 1
              \end{bmatrix}
              }_{\rA}
              \underbrace{
              \begin{bmatrix}
              \rst \\
              \rmtm
              \end{bmatrix}
              }_{\rxtm}
              
              \label{eq:smeqmat}
              \end{equation}.
              $$
              Compactly, the two-state LTI system is $\rxt = \rA^{t}\,\rxi$.
              Again, exponential and asymptotic stablility of the two-state  system requires its eigenvalues, $z_1 = 0$, and $\textcolor{Mahogany}{z_2 =1-\alpha\lambda}$ roots of the characteristic polynomial $\det(z\mathbf{I}-\mathbf{A}) = z(z-(1-\alpha\lambda)) = 0 $ to be distinct and have a magnitude strictly less than 1. The maximum system eigenvalue $\lvert z_2 \rvert < 1$, implies $-1 < 1-\alpha\lambda < 1$, and so $2>\alpha\lambda$, and $\alpha\lambda > 0$, meaning  $0 < \alpha < \tfrac{2}{\lambda}.$ 

              <p>
              By eigenvalue decomposition of the system matrix in $\rxt = \rA^{t}\,\rxi$, and taking any consistent norm $\|\cdot\|$ of both sides, we can obtain a certificate on the exponential convergence of the expected error states as
              $$ \boxed{\| \rxt \| \le \tfrac{2}{1+\beta_p} \cdot |\textcolor{Mahogany}{z_2}|^{t}\, \|\rxi\|.}$$
              </p>
            </section>
      
            <section class="note">
              <h4 id="note-title-2">Reflections: Mean Convergence</h4>
              The main results from the error convergence in the mean-sense are the stricter range
              $$
              \boxed{0 < \alpha < \tfrac{1}{\lambda},} 
              $$
              and the exponential covergence bound
              $$ \boxed{\| \rxt \| \le \tfrac{2}{1+\beta_p} \cdot |\textcolor{Mahogany}{z_2}|^{t}\, \|\rxi\|,}
              $$   
              where we denoted
              $\rxt$ as the two-state vector       
              $\mean{\begin{bmatrix}
              \derrtp\quad\errt
              \end{bmatrix}}^{\top}
              $.
              

              <h5>Play with the controls </h5>
              <ol>
                <li>Click on the RAW button</li>
                <li>Ensure the Input reads Step </li>
                <li>Click Play, to watch the sweep $0 < \alpha < \tfrac{1}{\lambda}$</li>
              </ol>
              
              <ol>
                <li>Click on the RAW button</li>
                <li>Ensure the Input reads Impulse</li>
                <li>Click Play, to watch the sweep $0 < \alpha < \tfrac{1}{\lambda}$</li>
              </ol>
            
              <b>Observations </b>
              <ol>
                <li> <b>Coupling</b>. Once, the change-level error diverges, the actual error also diverges.
                </li>
                <li>
                  The sweep of all choices in $0 < \alpha < \tfrac{1}{\lambda}$ is sufficient to stabilize the error's impulse response.
                </li>
                <li>
                  However, the same cannot be said for the error's step response, which is what we care about the most. It appears we need a tighter range to certify an error response that converges quickly.
                </li>                
                <li>
                  Values of $\alpha$ extremely close to zero yield slow but monotonic convergence.
                </li>
                <li>
                  In contrast, approaching $\alpha = \tfrac{1}{\lambda}$ practically, leads to highly oscillatory and divergent-looking behavior.
                </li>
                <li>
                  <b>Design Gap</b>. No single choice of $\alpha$ in this range gets us close to our true goal. The fastest we can go is not ripple-free.
                </li>
              </ol>
              
              <div class="teaser">The good news, is that we can obtain a much tighter and practical certificate, by analyzing the mean-square convergence of the error.
              </div>

              <p>
              In the next page, we will see that by analyzing convergence of the error in the mean-square sense, we get a tighter, strict certificate on the choice of the learning-rate
              $$
              \boxed{0 < \alpha \le \tfrac{1}{\sqrt{3}}\cdot\tfrac{1}{\lambda},} 
              $$ 
              </p>
              and further optimizing for a ripple-free choice in this range, gives
              $$
              \boxed{\alpha = \tfrac{1}{3}\cdot\tfrac{1}{\lambda}.} 
              $$ 

            </section>
           
            <section class="note">
              <h4 id="note-title-3">Error Dynamics (Mean-square Convergence)</h4>
              <p>
                We start by squaring the recursions in (\ref{eq:smc1}), which yields
                $$
                \begin{equation}              
                \label{eq:sqmc1}
                \begin{aligned} 
                \sderrtp &= \alpha^2 u_t^4 \cdot \sderrt - \alpha^2 u_t^4 \cdot \bigl(\serrtm - 2\,\errt\errtm \bigr), \\ 
                \serrt &= \sderrt - \serrtm + 2\,\errt\errtm.
                \end{aligned}
                \end{equation}
                $$                 
                Note that $\mean{u_t^4} = 3 \lambda^2$.
                Taking expectation of the terms in ($\ref{eq:sqmc1}$), and denoting $\rmt = \verrt$, $\rst = \vderrt$, $\beta_v = 3\,\alpha^2 \lambda^2$, and $k_v = -3\,\alpha^2 \lambda^2$, the mean-square error dynamics is
                $$
                \begin{equation}\label{eq:sqmeq}
                \boxed{
                \begin{aligned} 
                \rstp &= \beta_v\,\rst + k_v \bigr(\rmtm -  2\,\mean{\errt\errtm}\bigr)\\ 
                \rmt &= \rst - \rmtm + 2\,\mean{\errt\errtm}.
                \end{aligned}}
                \end{equation}
                $$ 

                Equation $(\ref{eq:sqmeq})$, tells us that the expected change-level squared error dynamics is that of a single-pole LTI lowpass filter,
                $\rstp = \beta_v\,\rst + k_v \, y_t$, with input $y_t = \rmtm -  2\,\mean{\errt\errtm}$, its transfer-function is 
                $$H_{v}(z) = \frac{k_v}{1-\beta_v\,z^{-1}}.$$
  
                The change-level system $H_{v}(z)$ is both asymptotically and exponentially stable if its single pole satisfies $\lvert \beta_v\rvert < 1$. Since $\alpha >0$, and $\beta_v = 3\,\alpha^2 \lambda^2 > 0$ this translates to $0 < 3\alpha^2\lambda^2 < 1$,  which imply
                $$\begin{equation}\label{eq:sqmewin}
                \boxed{0 < \alpha < \tfrac{1}{\sqrt{3}}\tfrac{1}{\lambda}.} 
                \end{equation}$$ 
                
                The overall dynamics of the mean-squared error, written in matrix form is
                $$
                \begin{equation}
                \underbrace{
                \begin{bmatrix}
                \rstp \\
                \rmt
                \end{bmatrix}
                }_{\rxt}
                =
                \underbrace{
                \begin{bmatrix}
                \beta_v & k_v \\
                1 & -1
                \end{bmatrix}
                }_{\rA}
                \underbrace{
                \begin{bmatrix}
                \rst \\
                \rmtm
                \end{bmatrix}
                }_{\rxtm}
                +
                \underbrace{
                  \begin{bmatrix}
                  2\,k_v & 0 \\
                  2 & 0
                  \end{bmatrix}
                  }_{\rB}
                  \underbrace{
                  \begin{bmatrix}
                  \mean{\errt\errtm} \\
                  0
                  \end{bmatrix}
                  }_{\rrt}.
                \label{eq:sqmeqmat}
                \end{equation}
                $$
                
               Stability of the LTI system requires the magnitude of the eigenvalues of $\rA$, $\lvert z_1 \rvert = 0$, and $\textcolor{Mahogany}{\lvert z_2 \rvert = \lvert 1-3\,\alpha^2\lambda^2 \rvert}$ obtained via $\det(z\mathbf{I}-\rA)=0 $ to be distinct and strictly less than 1. 
                Assume the overall system input is bounded, $\sup_t \lvert \rrt \rvert \le R,\quad 0 < R< \infty$, we can obtain the stability certificate for the error's mean-square convergence as
                $$
                \boxed{
                  \|\rxt\| \le \tfrac{2}{1-\beta_v}\Bigl( \textcolor{Mahogany}{\lvert z_2 \rvert } ^t \|\rxi\| + 2\,R\, \tfrac{1}{1-\textcolor{Mahogany}{\lvert z_2 \rvert }} \Bigr).
                }
                $$
                Equating the maximum eigenvalue magnitude for the mean-square covergence to that for the mean covergence obtained previously,
                $\lvert 1 - 3\,\alpha^2\lambda^2 \rvert = \lvert 1 - \alpha \lambda \rvert$, so that approximately, both converge to $0$ at the same rate, we get
                $$
                \boxed{\alpha = \frac{1}{3\,\lambda}}.
                $$

            </section>

            <section class="note">
              <h4 id="note-title-4">Reflections: Error Convergence</h4>
              $$ \errtp = \errt - \alpha \, \gradt.$$ 
              The main results to additionally guarantee error convergence in the mean-square sense are the stricter range
              $$
              \boxed{0 < \alpha \le \frac{1}{\sqrt{3}\lambda} < \frac{1}{\lambda},} 
              $$
              or the ripple-free, similar rate choice
              $$
              \boxed{\alpha = \frac{1}{3\,\lambda}}.
              $$
              
              <h5>Play with the controls </h5>
              <ol>
                <li>Click on the RAW button. This automataically sets $\alpha < \tfrac{1}{3\lambda}$. </li>
                <li>Ensure the Input reads either Step or Impulse </li>
                <li>Observe, the overall system response. </li>
              </ol>
              
              <ol>
                <li>Click on the RAW-OPT button. This automataically sets $\alpha = \tfrac{1}{\sqrt{3}\lambda}$. </li>
                <li>Ensure the Input reads either Step or Impulse</li>
                <li>Observe, the overall system response. </li>
              </ol>
            
              <b>Observations </b>
              <ol>            
                <li>
                  Choosing $\alpha = \tfrac{1}{\sqrt{3}\lambda}$ is tight, if we can tolerate minimal early overshoots.
                </li>
                <li>
                  Choosing $\alpha = \tfrac{1}{{3}\lambda}$, gives us a tight ripple-free fast behavior, though a little bit slower.
                </li>
                <li>
                  <b>Design Gap</b>. Neither choice of $\alpha$ in the stability range gets us close to our true goal.
                </li>
              </ol>
              
              <div class="teaser">The better news, is that we can still achieve our goal. In fact, we can do that with a much lesser effort and less stress than we have just gone through.
        
              <p>
              In the next page, we will introduce the <b>AutoSGM</b> structure that helps us achieve our <strong>at most two-step convergence goal</strong>, by smoothing (lowpass regularization of) the raw gradient input to the SGM via the means of a LTI filter.
              </p>
              This kind of structure has been called <b>momentum</b> in classic optimization and mainstream machine learning.
              <p>
              In other words, the SGM algorithm now uses a smooth version of the gradient, replace $\gradt$ with a smooth version $\gradvt$, and the error recursion becomes
              $$ \errtp = \errt - \alpha \, \gradvt.$$ 
              </p>
              </div>


            </section>
           
            <section class="note">
              <h4 id="note-title-change2">Step 2: Change-Level Dynamics (AutoSGM)</h4>
              <p>
                AutoSGM applies a first-order filter to the stochastic gradient, shaping the step:
                
            
                \[
                  H_{\beta,\gamma}(z) = \frac{\eta(1 - \gamma z^{-1})}{1 - \beta z^{-1}}.
                \]
            
            
                The iterate update is
                
            
                \[
                  w_{t+1} = w_t - \alpha_t \, v_t,
                \]
            
            
                where \( v_t \) is the filtered gradient.
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  A causal realization of the filter yields
                  
            
                  \[
                    v_t = \beta \, v_{t-1} + \eta \big(g_t - \gamma \, g_{t-1}\big).
                  \]
            
            
                  Therefore, the change in iterate is
                  
            
                \[
                    \Delta w_t = w_{t+1} - w_t = -\alpha_t \, v_t.
                  \]
            
            
                </p>
                <p>
                  The pole \( \beta \) sets memory (momentum), and the zero \( \gamma \) sets anticipation (look-ahead).
                  Tuning \((\beta,\gamma)\) recovers methods like Heavy-Ball (\(\gamma=0\)) and Nesterov (\(\gamma>0\)).
                </p>
              </details>
              <blockquote>
                Tip: Use \( \beta \) to smooth noisy gradients and \( \gamma \) to bias steps toward predicted descent.
              </blockquote>
            </section>
 
            <section class="note">
              <h4 id="note-title-error">Step 3: Overall Error Dynamics (AutoSGM)</h4>
              <p>
                For quadratic losses, let \( e_t = w_t - w^\star \) and \( g_t = H e_t \).
                AutoSGM induces a second-order recurrence in the error:
                
            
                \[
                  e_{t+1} = (I - \alpha_t \eta H) e_t
                            + \beta \big(e_t - e_{t-1}\big)
                            - \alpha_t \eta \gamma H \, e_{t-1}.
                ]
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  Starting from
                  
            
                  \[
                    w_{t+1} = w_t - \alpha_t (\beta v_{t-1} + \eta(g_t - \gamma g_{t-1})),
                  \]
            
            
                  with \( g_t = H e_t \) and \( v_{t-1} \) eliminating via the realization, we obtain the recurrence in \( e_t \) with coefficients set by \(\beta, \gamma, \eta, \alpha_t\).
                </p>
                <p>
                  Along an eigen-direction \( \lambda_i \) of \( H \), the scalar mode evolves as
                  
            
                  \[
                    e_{t+1}^{(i)} = (1 - \alpha_t \eta \lambda_i) e_t^{(i)}
                                    + \beta \big(e_t^{(i)} - e_{t-1}^{(i)}\big)
                                    - \alpha_t \eta \gamma \lambda_i \, e_{t-1}^{(i)}.
                  \]
            
            
                </p>
              </details>
              <blockquote>
                Tip: Stability requires the roots of the mode’s characteristic polynomial to lie inside the unit circle.
                Heavy-Ball corresponds to \( \gamma=0 \); Nesterov uses \( \gamma>0 \) to accelerate well-conditioned directions.
              </blockquote>
            </section>

            <section class="note">
              <h4 id="note-title-error-scalar">Step 3 (Scalar): AutoSGM Error Dynamics</h4>
              <p>
                Specializing to the scalar quadratic \( f(x) = \tfrac{1}{2}\lambda x^2 \), 
                the error is \( e_t = x_t - x^\star = x_t \).  
                The AutoSGM recurrence becomes
                
            
                \[
                  e_{t+1} = (1 - \alpha_t \eta \lambda) e_t
                            + \beta (e_t - e_{t-1})
                            - \alpha_t \eta \gamma \lambda \, e_{t-1}.
                \]
            
            
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  Starting from the filtered update
                  
            
                  \[
                    v_t = \beta v_{t-1} + \eta(\lambda e_t - \gamma \lambda e_{t-1}),
                  \]
            
            
                  and \( x_{t+1} = x_t - \alpha_t v_t \),  
                  substituting \( e_t = x_t \) yields the second‑order recurrence above.
                </p>
                <p>
                  The characteristic polynomial for this scalar mode is
                  
            
                  \[
                    z^2 - (1 + \beta - \alpha_t \eta \lambda) z
                        + (\beta - \alpha_t \eta \gamma \lambda) = 0.
                  \]
            
            
                  Its roots \( z_{1,2} \) govern stability and convergence.
                </p>
              </details>
              <blockquote>
                Tip: Convergence requires both roots \( z_{1,2} \) to lie inside the unit circle.  
                Heavy‑Ball is recovered with \( \gamma = 0 \); Nesterov with \( \gamma > 0 \).
              </blockquote>
            </section>
                                    

          </div>
        </div>
      
        <div class="note-controls">
          <button class="note-btn" id="prev">Back</button>
          <button class="note-btn" id="next" aria-label="Next note">Next</button>
        </div>
      </div>
      
    </div>

    <div class="item-lsidebar">
      <aside class="infoPanel" id="infoPanel">
        <h3><span class="cgreen">Auto</span>SGM: LTI System Info</h3>
        <dl class="sys-info">
          <dt>Pole (change-level)</dt>
          <dd class="cgreen">$ \beta_p = \beta - \eta \alpha \lambda$ = <span class="value" id="poleVal"></span></dd>

          <dt>Gain (change-level)</dt>
          <dd >$ k_p=-\eta\alpha\lambda(1-γ) $ = <span class="value" id="ingainVal"></span></dd>

          <dt><span id="fstate">Lowpass</span> stable $\beta_p$-range</dt>
          <dd class="cgreen">$({\beta_p}_{\min}, {\beta_p}_{\max})$ = <span class="value" id="poleLimits"></span></dd>
      
          <dt><span id="fstate">Lowpass</span> stable $\alpha$-range</dt>
          <dd class="cgreen">$(\alpha_{\min}, \alpha_{\max})$ = <span class="value" id="alphaLimits"></span></dd>
        </dl>

        <div id="controls">
            <div class="slider-group">
              <label>$\beta$ <span id="betaVal">0.90</span></label>
              <input id="beta" type="range" min="0.001" max="0.99" step="0.005" value="0.9">
            </div>

            <div class="slider-group">
              <label>$\gamma$ <span id="gammaVal">0.00</span></label>
              <input id="gamma" type="range" min="-1" max="1" step="0.001" value="0.0">
            </div>

            <div class="slider-group">
              <label>$\lambda$ <span id="lamVal">1.00</span></label>
              <input id="eigval" type="range" min="0.01" max="10.0" step="0.01" value="1.0">
            </div>

            <div class="slider-group">
              <label>$\alpha$ <span id="alphaVal">0.50</span></label>
              <input id="alpha" type="range" min="0.0" max="2.0" step="0.001" value="0.5">
            </div>

            <div class="slider-group">
                <label>Input</label>
                <select id="inputType">
                  <option value="step">Step (unit)</option>
                  <option value="impulse">Impulse</option>
                </select>
            </div>    

            <div style="display:flex;gap:8px;align-items:center;">
              <button id="play" class="btn">Play</button>
              <button id="pause" class="btn" disabled>Pause</button>
              <button id="reset" class="btn">Reset</button>
              <span class="status" id="stability">Stable</span>
            </div>
            <div style="display:flex;gap:8px;align-items:center;">   
              <button id="PHB" class="btn btnalg">PHB</button>
              <button id="NAG" class="btn btnalg">NAG</button>
              <button id="SFUN" class="btn btnalg">MLS</button>
            </div>     
            <div style="display:flex;gap:8px;align-items:center;">  
              <button id="NOF" class="btn btnalg">RAW</button>   
              <button id="OPT" class="btn btnalg">OPT</button>
              <button id="free" class="btn btnalg">Free</button>
            </div>    
        </div>  
      </aside>
    </div>

    <div class="item-lplt" id="leftPlotc">      
    </div>


    <div class="rplt-top" id="rightPlotctop"> 
    </div> 
       
    <div class="rplt-btm" id="rightPlotcbottom">
    </div>   

    <div class="item-foot">
      <section class="foot-sec1">
      <h4>First-order LTI change-level and overall error dynamics. </h4>
      <ol class="foot-ol1"> 
      <li>First plot shows the <em>iteration-domain response</em> of the SGM's error $ε[t]$ from an optimum point and its change-level $\Delta\varepsilon[t]$.</li>
      <li>Second plot shows <em>two roots of the overall error behavior</em>.
      <li>Third plot shows <em>change-level pole position</em>.</li> 
      </ol>
      <span> Highlighted in the second and third plots are <span class="cgreen">stable</span>, <span class="cred">oscillatory</span> and <span class="cred">unstable</span> (outside the unit-circle) ranges.</span>
      </section>
      
    </div>

</div>

<!-- page content  -->
<script src="quad_sim.js"></script>

</body>
</html>
