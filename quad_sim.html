
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>AutoSGM Quadratic Function Sim.</title>

    

  <script
    type="text/javascript"
    src="https://cdn.jsdelivr.net/npm/three@0.137.0/build/three.min.js"
  ></script>
  <script
    type="text/javascript"
    src="https://cdn.jsdelivr.net/npm/three@0.137.0/examples/js/controls/OrbitControls.js"
  ></script>
  <!--
    - a minified version mathbox.min.js is also available;
    - recommend using a specific version (not @latest) in public sites
  -->
  <script
    type="text/javascript"
    src="https://cdn.jsdelivr.net/npm/mathbox@latest/build/bundle/mathbox.js"
  ></script>
  <!-- <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/mathbox@latest/build/mathbox.css"
  /> -->
  <!-- <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"
  />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>  -->

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      chtml: {
        scale: 1.0
      },
      svg: {
          fontCache: 'none'
      }
    };
  </script>  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <script src="https://cdn.plot.ly/plotly-3.1.2.min.js" charset="utf-8"></script>


  <!-- CSS file in the same folder -->
  <link rel="stylesheet" href="quad_sim.css">
  
</head>
<body>
   
<div style="display: none">
    \(
    \newcommand{\RR}{\mathbb{R}}
    \newcommand{\bold}[1]{\mathbf{#1}}
    \newcommand{\rw}{\textrm{w}}
    \newcommand{\rg}{\textrm{g}}
    \newcommand{\rx}{\textrm{x}}
    \newcommand{\rh}{\textrm{h}}
    \newcommand{\mean}[1]{\mathbb{E}[#1]}
    \)
</div>
<div class="container">

    <div class="item-head">

      <div class="note-container" role="region">
        <div class="note-header">
          <div class="note-progress" aria-live="polite" id="note-progress">1 / 3</div>
        </div>
      
        <div class="note-viewport" aria-live="polite" aria-atomic="true">
          <div class="note-track" id="note-track">

            <section class="note">
              <h4 id="note-title-0">Problem setup</h4>
              Consider the simple but representative problem of minimizing per iteration $t$, a weighted quadratic loss function whose model is a 1-dimensional parameter-vector $\rw_t$.

              $$f(\rw_t) = \tfrac{1}{2} u_t^2\cdot\varepsilon_t^2,$$

              where the error from an optimum value $\rw^*$ is $\varepsilon_t = \rw_t - \rw^* $, and the weighting variable $u_t$ is a wide-sense stationary, zero-mean Gaussian data process independent of the error. 
              
              <p>This implies $u_t$ has a mean $\mean{u_t} = 0$, constant second-moment or variance $\mean{u_t^2} = λ$, $\mean{u_t\,u_k} = 0, \hbox{for } t\ne k$. </p>

              Differentiating $f(\rw_t)$ with respect to $\rw_t$, twice, we get the gradient and Hessian as,
              $$ \rg_t =  \nabla f(\rw_t) =  u_t^2\cdot \varepsilon_t, \quad \mean{\rg_t} = \lambda\cdot \mean{ε_t} $$
              
              $$ \rh_t =  \nabla^2 f(\rw_t) =  u_t^2, \quad \mean{\rh_t} = \lambda$$

              To optimize  $f(\rw_t)$, an algorithm of choice is known as the
              <strong>Stochastic Gradient Method</strong> (SGM). The simplest form of this algorithm is an iterative update rule of the form 
              $$ \rw_{t+1} = \rw_t - \alpha_t \cdot \rg_t.$$ 
              
              At each iteration, the update rule asks for a $\alpha_t$ from a <strong>learning rate</strong> oracle and $\rg_t$ from a gradient-generating oracle. While the algorithm neither rigidly assumes its input is <strong>stochastic</strong> nor a per iteration <strong>descent</strong> of $f(\rw_t)$ to work, these two terms have become commonly associated with the update rule.

              $$ \varepsilon_{t+1} = \varepsilon_t - \alpha_t \cdot \rg_t.$$ 
              
              <p>
               <strong>Goal</strong>. Our goal is for the algorithm, starting from any initial value $\varepsilon_0$, to drive its error to converge to its optimum value, $0$ in approximately one or two iterations. 
              </p>
              
              <!-- <details>
                <summary>Show derivation</summary>
                <p>For quadratic \( f(x) = \tfrac{1}{2}x^\top H x \), \( \nabla f(x) = Hx \), hence \( x_{t+1} = (I - \alpha H)x_t \).</p>
              </details>
              <blockquote>Tip: Slide the plot’s step size to see contraction change.</blockquote> -->
            </section>
      
            <section class="note" >
              <h4 id="note-title-2">Step 2: Error dynamics</h4>

              <p>Observe eigenvalue‑wise behavior: \( \varepsilon_{t+1}^{(i)} = (1 - \alpha \lambda_i)\varepsilon_t^{(i)} \).</p>
            </section>
      
            <section class="note">
              <h4 id="note-title-3">Step 3: Stability window</h4>
              <p>For convergence in the quadratic case, \( 0 < \alpha < \tfrac{2}{\lambda_{\max}} \).</p>
              <p>Plot highlight: when \( \alpha \ge \tfrac{2}{\lambda_{\max}} \), trajectories diverge.</p>
            </section>

            
            <section class="note">
              <h4 id="note-title-change">Step 2: Change-Level Dynamics (AutoSGM)</h4>
              <p>
                In AutoSGM, the raw stochastic gradient is passed through a first‑order filter 
                \( H_{\beta,\gamma}(z) = \dfrac{\eta(1 - \gamma z^{-1})}{1 - \beta z^{-1}} \).  
                The update rule becomes
                
            
                \[
                  w_{t+1} = w_t - \alpha_t \, v_t,
                \]
            
            
                where \( v_t \) is the filtered gradient.
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  The filter realization is
                  
            
                  \[
                    v_t = \beta v_{t-1} + \eta \big(g_t - \gamma g_{t-1}\big).
                  \]
            
            
                  The change in iterate is
                  
            
                  \[
                    \Delta w_t = w_{t+1} - w_t = -\alpha_t v_t.
                  \]
            
            
                </p>
                <p>
                  This shows that the *change‑level dynamics* are governed by the pole \( \beta \) 
                  and zero \( \gamma \) of the filter, which tune how past gradients shape the current step.
                </p>
              </details>
              <blockquote>
                Tip: In AutoSGM, momentum and look‑ahead (HB, NAG, Adam) are all special cases of this filter.  
                The pole \( \beta \) controls memory; the zero \( \gamma \) controls gradient anticipation.
              </blockquote>
            </section>

            <section class="note">
              <h4 id="note-title-change2">Step 2: Change-Level Dynamics (AutoSGM)</h4>
              <p>
                AutoSGM applies a first-order filter to the stochastic gradient, shaping the step:
                
            
                \[
                  H_{\beta,\gamma}(z) = \frac{\eta(1 - \gamma z^{-1})}{1 - \beta z^{-1}}.
                \]
            
            
                The iterate update is
                
            
                \[
                  w_{t+1} = w_t - \alpha_t \, v_t,
                \]
            
            
                where \( v_t \) is the filtered gradient.
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  A causal realization of the filter yields
                  
            
                  \[
                    v_t = \beta \, v_{t-1} + \eta \big(g_t - \gamma \, g_{t-1}\big).
                  \]
            
            
                  Therefore, the change in iterate is
                  
            
                \[
                    \Delta w_t = w_{t+1} - w_t = -\alpha_t \, v_t.
                  \]
            
            
                </p>
                <p>
                  The pole \( \beta \) sets memory (momentum), and the zero \( \gamma \) sets anticipation (look-ahead).
                  Tuning \((\beta,\gamma)\) recovers methods like Heavy-Ball (\(\gamma=0\)) and Nesterov (\(\gamma>0\)).
                </p>
              </details>
              <blockquote>
                Tip: Use \( \beta \) to smooth noisy gradients and \( \gamma \) to bias steps toward predicted descent.
              </blockquote>
            </section>
 
            <section class="note">
              <h4 id="note-title-error">Step 3: Overall Error Dynamics (AutoSGM)</h4>
              <p>
                For quadratic losses, let \( e_t = w_t - w^\star \) and \( g_t = H e_t \).
                AutoSGM induces a second-order recurrence in the error:
                
            
                \[
                  e_{t+1} = (I - \alpha_t \eta H) e_t
                            + \beta \big(e_t - e_{t-1}\big)
                            - \alpha_t \eta \gamma H \, e_{t-1}.
                ]
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  Starting from
                  
            
                  \[
                    w_{t+1} = w_t - \alpha_t (\beta v_{t-1} + \eta(g_t - \gamma g_{t-1})),
                  \]
            
            
                  with \( g_t = H e_t \) and \( v_{t-1} \) eliminating via the realization, we obtain the recurrence in \( e_t \) with coefficients set by \(\beta, \gamma, \eta, \alpha_t\).
                </p>
                <p>
                  Along an eigen-direction \( \lambda_i \) of \( H \), the scalar mode evolves as
                  
            
                  \[
                    e_{t+1}^{(i)} = (1 - \alpha_t \eta \lambda_i) e_t^{(i)}
                                    + \beta \big(e_t^{(i)} - e_{t-1}^{(i)}\big)
                                    - \alpha_t \eta \gamma \lambda_i \, e_{t-1}^{(i)}.
                  \]
            
            
                </p>
              </details>
              <blockquote>
                Tip: Stability requires the roots of the mode’s characteristic polynomial to lie inside the unit circle.
                Heavy-Ball corresponds to \( \gamma=0 \); Nesterov uses \( \gamma>0 \) to accelerate well-conditioned directions.
              </blockquote>
            </section>

            <section class="note">
              <h4 id="note-title-error-scalar">Step 3 (Scalar): AutoSGM Error Dynamics</h4>
              <p>
                Specializing to the scalar quadratic \( f(x) = \tfrac{1}{2}\lambda x^2 \), 
                the error is \( e_t = x_t - x^\star = x_t \).  
                The AutoSGM recurrence becomes
                
            
                \[
                  e_{t+1} = (1 - \alpha_t \eta \lambda) e_t
                            + \beta (e_t - e_{t-1})
                            - \alpha_t \eta \gamma \lambda \, e_{t-1}.
                \]
            
            
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  Starting from the filtered update
                  
            
                  \[
                    v_t = \beta v_{t-1} + \eta(\lambda e_t - \gamma \lambda e_{t-1}),
                  \]
            
            
                  and \( x_{t+1} = x_t - \alpha_t v_t \),  
                  substituting \( e_t = x_t \) yields the second‑order recurrence above.
                </p>
                <p>
                  The characteristic polynomial for this scalar mode is
                  
            
                  \[
                    z^2 - (1 + \beta - \alpha_t \eta \lambda) z
                        + (\beta - \alpha_t \eta \gamma \lambda) = 0.
                  \]
            
            
                  Its roots \( z_{1,2} \) govern stability and convergence.
                </p>
              </details>
              <blockquote>
                Tip: Convergence requires both roots \( z_{1,2} \) to lie inside the unit circle.  
                Heavy‑Ball is recovered with \( \gamma = 0 \); Nesterov with \( \gamma > 0 \).
              </blockquote>
            </section>
                                    

          </div>
        </div>
      
        <div class="note-controls">
          <button class="note-btn" id="prev">Back</button>
          <button class="note-btn" id="next" aria-label="Next note">Next</button>
        </div>
      </div>
      
    </div>

    <div class="item-lsidebar">
      <aside class="infoPanel" id="infoPanel">
        <h3><span class="cgreen">Auto</span>SGM: LTI System Info</h3>
        <dl class="sys-info">
          <dt>Pole (change-level)</dt>
          <dd class="cgreen">$ \beta_p = \beta - \eta \alpha \lambda$ = <span class="value" id="poleVal"></span></dd>

          <dt>Gain (change-level)</dt>
          <dd >$ -\eta\alpha\lambda(1-γ) $ = <span class="value" id="ingainVal"></span></dd>

          <dt><span id="fstate">Lowpass</span> stable $\beta_p$-range</dt>
          <dd class="cgreen">$({\beta_p}_{\min}, {\beta_p}_{\max})$ = <span class="value" id="poleLimits"></span></dd>
      
          <dt><span id="fstate">Lowpass</span> stable $\alpha$-range</dt>
          <dd class="cgreen">$(\alpha_{\min}, \alpha_{\max})$ = <span class="value" id="alphaLimits"></span></dd>
        </dl>

        <div id="controls">
            <div class="slider-group">
              <label>$\beta$ <span id="betaVal">0.90</span></label>
              <input id="beta" type="range" min="0.001" max="0.99" step="0.005" value="0.9">
            </div>

            <div class="slider-group">
              <label>$\gamma$ <span id="gammaVal">0.00</span></label>
              <input id="gamma" type="range" min="-1" max="1" step="0.001" value="0.0">
            </div>

            <div class="slider-group">
              <label>$\lambda$ <span id="lamVal">1.00</span></label>
              <input id="eigval" type="range" min="0.01" max="10.0" step="0.01" value="1.0">
            </div>

            <div class="slider-group">
              <label>$\alpha$ <span id="alphaVal">0.50</span></label>
              <input id="alpha" type="range" min="0.0" max="2.0" step="0.001" value="0.5">
            </div>

            <div class="slider-group">
                <label>Input</label>
                <select id="inputType">
                  <option value="step">Step (unit)</option>
                  <option value="impulse">Impulse</option>
                </select>
            </div>    

            <div style="display:flex;gap:8px;align-items:center;">
              <button id="play" class="btn">Play</button>
              <button id="pause" class="btn" disabled>Pause</button>
              <button id="reset" class="btn">Reset</button>
              <span class="status" id="stability">Stable</span>
            </div>
            <div style="display:flex;gap:8px;align-items:center;">   
              <button id="PHB" class="btn btnalg">PHB</button>
              <button id="NAG" class="btn btnalg">NAG</button>
              <button id="SFUN" class="btn btnalg">MLS</button>
            </div>     
            <div style="display:flex;gap:8px;align-items:center;">  
              <button id="NOF" class="btn btnalg">RAW</button>   
              <button id="OPT" class="btn btnalg">OPT</button>
              <button id="free" class="btn btnalg">Free</button>
            </div>    
        </div>  
      </aside>
    </div>

    <div class="item-lplt" id="leftPlotc">      
    </div>


    <div class="rplt-top" id="rightPlotctop"> 
    </div> 
       
    <div class="rplt-btm" id="rightPlotcbottom">
    </div>   

    <div class="item-foot">
      <section class="foot-sec1">
      <h4>First-order LTI change-level and overall error dynamics. </h4>
      <ol class="foot-ol1"> 
      <li>First plot shows the <em>iteration-domain response</em> of the SGM's error $ε[t]$ from an optimum point and its change-level $\Delta\varepsilon[t]$.</li>
      <li>Second plot shows <em>two roots of the overall error behavior</em>.
      <li>Third plot shows <em>change-level pole position</em>.</li> 
      </ol>
      <span> Highlighted in the second and third plots are <span class="cgreen">stable</span>, <span class="cred">oscillatory</span> and <span class="cred">unstable</span> (outside the unit-circle) ranges.</span>
      </section>
      
    </div>

</div>

<!-- page content  -->
<script src="quad_sim.js"></script>

</body>
</html>
