
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>AutoSGM Quadratic Function Sim.</title>

  <script>
    window.MathJax = {
      loader: {load: ['[tex]/color']},
      tex: {          
        inlineMath: [['$', '$'], ['\\(', '\\)']],        
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        packages: { '[+]': ['color']},
        tags: 'ams',        
        macros: {
          rw: "\\textrm{w}",
          rwt: "\\textcolor{OliveGreen}{\\rw_t}",
          rwtp: "\\textcolor{OliveGreen}{\\rw_{t+1}}",
          rmt: "\\textcolor{OrangeRed}{m_t}",
          rmtp: "\\textcolor{OrangeRed}{m_{t+1}}",
          rmtm: "\\textcolor{OrangeRed}{m_{t-1}}",
          rst: "\\textcolor{OrangeRed}{s_t}",
          rstp: "\\textcolor{OrangeRed}{s_{t+1}}",
          rstm: "\\textcolor{OrangeRed}{s_{t-1}}",
          rg: "\\textrm{g}",
          rh: "\\textrm{h}",
          rH: "\\mathbf{H}",
          rx: "\\textrm{x}",
          rxt: "\\rx_t",
          rxtp: "\\rx_{t+1}",
          rxtm: "\\rx_{t-1}",
          rxi: "\\rx_0",
          rA: "\\mathbf{A}",
          gradt: "\\textcolor{RoyalBlue}{\\rg_{t}}",
          gradtm: "\\textcolor{RoyalBlue}{\\rg_{t-1}}",
        },
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      svg: {
          fontCache: 'none'
      }
    };
  </script>  
  <script id="MathJax-script" type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <script src="https://cdn.plot.ly/plotly-3.1.2.min.js" charset="utf-8"></script>

  <!-- CSS file in the same folder -->
  <link rel="stylesheet" href="quad_sim.css">

  <!-- <script
  type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/three@0.137.0/build/three.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/three@0.137.0/examples/js/controls/OrbitControls.js"
></script>
<script
  type="text/javascript"
  src="https://cdn.jsdelivr.net/npm/mathbox@latest/build/bundle/mathbox.js"
></script> -->

<div style="display: none;">
$$
\newcommand{\errt}{\textcolor{RubineRed}{\varepsilon_t}}
\newcommand{\errtm}{\textcolor{RubineRed}{\varepsilon_{t-1}}}
\newcommand{\errtp}{\textcolor{RubineRed}{\varepsilon_{t+1}}}
\newcommand{\errti}{\textcolor{RubineRed}{\varepsilon_0}}
\newcommand{\derrtp}{\textcolor{RubineRed}{\Delta\varepsilon_{t+1}}}
\newcommand{\derrt}{\textcolor{RubineRed}{\Delta\varepsilon_{t}}}
\newcommand{\derrti}{\textcolor{RubineRed}{\Delta\varepsilon_{0}}}
$$
$$
\newcommand{\mean}[1]{\mathbb{E}[#1]}
\newcommand{\mgradt}{\mean{\gradt}}
\newcommand{\mgradtm}{\mean{\gradtm}}
\newcommand{\merrt}{\mean{\errt}}
\newcommand{\merrtm}{\mean{\errtm}}
\newcommand{\merrtp}{\mean{\errtp}}
\newcommand{\mderrtp}{\mean{\derrtp}}
\newcommand{\mderrt}{\mean{\derrt}}
$$ 
$$
\newcommand{\vgradt}{\mean{\gradt^2}}
\newcommand{\vgradtm}{\mean{\gradtm^2}}
\newcommand{\verrt}{\mean{\errt^2}}
\newcommand{\verrtm}{\mean{\errtm^2}}
\newcommand{\verrtp}{\mean{\errtp^2}}
\newcommand{\vderrtp}{\mean{\derrtp^2}}
\newcommand{\vderrt}{\mean{\derrt^2}}
$$   
</div>

</head>

<body>     

<div class="container">

    <div class="item-head">

      <div class="note-container" role="region">
        <div class="note-header">
          <div class="note-progress" aria-live="polite" id="note-progress">1 / 3</div>
        </div>
      
        <div class="note-viewport" aria-live="polite" aria-atomic="true">
          <div class="note-track" id="note-track">

            <section class="note">
              <h4 id="note-title-0">Problem setup</h4>
              Consider the simple but representative problem of minimizing, at each time-step or iteration $t>0$, a twice-differentiable, weighted quadratic loss function, 

              $$f(\rwt) = \tfrac{1}{2} u_t^2 \cdot \errt^2,$$

              where the model is a 1-dimensional parameter-vector $\rwt$, the error relative to an optimum value $\rw^*$ is $\errt = \rwt - \rw^* $, and the weighting variable $u_t$ is modeled as a wide-sense stationary, zero-mean Gaussian data process, statistically independent of the error. 
              
              <p>This implies $u_t$ has a mean $\mean{u_t} = 0$, a finite second-moment (or variance) $\mean{u_t^2} = λ$, with $0<\lambda<\infty$, and uncorrelated values across time $\mean{u_t\,u_k} = 0, \hbox{for } t\ne k$. Independence further implies that for any powers $i,j$, the joint moment $\mean{u_t^i \errt^j} = \mean{u_t^i}\mean{\errt^j}$.</p>

              Differentiating $f(\rwt)$ with respect to $\rwt$, we get the gradient $\gradt$ and Hessian $\rH_t$ as follows,
              $$ \begin{align}
              \gradt =  \nabla f(\rwt) =  u_t^2\cdot \varepsilon_t, & \quad \mean{\gradt} = \lambda\cdot \mean{\errt}\\
              \rH_t =  \nabla^2 f(\rwt) =  u_t^2,& \quad \mean{\rH_t} = \lambda
              \end{align}
              $$
              
              To minimize  $f(\rwt)$, an optimization algorithm of choice is known as the
              <strong>Stochastic Gradient Method</strong> (SGM). The simplest form of this algorithm is an iterative update rule of the form 
              $$ \rwtp = \rwt - \alpha_t \, \gradt.$$ 
              
              At each iteration, the update rule receives a non-negative $\alpha_t$ from a <strong>learning rate</strong> oracle and $\gradt$ from a gradient-generating oracle. Although, the algorithm updates $\rwt$ by taking a step in the direction of the gradient, it does not strictly assume its input is either <strong>exact</strong> or <strong>stochastic</strong>, nor does it guarantee a  <strong>descent</strong> of $f(\rwt)$ per iteration. Nonetheless, the notions of stochasticity and descent have become commonly associated with the update rule, even if they are not required for it to function.

              $$ \errtp = \errt - \alpha_t \, \gradt.$$ 
              <p>
              We will further assume that the system dynamics are linear time-invariant (LTI). This means $\alpha_t = \alpha, \forall t$, a constant learning rate across iterations, and so
              $$ \errtp = \errt - \alpha \, \gradt.$$ 
              
              <strong>Goal</strong>. In this controlled and interpretable setting, we desire that the algorithm drives its error, starting from any initial non-zero value $\errti$, to converge to its optimum value of <strong>zero</strong> in approximately one or two iterations. 
              </p>

              <div class="teaser">
              As we move forward, we will discover that <b>ensuring convergence</b> in this setting is equivalent to certifying the <b>stability conditions</b> of LTI filters.
              </div>
              
              
              <!-- <details>
                <summary>Show derivation</summary>
                <p>For quadratic \( f(x) = \tfrac{1}{2}x^\top H x \), \( \nabla f(x) = Hx \), hence \( x_{t+1} = (I - \alpha H)x_t \).</p>
              </details>
              <blockquote>Tip: Slide the plot’s step size to see contraction change.</blockquote> -->
            </section>
      
            <section class="note" >
              <h4 id="note-title-1">Error dynamics</h4>
              Recall that $\gradt = x_t^2\cdot \errt$, we can write the error equation as
              <p> 
                \[
                \begin{align} 
                \derrtp = -\alpha \, \gradt \\ 
                \errtp =  \errt +  \derrtp,
                \end{align}
                \]
              so that, $\gradt = u_t^2\cdot \errtm + u_t^2\cdot \derrt$, and
              </p>

              $$
              \begin{equation}              
              \label{eq:smc1}
              \begin{aligned} 
              \derrtp &= -\alpha u_t^2 \cdot \derrt -\alpha u_t^2 \cdot \errtm \\ 
              \errt &= \derrt + \errtm.
              \end{aligned}
              \end{equation}

              $$

              Taking expectation of the terms in ($\ref{eq:smc1}$), and denote $\rmt = \merrt$, $\rst = \mderrt$, $\beta_p = -\alpha \lambda$, and $k_p = -\alpha \lambda$, the expected error dynamics is

              $$
              \begin{equation}\label{eq:smeq}
              \boxed{
              \begin{aligned} 
              \rstp &= \beta_p\,\rst + k_p \, \rmtm \\ 
              \rmt &= \rst + \rmtm.
              \end{aligned}}
              \end{equation}
              $$ 


             Equation $(\ref{eq:smeq})$, tells us that the expected error $\rmt$ is the sum of the expected change-level error $\rst$, and that the expected change-level error dynamics is that of a single-pole filter,
             $$\rstp = \beta_p\,\rst + k_p \, \rmtm$$ with input $\rmtm$, its transfer-function is 
             $$H(z) = \tfrac{k_p}{1-\beta_p\,z^{-1}}$$

            The overall dynamics of the expected error, written in matrix form is
              $$
              \begin{equation}
              \underbrace{
              \begin{bmatrix}
              \rstp \\
              \rmt
              \end{bmatrix}
              }_{\rxt}
              =
              \underbrace{
              \begin{bmatrix}
              \beta_p & k_p \\
              1 & 1
              \end{bmatrix}
              }_{\rA}
              \underbrace{
              \begin{bmatrix}
              \rst \\
              \rmtm
              \end{bmatrix}
              }_{\rxtm}
              
              \label{eq:smeqmat}
              \end{equation}
              $$

              and compactly, the two-state LTI equation is $\rxt = \rA^{t}\,\rxi$.

             
            </section>
      
            <section class="note">
              <h4 id="note-title-3">Step 3: Stability window</h4>
              <p>For convergence in the quadratic case, \( 0 < \alpha < \tfrac{2}{\lambda_{\max}} \).</p>
              <p>Plot highlight: when \( \alpha \ge \tfrac{2}{\lambda_{\max}} \), trajectories diverge.</p>
            </section>

            
            <section class="note">
              <h4 id="note-title-change">Step 2: Change-Level Dynamics (AutoSGM)</h4>
              <p>
                In AutoSGM, the raw stochastic gradient is passed through a first‑order filter 
                \( H_{\beta,\gamma}(z) = \dfrac{\eta(1 - \gamma z^{-1})}{1 - \beta z^{-1}} \).  
                The update rule becomes
                
            
                \[
                  w_{t+1} = w_t - \alpha_t \, v_t,
                \]
            
            
                where \( v_t \) is the filtered gradient.
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  The filter realization is
                  
            
                  \[
                    v_t = \beta v_{t-1} + \eta \big(g_t - \gamma g_{t-1}\big).
                  \]
            
            
                  The change in iterate is
                  
            
                  \[
                    \Delta w_t = w_{t+1} - w_t = -\alpha_t v_t.
                  \]
            
            
                </p>
                <p>
                  This shows that the *change‑level dynamics* are governed by the pole \( \beta \) 
                  and zero \( \gamma \) of the filter, which tune how past gradients shape the current step.
                </p>
              </details>
              <blockquote>
                Tip: In AutoSGM, momentum and look‑ahead (HB, NAG, Adam) are all special cases of this filter.  
                The pole \( \beta \) controls memory; the zero \( \gamma \) controls gradient anticipation.
              </blockquote>
            </section>

            <section class="note">
              <h4 id="note-title-change2">Step 2: Change-Level Dynamics (AutoSGM)</h4>
              <p>
                AutoSGM applies a first-order filter to the stochastic gradient, shaping the step:
                
            
                \[
                  H_{\beta,\gamma}(z) = \frac{\eta(1 - \gamma z^{-1})}{1 - \beta z^{-1}}.
                \]
            
            
                The iterate update is
                
            
                \[
                  w_{t+1} = w_t - \alpha_t \, v_t,
                \]
            
            
                where \( v_t \) is the filtered gradient.
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  A causal realization of the filter yields
                  
            
                  \[
                    v_t = \beta \, v_{t-1} + \eta \big(g_t - \gamma \, g_{t-1}\big).
                  \]
            
            
                  Therefore, the change in iterate is
                  
            
                \[
                    \Delta w_t = w_{t+1} - w_t = -\alpha_t \, v_t.
                  \]
            
            
                </p>
                <p>
                  The pole \( \beta \) sets memory (momentum), and the zero \( \gamma \) sets anticipation (look-ahead).
                  Tuning \((\beta,\gamma)\) recovers methods like Heavy-Ball (\(\gamma=0\)) and Nesterov (\(\gamma>0\)).
                </p>
              </details>
              <blockquote>
                Tip: Use \( \beta \) to smooth noisy gradients and \( \gamma \) to bias steps toward predicted descent.
              </blockquote>
            </section>
 
            <section class="note">
              <h4 id="note-title-error">Step 3: Overall Error Dynamics (AutoSGM)</h4>
              <p>
                For quadratic losses, let \( e_t = w_t - w^\star \) and \( g_t = H e_t \).
                AutoSGM induces a second-order recurrence in the error:
                
            
                \[
                  e_{t+1} = (I - \alpha_t \eta H) e_t
                            + \beta \big(e_t - e_{t-1}\big)
                            - \alpha_t \eta \gamma H \, e_{t-1}.
                ]
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  Starting from
                  
            
                  \[
                    w_{t+1} = w_t - \alpha_t (\beta v_{t-1} + \eta(g_t - \gamma g_{t-1})),
                  \]
            
            
                  with \( g_t = H e_t \) and \( v_{t-1} \) eliminating via the realization, we obtain the recurrence in \( e_t \) with coefficients set by \(\beta, \gamma, \eta, \alpha_t\).
                </p>
                <p>
                  Along an eigen-direction \( \lambda_i \) of \( H \), the scalar mode evolves as
                  
            
                  \[
                    e_{t+1}^{(i)} = (1 - \alpha_t \eta \lambda_i) e_t^{(i)}
                                    + \beta \big(e_t^{(i)} - e_{t-1}^{(i)}\big)
                                    - \alpha_t \eta \gamma \lambda_i \, e_{t-1}^{(i)}.
                  \]
            
            
                </p>
              </details>
              <blockquote>
                Tip: Stability requires the roots of the mode’s characteristic polynomial to lie inside the unit circle.
                Heavy-Ball corresponds to \( \gamma=0 \); Nesterov uses \( \gamma>0 \) to accelerate well-conditioned directions.
              </blockquote>
            </section>

            <section class="note">
              <h4 id="note-title-error-scalar">Step 3 (Scalar): AutoSGM Error Dynamics</h4>
              <p>
                Specializing to the scalar quadratic \( f(x) = \tfrac{1}{2}\lambda x^2 \), 
                the error is \( e_t = x_t - x^\star = x_t \).  
                The AutoSGM recurrence becomes
                
            
                \[
                  e_{t+1} = (1 - \alpha_t \eta \lambda) e_t
                            + \beta (e_t - e_{t-1})
                            - \alpha_t \eta \gamma \lambda \, e_{t-1}.
                \]
            
            
              </p>
              <details>
                <summary>Show derivation</summary>
                <p>
                  Starting from the filtered update
                  
            
                  \[
                    v_t = \beta v_{t-1} + \eta(\lambda e_t - \gamma \lambda e_{t-1}),
                  \]
            
            
                  and \( x_{t+1} = x_t - \alpha_t v_t \),  
                  substituting \( e_t = x_t \) yields the second‑order recurrence above.
                </p>
                <p>
                  The characteristic polynomial for this scalar mode is
                  
            
                  \[
                    z^2 - (1 + \beta - \alpha_t \eta \lambda) z
                        + (\beta - \alpha_t \eta \gamma \lambda) = 0.
                  \]
            
            
                  Its roots \( z_{1,2} \) govern stability and convergence.
                </p>
              </details>
              <blockquote>
                Tip: Convergence requires both roots \( z_{1,2} \) to lie inside the unit circle.  
                Heavy‑Ball is recovered with \( \gamma = 0 \); Nesterov with \( \gamma > 0 \).
              </blockquote>
            </section>
                                    

          </div>
        </div>
      
        <div class="note-controls">
          <button class="note-btn" id="prev">Back</button>
          <button class="note-btn" id="next" aria-label="Next note">Next</button>
        </div>
      </div>
      
    </div>

    <div class="item-lsidebar">
      <aside class="infoPanel" id="infoPanel">
        <h3><span class="cgreen">Auto</span>SGM: LTI System Info</h3>
        <dl class="sys-info">
          <dt>Pole (change-level)</dt>
          <dd class="cgreen">$ \beta_p = \beta - \eta \alpha \lambda$ = <span class="value" id="poleVal"></span></dd>

          <dt>Gain (change-level)</dt>
          <dd >$ -\eta\alpha\lambda(1-γ) $ = <span class="value" id="ingainVal"></span></dd>

          <dt><span id="fstate">Lowpass</span> stable $\beta_p$-range</dt>
          <dd class="cgreen">$({\beta_p}_{\min}, {\beta_p}_{\max})$ = <span class="value" id="poleLimits"></span></dd>
      
          <dt><span id="fstate">Lowpass</span> stable $\alpha$-range</dt>
          <dd class="cgreen">$(\alpha_{\min}, \alpha_{\max})$ = <span class="value" id="alphaLimits"></span></dd>
        </dl>

        <div id="controls">
            <div class="slider-group">
              <label>$\beta$ <span id="betaVal">0.90</span></label>
              <input id="beta" type="range" min="0.001" max="0.99" step="0.005" value="0.9">
            </div>

            <div class="slider-group">
              <label>$\gamma$ <span id="gammaVal">0.00</span></label>
              <input id="gamma" type="range" min="-1" max="1" step="0.001" value="0.0">
            </div>

            <div class="slider-group">
              <label>$\lambda$ <span id="lamVal">1.00</span></label>
              <input id="eigval" type="range" min="0.01" max="10.0" step="0.01" value="1.0">
            </div>

            <div class="slider-group">
              <label>$\alpha$ <span id="alphaVal">0.50</span></label>
              <input id="alpha" type="range" min="0.0" max="2.0" step="0.001" value="0.5">
            </div>

            <div class="slider-group">
                <label>Input</label>
                <select id="inputType">
                  <option value="step">Step (unit)</option>
                  <option value="impulse">Impulse</option>
                </select>
            </div>    

            <div style="display:flex;gap:8px;align-items:center;">
              <button id="play" class="btn">Play</button>
              <button id="pause" class="btn" disabled>Pause</button>
              <button id="reset" class="btn">Reset</button>
              <span class="status" id="stability">Stable</span>
            </div>
            <div style="display:flex;gap:8px;align-items:center;">   
              <button id="PHB" class="btn btnalg">PHB</button>
              <button id="NAG" class="btn btnalg">NAG</button>
              <button id="SFUN" class="btn btnalg">MLS</button>
            </div>     
            <div style="display:flex;gap:8px;align-items:center;">  
              <button id="NOF" class="btn btnalg">RAW</button>   
              <button id="OPT" class="btn btnalg">OPT</button>
              <button id="free" class="btn btnalg">Free</button>
            </div>    
        </div>  
      </aside>
    </div>

    <div class="item-lplt" id="leftPlotc">      
    </div>


    <div class="rplt-top" id="rightPlotctop"> 
    </div> 
       
    <div class="rplt-btm" id="rightPlotcbottom">
    </div>   

    <div class="item-foot">
      <section class="foot-sec1">
      <h4>First-order LTI change-level and overall error dynamics. </h4>
      <ol class="foot-ol1"> 
      <li>First plot shows the <em>iteration-domain response</em> of the SGM's error $ε[t]$ from an optimum point and its change-level $\Delta\varepsilon[t]$.</li>
      <li>Second plot shows <em>two roots of the overall error behavior</em>.
      <li>Third plot shows <em>change-level pole position</em>.</li> 
      </ol>
      <span> Highlighted in the second and third plots are <span class="cgreen">stable</span>, <span class="cred">oscillatory</span> and <span class="cred">unstable</span> (outside the unit-circle) ranges.</span>
      </section>
      
    </div>

</div>

<!-- page content  -->
<script src="quad_sim.js"></script>

</body>
</html>
