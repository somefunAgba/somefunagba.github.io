<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(2)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(2) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(2) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(2) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Learning-Rate Annealing as Controlled First-Order Dynamic Systems | Research Notes</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="Learning-Rate Annealing as Controlled First-Order Dynamic Systems" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Signal processing meets deep learning optimization." /> <meta property="og:description" content="Signal processing meets deep learning optimization." /> <link rel="canonical" href="http://localhost:4000/" /> <meta property="og:url" content="http://localhost:4000/" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Learning-Rate Annealing as Controlled First-Order Dynamic Systems" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebSite","description":"Signal processing meets deep learning optimization.","headline":"Learning-Rate Annealing as Controlled First-Order Dynamic Systems","name":"Research Notes","url":"http://localhost:4000/"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing meets deep learning optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods for Better Learning</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Learning-Rate Annealing as Controlled First-Order Dynamic Systems category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/" class="nav-list-link">Learning-Rate Annealing as Controlled First-Order Dynamic Systems</a><ul class="nav-list"><li class="nav-list-item"><a href="/summary" class="nav-list-link">Summary</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2025. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="why-cosine-annealing-works"> <a href="#why-cosine-annealing-works" class="anchor-heading" aria-labelledby="why-cosine-annealing-works"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Cosine Annealing Works </h1> <p class="fs-6 fw-300">and Why You Don’t Actually Need It.</p><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#why-cosine-annealing-works" id="markdown-toc-why-cosine-annealing-works">Why Cosine Annealing Works</a> <ol> <li><a href="#the-secret-isnt-the-shape-its-the-rate-of-change" id="markdown-toc-the-secret-isnt-the-shape-its-the-rate-of-change">The Secret Isn’t the Shape, It’s the Rate of Change</a></li> <li><a href="#a-three-stage-recipe-for-effective-learning-rate-schedules" id="markdown-toc-a-three-stage-recipe-for-effective-learning-rate-schedules">A Three-Stage Recipe for Effective Learning Rate Schedules</a></li> <li><a href="#figure-placeholders" id="markdown-toc-figure-placeholders">Figure Placeholders</a></li> <li><a href="#building-better-simpler-alternatives" id="markdown-toc-building-better-simpler-alternatives">Building Better, Simpler Alternatives</a></li> <li><a href="#a-twist-decoupling-the-schedule-from-training-time" id="markdown-toc-a-twist-decoupling-the-schedule-from-training-time">A Twist: Decoupling the Schedule from Training Time</a></li> <li><a href="#-interesting-observations" id="markdown-toc--interesting-observations">💡 Interesting Observations</a></li> <li><a href="#what-this-means-for-practitioners" id="markdown-toc-what-this-means-for-practitioners">What This Means for Practitioners</a> <ol> <li><a href="#practical-impact" id="markdown-toc-practical-impact">Practical Impact</a></li> <li><a href="#-the-big-picture" id="markdown-toc--the-big-picture">📈 The Big Picture</a></li> </ol> </li> <li><a href="#-references" id="markdown-toc--references">📚 References</a></li> <li><a href="#-how-to-try-it" id="markdown-toc--how-to-try-it">🛠 How to Try It</a></li> </ol> </li> </ol> </details><hr /> <p>If you’ve trained a large neural network in the last few years, you’ve probably used or heard of <strong>cosine annealing</strong>. It’s a go-to learning rate schedule, celebrated for its ability to coax better performance out of deep learning models. Learning rate schedules are the unsung heroes of deep learning optimization. In the ever-evolving landscape of deep learning, learning rate schedules play a pivotal role in training stability and generalization.</p> <p>The idea is simple:<br /> You start with a higher learning rate and gradually decrease it following a cosine curve, ending near zero. It’s a standard trick of the trade that consistently delivers smoother convergence and better results.</p> <p>But a fundamental question has lingered: <strong>Why is it so effective?</strong> Is there something magical about the cosine function itself?</p> <p>In this paper, <em>Annealing via Window Functions</em>, we emerge with a powerful insight: <strong>The magic isn’t in the cosine at all. It’s in its behavior.</strong></p> <p>This insight not only explains the success of existing methods but also opens the door to simpler, more efficient alternatives.</p><hr /> <h2 id="the-secret-isnt-the-shape-its-the-rate-of-change"> <a href="#the-secret-isnt-the-shape-its-the-rate-of-change" class="anchor-heading" aria-labelledby="the-secret-isnt-the-shape-its-the-rate-of-change"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Secret Isn’t the Shape, It’s the Rate of Change </h2> <p>We reframe learning rate schedules through the lens of <strong>classical signal processing</strong>, viewing them as <strong>finite-time window functions</strong> — functions that shape a signal over a finite interval.</p> <p>Such functions have been heavily studied and applied in spectral analysis. In deep learning, <em>they control how aggressively the learning algorithm explores vs. exploits</em> in the parameter space.</p> <p>Analyzing popular schedules like cosine annealing and linear decay, we found:</p> <ul> <li>Success isn’t tied to the specific formula.</li> <li>What matters is the <strong>smooth, controlled rate of change</strong> over time.</li> </ul> <p>We introduce a key metric: the <strong>time‑normalized relative rate of change</strong> \(\gamma(t)\). This captures how quickly the learning rate decays at any point.</p><hr /> <h2 id="a-three-stage-recipe-for-effective-learning-rate-schedules"> <a href="#a-three-stage-recipe-for-effective-learning-rate-schedules" class="anchor-heading" aria-labelledby="a-three-stage-recipe-for-effective-learning-rate-schedules"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Three-Stage Recipe for Effective Learning Rate Schedules </h2> <p>It turns out that effective schedules \(\digamma(t)\), like cosine annealing are <strong>finite-time window functions</strong> follow a distinct three-stage pattern in their rate of change \(\gamma(t)\):</p> \[\gamma(t) = \frac{1 - \digamma(t)}{t \cdot \digamma(t)}\] <p>We then impose <strong>three-stage constraints</strong> on \(\gamma(t)\):</p> <ol> <li> <p><strong>Early-to-Mid Stage (Exploration)</strong><br /> The rate of change is kept uniformly small. This prevents the learning rate from dropping too quickly, allowing the model to freely explore the vast landscape of possible solutions and escape poor local minima.</p> </li> <li> <p><strong>Mid-to-Late Stage (Transition)</strong><br /> The rate of change \(\gamma(t)\) increases smoothly. This is the crucial transition from broad exploration to focused fine-tuning.</p> </li> <li> <p><strong>Late Stage (Exploitation)</strong><br /> The rate of change grows rapidly, causing the learning rate to plummet. This allows the model to lock onto a promising minimum and converge to a refined solution.</p> </li> </ol> <p>This three-stage process ensures a balanced trade-off between exploring the problem space and exploiting promising regions. Cosine annealing and linear decay are successful precisely because they naturally exhibit this favorable <em>first-order dynamic behavior</em>.</p> <p>This behavior is visualized in the paper (Figure 1), showing how cosine and linear decay schedules naturally satisfy these constraints.</p> <h2 id="figure-placeholders"> <a href="#figure-placeholders" class="anchor-heading" aria-labelledby="figure-placeholders"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Figure Placeholders </h2> <p><strong>Figure 1:</strong> Example cosine annealing schedule with ( \gamma(t) ) overlay.<br /> <code class="language-plaintext highlighter-rouge">![Cosine Annealing with Gamma Overlay](figures/cosine_gamma.png)</code></p> <p><strong>Figure 2:</strong> Polynomial schedule tuned to match three‑stage ( \gamma(t) ) behavior.<br /> <code class="language-plaintext highlighter-rouge">![Polynomial Schedule with Gamma Overlay](figures/poly_gamma.png)</code></p> <p><strong>Figure 3:</strong> Logistic sigmoid schedule with equivalent ( \gamma(t) ) dynamics.<br /> <code class="language-plaintext highlighter-rouge">![Sigmoid Schedule with Gamma Overlay](figures/sigmoid_gamma.png)</code></p><hr /> <h2 id="building-better-simpler-alternatives"> <a href="#building-better-simpler-alternatives" class="anchor-heading" aria-labelledby="building-better-simpler-alternatives"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Building Better, Simpler Alternatives </h2> <p>Different shapes, same dynamics. This discovery is more than just a neat explanation; it’s a practical blueprint for designing new learning rate schedules.</p> <p>If the underlying function doesn’t matter, can we create simpler ones that follow the same three-stage rule?<br /> The answer is a resounding <strong>yes</strong>.</p> <p>We designed and tested several computationally cheaper alternatives based on simple polynomials and logistic sigmoid functions. By tuning these functions to match the three-stage behavior of cosine annealing, they achieved identical — and in some cases, slightly better — performance.</p> <p><strong>Experiments:</strong></p> <ul> <li>GPT-2 models for language tasks</li> <li>ResNet-18 for image classification</li> </ul> <p>The new, simpler schedules that adhered to the three-stage pattern performed just as well as the established baselines.<br /> Conversely, schedules that were designed to violate this pattern consistently performed worse, plateauing at a higher loss.</p> <p>These results are summarized in Tables 1–4 and Figures 5–13 of the paper.</p><hr /> <h2 id="a-twist-decoupling-the-schedule-from-training-time"> <a href="#a-twist-decoupling-the-schedule-from-training-time" class="anchor-heading" aria-labelledby="a-twist-decoupling-the-schedule-from-training-time"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Twist: Decoupling the Schedule from Training Time </h2> <p>The paper introduces another fascinating idea: replacing the standard linear progression of time \(t / \tau\) with a <strong>quasi-random Kronecker sequence</strong>.</p> <ul> <li>Populates the interval from 0 to 1 uniformly but non-monotonically.</li> <li>Decouples the schedule’s design from the total training length.</li> <li>Offers greater flexibility without sacrificing performance.</li> </ul><hr /> <h2 id="-interesting-observations"> <a href="#-interesting-observations" class="anchor-heading" aria-labelledby="-interesting-observations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 💡 Interesting Observations </h2> <p>In the paper the <strong>raised-cosine window</strong> is also linked to <strong>Chebyshev acceleration</strong> in convex-quadratic optimization, offering a deeper mathematical justification.</p><hr /> <h2 id="what-this-means-for-practitioners"> <a href="#what-this-means-for-practitioners" class="anchor-heading" aria-labelledby="what-this-means-for-practitioners"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What This Means for Practitioners </h2> <p>The key takeaway from this research is a paradigm shift in how we should think about learning rate schedules.</p> <ul> <li> <p><strong>Focus on behavior, not formulas</strong><br /> Don’t be dogmatic about using a cosine function. What matters is controlling the schedule’s rate of change to follow the three-stage exploration–exploitation pattern.</p> </li> <li> <p><strong>Simpler can be better</strong><br /> You can use computationally cheaper functions, like simple polynomials, to achieve the same or better results as cosine annealing, potentially speeding up your workflow.</p> </li> <li> <p><strong>A principled design space</strong><br /> This framework provides a clear, theoretical foundation for designing and tuning custom learning rate schedules tailored to specific needs — moving us from “black magic” to principled engineering. Designing schedules in \(\gamma(t)\)-space gives both interpretability and robustness.</p> </li> </ul><hr /> <p>In the end, cosine annealing isn’t magic. It’s just a very good implementation of a fundamental principle.<br /> We now have the blueprint to understand that principle and build upon it.</p> <h3 id="practical-impact"> <a href="#practical-impact" class="anchor-heading" aria-labelledby="practical-impact"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Practical Impact </h3> <p>This work provides a robust foundation for designing learning rate schedules that are both theoretically sound and empirically effective. By treating schedules as window functions, researchers and practitioners gain a flexible toolkit for optimizing training dynamics. The framework is model-agnostic and applies across architectures and datasets. The framework is particularly valuable for large-scale models and long training runs, where schedule design can significantly impact convergence and generalization.</p> <h3 id="-the-big-picture"> <a href="#-the-big-picture" class="anchor-heading" aria-labelledby="-the-big-picture"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📈 The Big Picture </h3> <p>This isn’t just a tweak — it’s a <strong>framework</strong> for thinking about learning‑rate schedules.</p> <p>Once you see them as first‑order dynamic systems, you can:</p> <ul> <li>Predict their behavior.</li> <li>Design them systematically.</li> <li>Transfer them across domains.</li> </ul><hr /> <h2 id="-references"> <a href="#-references" class="anchor-heading" aria-labelledby="-references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 📚 References </h2> <p>See the paper’s <a href="/summary">summary</a> or the full paper for detailed derivations, experimental setups, and additional results.</p><hr /> <h2 id="-how-to-try-it"> <a href="#-how-to-try-it" class="anchor-heading" aria-labelledby="-how-to-try-it"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 🛠 How to Try It </h2> <p>I’ve released a minimal PyTorch implementation with:</p> <ul> <li>Drop‑in schedules for HuggingFace Transformers and torchvision.</li> <li>Visualization scripts for γ(t) profiles.</li> <li>Examples on GPT‑2 (Shakespeare/WikiText) and ResNet‑18 (CIFAR‑10).</li> </ul> <p><a href="https://github.com/yourusername/horizonlr"><strong>GitHub Repo →</strong></a></p><hr /> <p>💡 <strong>Next up:</strong> If you’re working on large‑scale optimization and want to collaborate, let’s talk.</p><hr /> <p><img src="/assets/fig1_unified_schedules_annotated.png" alt="Unified Schedules" /><br /> <img src="/assets/fig2_three_stage_constraints_annotated.png" alt="Three-Stage Constraints" /><br /> <img src="/assets/fig3_horizon_free_annotated.png" alt="Horizon-Free Scheduling" /></p> <hr> <h2 class="text-delta">Table of contents</h2> <ul> <li> <a href="/summary">Summary</a> </li> </ul> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
