<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(5)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(5) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(5) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(5) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Learning-Rate Annealing | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Learning-Rate Annealing" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Signal processing meets deep learning optimization." /> <meta property="og:description" content="Signal processing meets deep learning optimization." /> <link rel="canonical" href="http://localhost:4000/asgm_lrwinds" /> <meta property="og:url" content="http://localhost:4000/asgm_lrwinds" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-10-25T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Learning-Rate Annealing" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-25T00:00:00-07:00","datePublished":"2025-10-25T00:00:00-07:00","description":"Signal processing meets deep learning optimization.","headline":"Learning-Rate Annealing","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm_lrwinds"},"url":"http://localhost:4000/asgm_lrwinds"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ <!-- TROLRS --> $$ \newcommand{\trobjsca}{\mathcal{D}[t,i]} \newcommand{\trobjmat}{\mathcal{D}[t]} \newcommand{\Tr}{\mathrm{Tr}} \newcommand{\step}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{g}[t,i]} \newcommand{\stepv}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{v}[t,i]} \newcommand{\matstep}{\Delta[t+1] = -\alpha[t]\,\mathbf{g}[t]} \newcommand{\matstepv}{\Delta[t+1] = -\alpha[t]\,\mathbf{v}[t]} \newcommand{\ngrad}{\bar{\mathbf{g}}[t,i]} \newcommand{\ngradv}{\bar{\mathbf{v}}[t,i]} \newcommand{\ngradsq}{\bar{\mathbf{g}}^2[t,i]} \newcommand{\ngradvsq}{\bar{\mathbf{v}}^2[t,i]} \newcommand{\nmatgrad}{\bar{\mathbf{g}}[t]} \newcommand{\nmatgradv}{\bar{\mathbf{v}}[t]} \newcommand{\fof}{\mathbb{H}_{\beta,\,\gamma}} \newcommand{\expg}{\mathbb{E}\big[\mathbf{g}[t,i]\big]} \newcommand{\expv}{\mathbb{E}\big[\mathbf{v}[t,i]\big]} $$ $$ \newcommand{\stepmom}{\mathbb{E}\big[{\Delta}^2[t+1, i]\big]} \newcommand{\matstepmom}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\Delta[t+1]\big]} \newcommand{\gmatstepmom}{\mathbb{E}\big[\Tr\big(\left<{\Delta[t+1],\Delta[t+1]}\right>\big)\big]} $$ $$ \newcommand{\stepcorrng}{\mathbb{E}\big[\Delta[t+1, i]\,\ngrad\big]} \newcommand{\stepcorrngv}{\mathbb{E}\big[\Delta[t+1, i]\,\ngradv\big]} \newcommand{\matstepcorrng}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgrad \big) \big]} \newcommand{\matstepcorrngv}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgradv \big) \big]} \newcommand{\gmatstepcorrng}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgrad}\right\rangle \big)\big]} \newcommand{\gmatstepcorrngv}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgradv}\right\rangle \big)\big]} $$ $$ \newcommand{\stepcorru}{\mathbb{E}\big[\Delta[t+1, i]\,\mathbf{u}[t,i]\big]} \newcommand{\matstepcorru}{\mathbb{E}\big[\Delta^\intercal[t+1]\,\mathbf{u}[t]\big]} $$ $$ \newcommand{\numngradcorr}{\mathbb{E}\big[\ngradsq\big]} \newcommand{\numwgcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngrad\big]} \newcommand{\numngradvcorr}{\mathbb{E}\big[\ngradvsq\big]} \newcommand{\numwvcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngradv\big]} \newcommand{\dengmom}{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]} \newcommand{\dengmomv}{\mathbb{E}\big[\mathbf{v}^2[t,i]\big]} \newcommand{\matdengmom}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]} \newcommand{\matdengmomv}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]} \newcommand{\dengmomsqrt}{\sqrt{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]}} \newcommand{\matdengmomsqrt}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matdenvmomsqrt}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matngradcorr}{\mathbb{E}\big[\nmatgrad\nmatgrad^{\intercal}\big]} \newcommand{\matngradvcorr}{\mathbb{E}\big[\nmatgradv\nmatgradv^{\intercal}\big]} \newcommand{\matwngradcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgrad^{\intercal}\big]} \newcommand{\matwngradvcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgradv^{\intercal}\big]} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing, and control in learning and optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods</a><ul class="nav-list"><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Smooth Learning Dynamics</a></li><li class="nav-list-item"><a href="/asgm_trolrs" class="nav-list-link">Trust-region Optimal Learning rates</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Conjugated Directions</a></li><li class="nav-list-item"><a href="/lpf_not_ema" class="nav-list-link">Momentum is not an EMA</a></li><li class="nav-list-item"><a href="/asgm_lrwinds" class="nav-list-link">Learning-Rate Annealing</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> Â© 2026. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/asgm.html">AutoSGM: Unifying Momentum Methods</a></li> <li class="breadcrumb-nav-list-item"><span>Learning-Rate Annealing</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="why-cosine-annealing-works"> <a href="#why-cosine-annealing-works" class="anchor-heading" aria-labelledby="why-cosine-annealing-works"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Cosine Annealing Works </h1> <p class="fs-6 fw-300">and Why We Donâ€™t Actually Need It.</p> <it>Work in Progress</it> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2">Page created: Oct 25 2025 at 12:00 AM</p> </div><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#why-cosine-annealing-works" id="markdown-toc-why-cosine-annealing-works">Why Cosine Annealing Works</a> <ol> <li><a href="#constant-setup" id="markdown-toc-constant-setup">Constant Setup</a></li> <li><a href="#iterative-setup" id="markdown-toc-iterative-setup">Iterative Setup</a></li> </ol> </li> </ol> </details><hr /> <blockquote> <p>Learning rate schedules are celebrated for their ability to coax better performance out of deep learning models, and play a pivotal role in training stability and generalization. But a fundamental question has lingered: <strong>Why is it so effective?</strong> Is there something magical about these schedule functions?</p> </blockquote> <!-- In this paper, *Annealing via Window Functions*, we emerge with a powerful insight: **The magic isn't in the cosine at all. It's in its behavior.** This insight not only provides an explanation for the success of commonly-used annealing functions but also opens the door to simpler, more efficient alternatives. --><hr /> <h4 id="constant-setup"> <a href="#constant-setup" class="anchor-heading" aria-labelledby="constant-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Constant Setup </h4> <p>Let \(n&gt;1\), \(\alpha[t], \mathbf{w}[t], \mathbf{g}[t], \Delta[t+1] \in \mathbb{R}^{n \times 1}\)</p> <p>For each coordinate \(i\),</p> <div class="text-center">\[\boxed{ {\mathbf{w}[t+1, i]} = {\mathbf{w}[t,i]} + {\Delta[t+1,i]} }\] </div> \[\boxed{ \step }\] <p>Let \(0 &lt; \mu \le 1\) be a trust-region penalty constant that enforces the per-iteration subproblem</p> \[\boxed{ \trobjsca = \stepmom + \mu\,\stepcorrng }\] <p>Since</p> <p class="text-center">\(\frac{d^2\trobjsca}{d{}\alpha[t,i]^2} = \dengmom\),</p> <p>the subproblem defined by \(\trobjsca\) is strongly convex in \(\alpha[t,i]\). Let the normalized gradient be \(\ngrad = \frac{\mathbf{g}[t,i]}{\dengmomsqrt}\), where \(\numngradcorr=1\).</p> <p>Minimizing \(\trobjsca\) w.r.t \(\alpha[t,i]\) gives</p> <div class="rbox"> $$ \boxed{ \begin{align}\label{lrmom} \alpha[t,i] = \mu\,\frac{1}{\dengmomsqrt} \end{align} } $$ </div> <p>which is in the form</p> \[\alpha[t,i] = \mu\,\Phi[t,i], \quad \Phi[t,i] = \frac{1}{\dengmomsqrt} .\]<hr /> <h4 id="iterative-setup"> <a href="#iterative-setup" class="anchor-heading" aria-labelledby="iterative-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Iterative Setup </h4> <p>In general, to enforce the per-iteration subproblem, replace the trust-region constant \(\mu\) with an iterative form \(0 \le \mu[t] \le 1\), where</p> <p class="text-center">\(\mu[t]=\mu\,\digamma[t], \quad 0\le \digamma[t] \le 1\),</p> <p>and \(0 &lt; \mu \le 1\) is the trust-region penalty constant.</p> <p>The trust-region objective becomes</p> \[\boxed{ \trobjsca = \stepmom + \mu[t]\,\stepcorrng }\] <p>Minimizing \(\trobjsca\) w.r.t \(\alpha[t,i]\) gives</p> <div class="rbox"> $$ \boxed{ \begin{align}\label{lrmomt} \alpha[t,i] = \mu[t]\,\frac{1}{\dengmomsqrt} \end{align} } $$ </div> <p>which is in the form</p> \[\alpha[t,i] = \mu[t]\,\Phi[t,i].\]<hr /><hr /> <!-- ## The Secret Isn't the Shape, It's the Rate of Change We reframe learning rate schedules through the lens of **classical signal processing**, viewing them as **finite-time window functions** â€” functions that shape a signal over a finite interval. Such functions have been heavily studied and applied in spectral analysis. In deep learning, *they control how aggressively the learning algorithm explores vs. exploits* in the parameter space. Analyzing popular schedules like cosine annealing and linear decay, we found: - Success isn't tied to the specific formula. - What matters is the **smooth, controlled rate of change** over time. We introduce a key metric: the **rate function** $$\gamma(t)$$. This captures how quickly the learning rate decays at any point. --><hr /> <!-- ## A Three-Stage Recipe for Effective Learning Rate Schedules It turns out that effective schedules $$ \digamma(t) $$, like cosine annealing are **finite-time window functions** follow a distinct three-stage pattern in their rate of change $$\bar{\gamma}(t) $$: $$ \gamma(t) = \frac{1 - \digamma(t)}{t \cdot \digamma(t)} $$ We then develop **three-stage guidelines** on $$\bar{\gamma}(t) $$: 1. **Early-to-Mid Stage (Exploration)** The rate of change is kept uniformly small. This prevents the learning rate from dropping too quickly, allowing the model to freely explore the vast landscape of possible solutions and escape poor local minima. 2. **Mid-to-Late Stage (Transition)** The rate of change $$\bar{\gamma}(t) $$ increases smoothly. This is the crucial transition from broad exploration to focused fine-tuning. 3. **Late Stage (Exploitation)** The rate of change grows rapidly, causing the learning rate to plummet. This allows the model to lock onto a promising minimum and converge to a refined solution. This three-stage process ensures a balanced trade-off between exploring the problem space and exploiting promising regions. Cosine annealing and linear decay are successful precisely because they naturally exhibit this favorable *first-order dynamic behavior*. This behavior is visualized in the paper (Figure 1), showing how cosine and linear decay schedules naturally satisfy these constraints. ![Three-Stage Window](/assets/baselines_y_False.png){:width="45%"} ![Three-Stage Rate](/assets/baselines_rate_False.png){:width="45%"} --><hr /> <!-- ## Building Better, Simpler Alternatives --> <!-- Different shapes, same dynamics. This discovery is more than just a neat explanation; it's a practical blueprint for designing new learning rate schedules. If the underlying function doesn't matter, can we create simpler ones that follow the same three-stage rule? The answer is a resounding **yes**. We designed and tested several computationally cheaper alternatives based on simple polynomials and logistic sigmoid functions. By tuning these functions to match the three-stage behavior of cosine annealing, they achieved identical â€” and in some cases, slightly better â€” performance. **Experiments:** - GPT-2 models for language tasks - ResNet-18 for image classification The new, simpler schedules that adhered to the three-stage pattern performed just as well as the established baselines. Conversely, schedules that were designed to violate this pattern consistently performed worse, plateauing at a higher loss. These results are summarized in Tables 1â€“4 and Figures 5â€“13 of the paper. --><hr /> <!-- ## Decoupling the Schedule from Training Time The paper introduces another fascinating idea: replacing the standard linear progression of time $$ t / \tau $$ with a **quasi-random Kronecker sequence**. - Populates the interval from 0 to 1 uniformly but non-monotonically. - Decouples the schedule's design from the total training length. - Offers greater flexibility without sacrificing performance. --><hr /> <!-- ## ðŸ’¡ Interesting Observations In the paper the **raised-cosine window** is also linked to **Chebyshev acceleration** in convex-quadratic optimization, offering a deeper mathematical justification. --><hr /> <!-- ## What This Means for Practitioners The key takeaway from this research is a paradigm shift in how we should think about learning rate schedules. - **Focus on behavior, not formulas** Don't be dogmatic about using a cosine function. What matters is controlling the schedule's rate of change to follow the three-stage explorationâ€“exploitation pattern. - **Simpler can be better** You can use computationally cheaper functions, like simple polynomials, to achieve the same or better results as cosine annealing, potentially speeding up your workflow. - **A principled design space** This framework provides a clear, theoretical foundation for designing and tuning custom learning rate schedules tailored to specific needs â€” moving us from â€œblack magicâ€ to principled engineering. Designing schedules in $$\bar{\gamma}(t) $$-space gives both interpretability and robustness. --><hr /> <!-- In the end, cosine annealing isn't magic. It's just a very good implementation of a fundamental principle. We now have the blueprint to understand that principle and build upon it. ### Practical Impact This work provides a robust foundation for designing learning rate schedules that are both theoretically sound and empirically effective. By treating schedules as window functions, researchers and practitioners gain a flexible toolkit for optimizing training dynamics. The framework is model-agnostic and applies across architectures and datasets. The framework is particularly valuable for large-scale models and long training runs, where schedule design can significantly impact convergence and generalization. ### ðŸ“ˆ The Big Picture This isn't a tweak but a **framework** for thinking about learningâ€‘rate schedules. Once you see them as firstâ€‘order dynamic systems, you can: - Predict their behavior. - Design them systematically. - Transfer them across domains. --><hr /> <!-- ## ðŸ“š References See the paper's [summary](/summary) or the full paper for detailed derivations, experimental setups, and additional results. --><hr /> <!-- ## ðŸ›  How to Try It Iâ€™ve released a minimal PyTorch implementation with: - Dropâ€‘in schedules for HuggingFace Transformers and torchvision. - Visualization scripts for Î³(t) profiles. - Examples on GPTâ€‘2 (Shakespeare/WikiText) and ResNetâ€‘18 (CIFARâ€‘10). ## Figure Placeholders **Figure 1:** Example cosine annealing schedule with \(\bar{\gamma}(t) \) overlay. `![Cosine Annealing with Gamma Overlay](figures/cosine_gamma.png)` **Figure 2:** Polynomial schedule tuned to match threeâ€‘stage \(\bar{\gamma}(t) \) behavior. `![Polynomial Schedule with Gamma Overlay](figures/poly_gamma.png)` **Figure 3:** Logistic sigmoid schedule with equivalent \(\bar{\gamma}(t) \) dynamics. `![Sigmoid Schedule with Gamma Overlay](figures/sigmoid_gamma.png)` [**GitHub Repo â†’**](https://github.com/yourusername/horizonlr) --><hr /> <!-- ðŸ’¡ **Next up:** If youâ€™re working on largeâ€‘scale optimization and want to collaborate, letâ€™s talk. --><hr /> <!-- ![Unified Schedules](assets/fig1_unified_schedules_annotated.png) --> <!-- ![Horizon-Free Scheduling](assets/fig3_horizon_free_annotated.png) --> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2"> Page last modified: <span class="d-inline-block">Oct 25 2025 at 12:00 AM</span>. </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
