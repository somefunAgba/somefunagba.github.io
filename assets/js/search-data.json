{"0": {
    "doc": "About Me",
    "title": "About Me",
    "content": "I am Oluwasegun, a PhD candidate in Artificial Intelligence and Electrical &amp; Computer Engineering at Oregon State University, where I study stochastic gradient learning. Stochastic gradient learning can be considered as one of the most important algorithms in the world, behind the success of deep learning. As an early-career researcher, I am currently studying why training deep neural networks via the stochastic gradient algorithm succeeds in practice. While classic optimization theory motivates, they do not provide exact fundamental guarantees for the deep learning setting. My current research shows that a key factor is the presence of fundamental signal processing elements found in almost all practical stochastic gradient learning variants. Long‚Äëterm vision: To establish myself as a leader in AI, in the future, I see myself as an engineer in a related role, either as a research scientist or professor. I am open to new challenges and look to continue research that closes the gap on the key fundamental dynamics behind the learning algorithms that power deep learning. Through this, I hope to deepen our understanding of why certain learning algorithms succeed and collaborate to develop new approaches that are both reliable and grounded in theory for solving complex real-world problems. üì¨ Let‚Äôs connect: If you‚Äôre interested in research collaboration, applied AI projects, feel free to reach out via LinkedIn or Email. ",
    "url": "/about",
    
    "relUrl": "/about"
  },"1": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "AutoSGM",
    "content": "Connecting the dots ‚Ä¶ HB, NAG, Adam. Page created: Sep 11 2025 at 12:00 AM . | AutoSGM . | üåÄ The Core Update Rule | üìê Optimal Learning Rate . | Practical Approximation | EMA Realizations | Partial-correlation: Robust EMA estimate . | 1. Input Clipping | 2. Output Clipping and Max-Normalization | 3. Layer-wise smoothing | . | Alternatives: Relaxed Upper-bound variant | . | üß© Unifying PHB, NAG, and Adam | üéØ Lowpass Regularization | üìä Key Empirical Findings . | 1. GPT-2 on Shakespeare-char: ~32% lower test loss over fixed-numerator baseline. | 2. VIT on CIFAR10. | 3. ResNet18 on CIFAR10. | 4. GPT-2 on WikiText-103. | . | üèÅ Conclusion | . | . Momentum-based stochastic gradient methods such as Polyak‚Äôs Heavy Ball (PHB), Nesterov‚Äôs Accelerated Gradient (NAG), and Adam dominate deep learning optimization. They are often treated as separate algorithms, but in our recent work, we show they are all special cases of a single signal-processing (DSP) structure: the Automatic Stochastic Gradient Method (AutoSGM). AutoSGM reframes these optimizers through the lens of a first-order lowpass filter applied to the stochastic gradient, revealing: . | A principled way to tune momentum via the filter‚Äôs pole and zero. | An optimal, iteration-dependent learning rate that Adam approximates. | Momentum as a natural form of lowpass regularization that smooths the loss surface. | . All algebraic operations are sample-by-sample (elementwise) unless otherwise stated. The shorthand notation \\((t,i)\\) denotes the \\(i\\)-th element of a vector at iteration \\(t\\). ",
    "url": "/asgm.html#autosgm",
    
    "relUrl": "/asgm.html#autosgm"
  },"2": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "üåÄ The Core Update Rule",
    "content": "The classic stochastic gradient method (SGM) updates parameters as: . \\[\\mathbf{w}(t+1,i) = \\mathbf{w}(t,i) - \\alpha(t,i) \\, \\mathbf{g}(t,i)\\] where: . | \\(\\mathbf{g}(t,i) = \\nabla f(\\mathbf{w}(t,i))\\) is an unbiased stochastic gradient component, | \\(\\alpha(t,i)\\) denotes the learning rate at iteration \\(t\\), determined via a selected oracle function. | . In AutoSGM, we replace the stochastic gradient with a smoothed version: . \\[\\mathbf{w}(t+1,i) = \\mathbf{w}(t,i) - \\alpha(t,i) \\, H_{\\beta,\\gamma}(\\mathbf{g}(t,i))\\] Here, \\(H_{\\beta,\\gamma}\\) is a first-order filter with transfer function: . \\[H(z) = \\eta \\, \\frac{1 - \\gamma z^{-1}}{1 - \\beta z^{-1}}, \\quad 0 \\le \\beta &lt; 1, \\ \\gamma &lt; \\beta\\] The time-domain realization is: . \\[\\mathbf{v}(t,i) = \\beta\\,\\mathbf{v}(t-1,i) + \\eta\\,(\\mathbf{g}(t,i) - \\gamma\\,\\mathbf{g}(t-1,i))\\] See the learning dynamics of the stochastic gradient update. ",
    "url": "/asgm.html#-the-core-update-rule",
    
    "relUrl": "/asgm.html#-the-core-update-rule"
  },"3": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "üìê Optimal Learning Rate",
    "content": "We assume that both the objective function and its gradient are Lipschitz continuous (Bottou et al., 2018). Let \\(\\mathbb{E}\\) denote expectation with respect to the model distribution \\(p(\\mathbf{w})\\) parameterized by \\(\\mathbf{w}\\). For a log-likelihood objective \\(f=\\ln p(\\mathbf{w})\\), by the score-function identity, the expected gradient is zero for all \\(\\mathbf{w}\\), not only at the optimum (Moon &amp; Stirling, 2000; Van Trees et al., 2013), so that \\(\\mathbb{E}[\\mathbf{g}(t,i)] = 0\\). By minimizing the expected squared error \\(\\mathbb{E}[\\mathbf{e}(t,i)^2]\\), at iteration \\(t\\), we derive an iteration-dependent optimal learning rate . \\[\\alpha(t,i)^\\star = \\frac{\\mathbb{E}[\\mathbf{w}(t,i) \\,\\mathbf{g}(t,i)]}{\\mathbb{E}[\\mathbf{g}(t,i)^2]},\\] where \\(\\mathbf{e}(t,i) = \\mathbf{w}(t,i) - {\\mathbf{w}(i)}^\\star\\) is the parameter error vector, the gap between current weights and a local optimum. Practical Approximation . In practice, the optimal learning‚Äërate expression is realized using standard adaptive‚Äëfiltering techniques (Diniz, 2020; Haykin, 2014), which involve the following steps: . | Expectations are estimated with exponential moving averages (EMA). | For numerical stability, we use the normalized gradient form. | As a safety margin, the iteration-dependent locally-optimal learning rate estimate is modulated with a small \\(\\mu\\digamma(t)\\) which acts as a trust-region against the effect of noisy gradient scales. | . Let \\(0 \\le \\mu\\digamma(t) \\le 1\\), where \\(\\mu &gt; 0\\), \\(0\\le \\digamma(t) \\le 1\\) is a learning-rate schedule. Define . \\[\\bar{\\mathbf{g}}(t,i) = \\frac{\\mathbf{g}(t,i)}{\\sqrt{\\mathbb{E}[\\mathbf{g}(t,i)^2]}},\\] where \\(\\bar{\\mathbf{g}}(t,i)\\) is the normalized gradient scaled to its unit root-mean-square (RMS) value. The learning rate becomes . \\[\\alpha(t,i) = \\mu \\digamma(t)\\, \\frac{\\mathbb{E}[\\mathbf{w}(t,i) \\,\\bar{\\mathbf{g}}(t,i)]}{\\sqrt{\\mathbb{E}[\\mathbf{g}(t,i)^2]}}.\\] . EMA Realizations . Track the second moment of the gradient: . \\[\\mathbf{b}(t,i) = \\beta_b \\,\\mathbf{b}(t-1,i) + \\mu (1 - \\beta_b) \\,\\mathbf{g}(t,i)^2\\] ‚Üí moment estimation. Define the RMS-normalizer: . \\[\\mathbf{d}(t,i) = \\sqrt{\\frac{\\mathbf{b}(t,i)}{1 - \\beta_b^t}} + \\epsilon\\] ‚Üí bias-corrected RMS with small \\(\\epsilon\\) (Honig &amp; Messerschmitt, 1984) to prevent division by zero. Track the numerator term: . \\[\\mathbf{a}(t,i) = \\beta_a \\,\\mathbf{a}(t-1,i) + \\mu \\,\\mathbf{w}(t,i) \\,\\bar{\\mathbf{g}}(t,i)\\] ‚Üí running estimate of the weight‚Äìgradient correlation. Finally: . \\[\\alpha(t,i) = \\digamma(t)\\,\\frac{\\mathbf{a}(t,i)}{\\mathbf{d}(t,i)}.\\] Note: This reduces to only adaptive moment estimation when \\(\\mathbf{a}(t,i)\\) is replaced by a fixed constant \\(1\\). \\[\\alpha(t,i) = \\mu\\digamma(t)\\,\\frac{1}{\\mathbf{d}(t,i)}.\\] . Partial-correlation: Robust EMA estimate . The learning-rate‚Äôs EMA numerator estimates a partial-correlation. EMAs assume a well-behaved noise mode (Huber, 1992). Heavy‚Äëtailed stochastic weight-gradient noisy correlations, noisy sign flips and occasional magnitude spikes can break this assumption (Zoubir et al., 2018). As an empirical safeguard, before dividing the numerator by the denominator, we want to ensure that the numerator remains positive, well‚Äëbounded, and free from overshoot due to noisy, heavy-tailed gradient statistics. In other words, we want to robustify the partial correlation estimate from the EMA without distorting the bulk of the signal observed via its input \\(\\mathbf{u}(t,i) = \\mathbf{w}(t,i) \\, \\bar{\\mathbf{g}}(t,i)\\) . 1. Input Clipping . Heavy-tailed gradient noise statistics induce erroneous spikes that can dominate the EMA‚Äôs estimate over many iterations and increase its bias from the true mean estimate. Since, we do not know the probability distribution, Markov‚Äôs inequality gives a rationale for how often large such values can occur. The inequality . \\[\\mathbb{Pr}[|u| \\ge c\\,\\mathbb{E}[|u|] ] \\le \\frac{1}{c},\\] relates how large the magnitude of \\(u\\) can be relative to its expected magnitude. Huberisation (Zoubir et al., 2012; Menon et al., 2020) is a practical way to robustly mitigate such heavy-tailed values. The Huber clipping function bounds the most extreme outliers (\\(c \\times\\) the expected scale) before they enter the EMA, while avoiding dead zones and allowing moderate estimates to pass through untouched relative to its expected scale. \\[\\psi_{c}(u) = \\begin{cases} u, &amp; |u| \\le c\\,\\mathbb{E}[|u|] \\\\ \\mathrm{sign}(u) \\cdot c\\,\\mathbb{E}[|u|], &amp; |u| &gt; c\\,\\mathbb{E}[|u|]. \\end{cases}\\] Here, \\(u\\) denotes the instantaneous input, and \\(c\\) is a scale multiplier used to clip only extreme outliers relative to the expected scale. For example, setting \\(c=4\\) corresponds to the prior that the probability \\(p\\) of its magnitude \\(|u|\\) exceeding four times its mean is no more than \\(25\\%\\). Equivalently, the probability that \\(|u|\\) remains below this threshold is at least \\(1-p=75\\%\\). Therefore, the interval defined by the 25‚Äì75% quantiles captures the bulk of the distribution, while the clipping function suppresses only the most extreme values. This yields a robust EMA estimator that is less sensitive to heavy‚Äëtailed noise and spurious magnitude spikes. Using the instantaneous partial correlation \\(\\mathbf{u}(t,i)\\), we can adaptively estimate the expected scale, via the EMA estimate . \\[\\hat{\\mathbf{u}}(t,i) = \\beta_a \\, \\hat{\\mathbf{u}}(t-1,i) + (1 - \\beta_a)\\,|{\\mathbf{u}}(t,i)|,\\] where \\(\\hat{\\mathbf{u}}(t,i)\\) adapts to the typical scale of \\(\\mathbf{u}(t,i)\\) in each layer. 2. Output Clipping and Max-Normalization . Numerically, we want to ensure the estimate for the partial-correlation numerator stays within a predictable, and reasonable range, while ensuring \\(\\alpha(t,i) \\ge 0\\). From the inequality \\(0 \\le (\\mathbf{w}(t,i)-\\bar{\\mathbf{g}}(t,i))^2\\), we have that \\(\\mathbf{w}(t,i) \\,\\bar{\\mathbf{g}}(t,i) \\ \\le\\ \\frac{1}{2}\\,\\big(\\mathbf{w}(t,i)^2 + \\bar{\\mathbf{g}}(t,i)^2\\big),\\) and so obtain the max-bound . \\[\\mathbb{E}[\\mathbf{w}(t,i) \\,\\bar{\\mathbf{g}}(t,i)] \\le \\,\\mathbb{E}[\\mathbf{w}(t,i)^2] + \\mathbb{E}[\\bar{\\mathbf{g}}(t,i)^2] = \\mathbb{E}[\\mathbf{w}(t,i)^2] + 1.\\] \\(\\mathbb{E}[\\mathbf{w}(t,i)^2]\\) can be realized by maintaining an EMA estimate . \\[\\mathbf{s}(t,i) = \\beta_a \\,\\mathbf{s}(t-1,i) + (1 - \\beta_a) \\,\\mathbf{w}(t,i)^2,\\] and the max-normalizer is \\(\\bar{\\mathbf{s}}(t,i) = 1 + \\mathbf{s}(t,i)\\). Estimation noise can flip signs of the numerator estimate or inflate or deflate the learning rate ratio, leading to unstable or zeroed steps. When the sign flips negative. The default clipping the minimum to zero stalls learning. Since learning-rate constant \\(\\mu\\) already acts as a safety margin (trust-region), a safer trust-region approach is to take the magnitude and clip the estimated learning-rate around \\(\\mu\\) using \\(\\bar{\\mathbf{s}}(t,i)\\) and \\(\\mathbf{d}(t,i)\\). Taken together, these practical clipping techniques, helps the partial correlation estimate to remain within a predictable dynamic range, leading to . \\[\\tilde{\\mathbf{a}}(t,i) = \\beta_a \\, \\tilde{\\mathbf{a}}(t-1,i) + \\mu\\, \\bar{\\mathbf{s}}(t,i)^{-1}\\cdot{\\psi_{c} (\\mathbf{u}(t,i))}\\] \\[\\mathbf{a}(t,i) = \\max\\bigl(0,\\, \\min\\bigl( |\\tilde{\\mathbf{a}}(t,i)|, \\, \\mu\\,\\bar{\\mathbf{s}}(t,i) \\bigr) \\bigr).\\] 3. Layer-wise smoothing . To account for intra-layer structure and variability in neural networks, we observed that replacing the raw estimates with their layerwise mean, ensured more numerically stable parameter adaptation within each layer. Specifically, for a given layer \\(\\ell\\), with parameter size \\(n_\\ell\\), the numerator update is averaged as . \\[\\mathbf{a}(t,i) ‚Üê \\frac{1}{n_\\ell} \\sum_{i=1}^{n_\\ell} \\mathbf{a}(t,i).\\] . Alternatives: Relaxed Upper-bound variant . Since \\(\\mathbf{u}(t,i) = \\mathbf{w}(t,i) \\,\\bar{\\mathbf{g}}(t,i) \\ \\le\\ \\frac{1}{2}\\,\\big(\\mathbf{w}(t,i)^2 + \\bar{\\mathbf{g}}(t,i)^2\\big),\\) we may replace \\(\\mathbf{u}(t,i)\\) with a relaxed form of the symmetric upper-bound, \\(\\tilde{\\mathbf{u}}(t,i) = \\mathbf{w}(t,i)^2 + \\bar{\\mathbf{g}}(t,i)^2\\), weighted by \\(\\mu &lt; \\frac{1}{2}\\). A proxy estimate of the partial-correlation then becomes . \\[\\tilde{\\mathbf{a}}(t,i) = \\beta_a \\,\\tilde{\\mathbf{a}}(t-1,i) + (1 - \\beta_a) \\,\\mu\\,\\tilde{\\mathbf{u}}(t,i).\\] \\[\\mathbf{a}(t,i) = \\min\\bigl( |\\tilde{\\mathbf{a}}(t,i)|, \\, \\mu \\bigr).\\] Layer-wise smoothing can be applied next. ",
    "url": "/asgm.html#-optimal-learning-rate",
    
    "relUrl": "/asgm.html#-optimal-learning-rate"
  },"4": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "üß© Unifying PHB, NAG, and Adam",
    "content": "By choosing \\(\\beta, \\gamma, \\alpha(t,i)\\) appropriately, AutoSGM recovers . | Algorithm | \\(\\beta\\) | \\(\\gamma\\) | \\(\\eta\\) | \\(\\alpha(t,i)\\) | . | Basic | \\(0\\) | \\(0\\) | \\(0\\) | \\(\\mu \\digamma(t)\\) | . | PHB | \\(‚úì\\) | \\(0\\) | \\(1\\) | \\(\\mu \\digamma(t)\\) | . | NAG | \\(‚úì\\) | \\({\\beta}/{(1+\\beta)}\\) | \\((1+\\beta)\\) | \\(\\mu \\digamma(t)\\) | . | Adam | \\(‚úì\\) | \\(0\\) | \\(1-\\beta\\) | \\({\\mu} \\digamma(t) \\cdot{\\mathbf{d}(t,i)}^{-1}\\) | . ",
    "url": "/asgm.html#-unifying-phb-nag-and-adam",
    
    "relUrl": "/asgm.html#-unifying-phb-nag-and-adam"
  },"5": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "üéØ Lowpass Regularization",
    "content": "Incorporating momentum is known to practically help stabilize learning dynamics and avoid shallow local minima (Haykin, 2008). In the paper, we use the impulse response of the filter to show that smoothing the gradient (also called momentum) is approximately equivalent to smoothing the loss surface: . This Lowpass regularization due to smoothing the gradient reflects the stabilized training effect of: . | reduced noise in the gradient updates, | improved convergence to flatter local minima, | . often observed. ",
    "url": "/asgm.html#-lowpass-regularization",
    
    "relUrl": "/asgm.html#-lowpass-regularization"
  },"6": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "üìä Key Empirical Findings",
    "content": "Using Adam as a fixed-numerator baseline for the learning rate, we tested the AutoSGM framework using our iteration-dependent learning-rate realization on CIFAR-10 (ViT, ResNet) and language modeling (GPT-2 on WikiText and Shakespeare): . | Tuning the filter‚Äôs zero \\(\\gamma\\) improved performance in most cases. | Iteration-dependent learning rate numerator (circled dots) outperformed fixed numerator (squared dots). | . 1. GPT-2 on Shakespeare-char: ~32% lower test loss over fixed-numerator baseline. | | . 2. VIT on CIFAR10. | | . 3. ResNet18 on CIFAR10. | | . 4. GPT-2 on WikiText-103. | | . ",
    "url": "/asgm.html#-key-empirical-findings",
    
    "relUrl": "/asgm.html#-key-empirical-findings"
  },"7": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "üèÅ Conclusion",
    "content": "AutoSGM offers a unified, interpretable, and tunable framework for momentum-based optimization. By viewing PHB, NAG, and Adam as points in the AutoSGM parameter space, we can: . | Design new stochastic gradient optimizers with principled stability and error bounds. | Achieve better generalization through lowpass regularization. | Improve learning rate algorithms. | . üí° Takeaway: If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. AutoSGM gives you the structure or map. | Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. SIAM Review, 60(2), 223‚Äì311. | Moon, T. K., &amp; Stirling, W. C. (2000). Mathematical Methods and Algorithms for Signal Processing. Prentice Hall. | Van Trees, H. L., Bell, K. L., &amp; Tian, Z. (2013). Detection Estimation and Modulation Theory, Detection, Estimation, and Filtering Theory, Part I (2nd ed.). Wiley. | Diniz, P. S. R. (2020). Adaptive Filtering: Algorithms and Practical Implementation (5th edition). Springer International Publishing. https://doi.org/10.1007/978-3-030-29057-3 | Haykin, S. (2014). Adaptive Filter Theory (5th, intern.). Pearson. | Honig, M. L., &amp; Messerschmitt, D. G. (1984). Adaptive Filters: Structures, Algorithms and Applications. Kluwer Academic Publishers. | Huber, P. J. (1992). Robust Estimation of a Location Parameter. In S. Kotz &amp; N. L. Johnson (Eds.), Breakthroughs in Statistics: Methodology and Distribution (pp. 492‚Äì518). Springer. https://doi.org/10.1007/978-1-4612-4380-9_35 | Zoubir, A. M., Koivunen, V., Ollila, E., &amp; Muma, M. (2018). Robust Statistics for Signal Processing (1st ed.). Cambridge University Press. https://doi.org/10.1017/9781139084291 | Zoubir, A. M., Koivunen, V., Chakhchoukh, Y., &amp; Muma, M. (2012). Robust Estimation in Signal Processing: A Tutorial-Style Treatment of Fundamental Concepts. IEEE Signal Processing Magazine, 29(4), 61‚Äì80. | Menon, A. K., Rawat, A. S., Reddi, S. J., &amp; Kumar, S. (2020). Can Gradient Clipping Mitigate Label Noise? Proceedings of the 8th International Conference on Learning Representations. | Haykin, S. (2008). Neural Networks and Learning Machines (3rd edition). Pearson. | . ",
    "url": "/asgm.html#-conclusion",
    
    "relUrl": "/asgm.html#-conclusion"
  },"8": {
    "doc": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "title": "AutoSGM: Unifying Momentum Methods for Better Learning",
    "content": " ",
    "url": "/asgm.html",
    
    "relUrl": "/asgm.html"
  },"9": {
    "doc": "Learning Dynamics",
    "title": "Stochastic Gradient Learning Dynamics",
    "content": "Linear Time (Iteration) Varying System. Given a gradient-generating system like a deep neural network, the AutoSGM framework exposes the exact update trajectory of each trainable parameter via the stochastic gradient algorithm under a lowpass filter (momentum) and iteration-dependent learning-rate oracle as the dynamics of a first-order linear time (iteration) varying (LTV) filter. This LTV description makes it possible to apply linear systems, control and signal‚Äëprocessing tools to reason about stability, transient response, noise attenuation and steady-state convergence tradeoffs. | Stochastic Gradient Learning Dynamics . | Stability | Iteration-dependent Learning-rate Oracle | Decoupled weight decay is not decoupled after all! | . | . Formally, at each iteration \\(t\\), a single parameter \\(\\mathbf{w}[t, i]\\) update via its gradient component \\(\\mathbf{g}[t, i]\\) follows the trajectory . \\[\\Delta\\mathbf{w}[t+1, i] = \\beta\\,r[t, i]\\cdot\\Delta \\mathbf{w}[t,i] + \\eta\\,\\alpha[t,i]\\cdot \\mathbf{e}[t,i],\\] with input as . \\[\\mathbf{e}[t, i] = \\bigl(\\gamma\\,\\mathbf{g}[t-1, i] - \\mathbf{g}[t, i] \\bigr) + \\rho\\,\\eta^{-1}\\bigl(\\beta \\,\\tilde{\\mathbf{w}}[t-1, i] - \\tilde{\\mathbf{w}}[t, i]\\bigr),\\] or . \\[\\mathbf{e}[t, i] = \\bigl(\\gamma\\,\\mathbf{g}[t-1, i] - \\mathbf{g}[t, i] \\bigr) + \\rho^{\\prime}\\bigl(\\gamma \\,{\\mathbf{w}}[t-1, i] - {\\mathbf{w}}[t, i]\\bigr),\\] and finally, . \\(\\mathbf{w}[t+1, i] = \\mathbf{w}[t,i] + \\Delta \\mathbf{w}[t+1,i]\\), . where . | \\(\\alpha[t,i] = \\frac{\\mu}{1-\\beta^{t}}\\,\\digamma[t] \\cdot\\frac{\\mathbf{a}[t,i]}{\\mathbf{d}[t,i]}\\) is the iteration-dependent learning rate oracle, with a trust-region constant \\(0 &lt; \\mu &lt; 1\\), a window function \\(0 \\le \\digamma[t] \\le 1\\) as the learning-rate schedule. The learning-rate‚Äôs numerator and denominator functions are respectively \\(\\mathbf{a}[t,i]\\), \\(\\mathbf{d}[t,i]\\). | \\(r[t,i] = \\alpha[t,i]/\\alpha[t-1,i]\\) is the learning rate ratio, | \\(\\beta\\) is the lowpass filter‚Äôs pole parameter selected for stability \\(0 \\le \\beta &lt; 1\\), | \\(\\gamma\\) is the lowpass filter‚Äôs zero parameter, selected such that \\(\\gamma &lt; \\beta\\), | \\(\\eta\\) is a constant selected as \\((1-\\beta)/(1-\\gamma)\\) such the steady-state (DC) gain of the lowpass filter is unity, | \\(\\rho \\ge 0\\) is a small weight-decay constant, selected relative to \\(\\eta\\), | \\(\\tilde{\\mathbf{w}}[t, i] = \\mathbf{d}[t,i]\\,{\\mathbf{w}}[t, i]\\) is the scaled parameter via the learning-rate‚Äôs denominator. | . Under the mild assumptions of local smoothness of the loss function generating the gradient, and bounded gradient moments, the trajectory input \\(\\mathbf{e}[t, i]\\) is bounded. Stability . The lowpass pole \\(\\beta\\) shapes the exponential stability margin of the system \\(\\lim_{t\\to\\infty} \\| r[t,i] \\|&lt; \\beta^{-1}\\) and the low-frequency properties of the lowpass filter. For uniform exponential stability (boundedness and asymptotic behavior of the trajectory solution): . | Stochastic gradient, LTI system: \\(0 &lt; \\beta &lt; 1\\) (necessary and sufficient). | Stochastic gradient, LTV system: \\(0 &lt; \\beta &lt; 1\\), \\(\\sup_t \\alpha[t,i] &lt; \\infty\\) (necessary and sufficient). | . In addition, if \\(0 &lt; \\beta &lt; 1\\) to sufficiently ensure asymptotic stability, \\(\\| r[t,i] \\| \\to 0\\) as \\(t\\) grows large, we need \\(\\digamma[t] \\to 0\\) as \\(t \\to \\infty\\). This explains why learning-rate annealing is important. The iteration-dependent learning rate (or gain) \\(\\alpha[t,i]\\) controls both the stability and convergence of the system. For BIBO stability: . | \\(0 &lt; \\beta &lt; 1\\), and \\(\\sup_t \\alpha[t,i] &lt; \\infty\\) (necessary and sufficient). | . ensures a bounded input leads to a bounded trajectory output \\(\\Delta\\mathbf{w}[t+1, i]\\). Iteration-dependent Learning-rate Oracle . The iteration-dependent denominator part of an optimal choice of learning rate is an adaptive moment estimator . \\(\\mathbf{d}[(t,i)] = {\\mathbb{E}[\\mathbf{g}(t,i)^2]}\\). The iteration-dependent numerator part of an optimal choice of learning rate is a partial correlation estimator . \\(\\mathbf{a}[(t,i)] = {\\mathbb{E}[\\mathbf{w}(t,i)\\mathbf{g}(t,i)]}\\). Decoupled weight decay is not decoupled after all! . The input trajectory for what has been called decoupled weight-decay . \\[\\mathbf{e}[t, i] = \\bigl(\\gamma\\,\\mathbf{g}[t-1, i] - \\mathbf{g}[t, i] \\bigr) + \\rho\\,\\eta^{-1}\\bigl(\\beta \\,\\tilde{\\mathbf{w}}[t-1, i] - \\tilde{\\mathbf{w}}[t, i]\\bigr),\\] is exactly equivalent to the standard weight-decay . \\[\\mathbf{e}[t, i] = \\bigl(\\gamma\\,\\mathbf{g}[t-1, i] - \\mathbf{g}[t, i] \\bigr) + \\rho^{\\prime}\\bigl(\\gamma \\,{\\mathbf{w}}[t-1, i] - {\\mathbf{w}}[t, i]\\bigr),\\] when \\(\\rho^{\\prime} = \\rho \\eta^{-1}\\), \\(\\beta=\\gamma\\), and \\(\\tilde{\\mathbf{w}}[t, i] = {\\mathbf{w}}[t, i]\\). ",
    "url": "/learning_dynamics#stochastic-gradient-learning-dynamics",
    
    "relUrl": "/learning_dynamics#stochastic-gradient-learning-dynamics"
  },"10": {
    "doc": "Learning Dynamics",
    "title": "Learning Dynamics",
    "content": " ",
    "url": "/learning_dynamics",
    
    "relUrl": "/learning_dynamics"
  },"11": {
    "doc": "Home",
    "title": "Signal processing meets deep learning optimization",
    "content": "A notebook for my current research. From control theory and signal processing foundations to modern optimization methods for deep learning. About Me . AutoSGM View it on GitHub . PID View it on GitHub . | About Me | ",
    "url": "/#signal-processing-meets-deep-learning-optimization",
    
    "relUrl": "/#signal-processing-meets-deep-learning-optimization"
  },"12": {
    "doc": "Home",
    "title": "About Me",
    "content": ". --> ",
    "url": "/#about-me",
    
    "relUrl": "/#about-me"
  },"13": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"14": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "Why Cosine Annealing Works",
    "content": "and Why We Don‚Äôt Actually Need It. | Why Cosine Annealing Works . | The Secret Isn‚Äôt the Shape, It‚Äôs the Rate of Change | A Three-Stage Recipe for Effective Learning Rate Schedules | Building Better, Simpler Alternatives | Decoupling the Schedule from Training Time | üí° Interesting Observations | What This Means for Practitioners . | Practical Impact | üìà The Big Picture | . | üìö References | . | . If you‚Äôve trained a large neural network in the last few years, you‚Äôve probably used or heard of cosine annealing. It‚Äôs a go-to learning rate schedule, celebrated for its ability to coax better performance out of deep learning models. Learning rate schedules are the unsung heroes of deep learning optimization. In the ever-evolving landscape of deep learning, learning rate schedules play a pivotal role in training stability and generalization. The idea is simple: You start with a higher learning rate and gradually decrease it following a cosine curve, ending near zero. It‚Äôs a standard trick of the trade that consistently delivers smoother convergence and better results. But a fundamental question has lingered: Why is it so effective? Is there something magical about the cosine function itself? . In this paper, Annealing via Window Functions, we emerge with a powerful insight: The magic isn‚Äôt in the cosine at all. It‚Äôs in its behavior. This insight not only provides an explanation for the success of commonly-used annealing functions but also opens the door to simpler, more efficient alternatives. ",
    "url": "/lrwinds.html#why-cosine-annealing-works",
    
    "relUrl": "/lrwinds.html#why-cosine-annealing-works"
  },"15": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "The Secret Isn‚Äôt the Shape, It‚Äôs the Rate of Change",
    "content": "We reframe learning rate schedules through the lens of classical signal processing, viewing them as finite-time window functions ‚Äî functions that shape a signal over a finite interval. Such functions have been heavily studied and applied in spectral analysis. In deep learning, they control how aggressively the learning algorithm explores vs. exploits in the parameter space. Analyzing popular schedules like cosine annealing and linear decay, we found: . | Success isn‚Äôt tied to the specific formula. | What matters is the smooth, controlled rate of change over time. | . We introduce a key metric: the rate function \\(\\gamma(t)\\). This captures how quickly the learning rate decays at any point. ",
    "url": "/lrwinds.html#the-secret-isnt-the-shape-its-the-rate-of-change",
    
    "relUrl": "/lrwinds.html#the-secret-isnt-the-shape-its-the-rate-of-change"
  },"16": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "A Three-Stage Recipe for Effective Learning Rate Schedules",
    "content": "It turns out that effective schedules \\(\\digamma(t)\\), like cosine annealing are finite-time window functions follow a distinct three-stage pattern in their rate of change \\(\\bar{\\gamma}(t)\\): . \\[\\gamma(t) = \\frac{1 - \\digamma(t)}{t \\cdot \\digamma(t)}\\] We then develop three-stage guidelines on \\(\\bar{\\gamma}(t)\\): . | Early-to-Mid Stage (Exploration) The rate of change is kept uniformly small. This prevents the learning rate from dropping too quickly, allowing the model to freely explore the vast landscape of possible solutions and escape poor local minima. | Mid-to-Late Stage (Transition) The rate of change \\(\\bar{\\gamma}(t)\\) increases smoothly. This is the crucial transition from broad exploration to focused fine-tuning. | Late Stage (Exploitation) The rate of change grows rapidly, causing the learning rate to plummet. This allows the model to lock onto a promising minimum and converge to a refined solution. | . This three-stage process ensures a balanced trade-off between exploring the problem space and exploiting promising regions. Cosine annealing and linear decay are successful precisely because they naturally exhibit this favorable first-order dynamic behavior. This behavior is visualized in the paper (Figure 1), showing how cosine and linear decay schedules naturally satisfy these constraints. ",
    "url": "/lrwinds.html#a-three-stage-recipe-for-effective-learning-rate-schedules",
    
    "relUrl": "/lrwinds.html#a-three-stage-recipe-for-effective-learning-rate-schedules"
  },"17": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "Building Better, Simpler Alternatives",
    "content": "Different shapes, same dynamics. This discovery is more than just a neat explanation; it‚Äôs a practical blueprint for designing new learning rate schedules. If the underlying function doesn‚Äôt matter, can we create simpler ones that follow the same three-stage rule? The answer is a resounding yes. We designed and tested several computationally cheaper alternatives based on simple polynomials and logistic sigmoid functions. By tuning these functions to match the three-stage behavior of cosine annealing, they achieved identical ‚Äî and in some cases, slightly better ‚Äî performance. Experiments: . | GPT-2 models for language tasks | ResNet-18 for image classification | . The new, simpler schedules that adhered to the three-stage pattern performed just as well as the established baselines. Conversely, schedules that were designed to violate this pattern consistently performed worse, plateauing at a higher loss. These results are summarized in Tables 1‚Äì4 and Figures 5‚Äì13 of the paper. ",
    "url": "/lrwinds.html#building-better-simpler-alternatives",
    
    "relUrl": "/lrwinds.html#building-better-simpler-alternatives"
  },"18": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "Decoupling the Schedule from Training Time",
    "content": "The paper introduces another fascinating idea: replacing the standard linear progression of time \\(t / \\tau\\) with a quasi-random Kronecker sequence. | Populates the interval from 0 to 1 uniformly but non-monotonically. | Decouples the schedule‚Äôs design from the total training length. | Offers greater flexibility without sacrificing performance. | . ",
    "url": "/lrwinds.html#decoupling-the-schedule-from-training-time",
    
    "relUrl": "/lrwinds.html#decoupling-the-schedule-from-training-time"
  },"19": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "üí° Interesting Observations",
    "content": "In the paper the raised-cosine window is also linked to Chebyshev acceleration in convex-quadratic optimization, offering a deeper mathematical justification. ",
    "url": "/lrwinds.html#-interesting-observations",
    
    "relUrl": "/lrwinds.html#-interesting-observations"
  },"20": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "What This Means for Practitioners",
    "content": "The key takeaway from this research is a paradigm shift in how we should think about learning rate schedules. | Focus on behavior, not formulas Don‚Äôt be dogmatic about using a cosine function. What matters is controlling the schedule‚Äôs rate of change to follow the three-stage exploration‚Äìexploitation pattern. | Simpler can be better You can use computationally cheaper functions, like simple polynomials, to achieve the same or better results as cosine annealing, potentially speeding up your workflow. | A principled design space This framework provides a clear, theoretical foundation for designing and tuning custom learning rate schedules tailored to specific needs ‚Äî moving us from ‚Äúblack magic‚Äù to principled engineering. Designing schedules in \\(\\bar{\\gamma}(t)\\)-space gives both interpretability and robustness. | . In the end, cosine annealing isn‚Äôt magic. It‚Äôs just a very good implementation of a fundamental principle. We now have the blueprint to understand that principle and build upon it. Practical Impact . This work provides a robust foundation for designing learning rate schedules that are both theoretically sound and empirically effective. By treating schedules as window functions, researchers and practitioners gain a flexible toolkit for optimizing training dynamics. The framework is model-agnostic and applies across architectures and datasets. The framework is particularly valuable for large-scale models and long training runs, where schedule design can significantly impact convergence and generalization. üìà The Big Picture . This isn‚Äôt just a tweak ‚Äî it‚Äôs a framework for thinking about learning‚Äërate schedules. Once you see them as first‚Äëorder dynamic systems, you can: . | Predict their behavior. | Design them systematically. | Transfer them across domains. | . ",
    "url": "/lrwinds.html#what-this-means-for-practitioners",
    
    "relUrl": "/lrwinds.html#what-this-means-for-practitioners"
  },"21": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "üìö References",
    "content": "See the paper‚Äôs summary or the full paper for detailed derivations, experimental setups, and additional results. üí° Next up: If you‚Äôre working on large‚Äëscale optimization and want to collaborate, let‚Äôs talk. ",
    "url": "/lrwinds.html#-references",
    
    "relUrl": "/lrwinds.html#-references"
  },"22": {
    "doc": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "title": "Learning-Rate Annealing as Controlled First-Order Dynamic Systems",
    "content": " ",
    "url": "/lrwinds.html",
    
    "relUrl": "/lrwinds.html"
  },"23": {
    "doc": "Summary",
    "title": "Annealing via Window Functions",
    "content": "Practical Learning-Rate Schedules with Controlled First-Order Behavior. We unify cosine annealing, linear decay, and new alternatives under a single signal-processing inspired framework. This lets you design schedules with the same convergence benefits, but more flexibility, efficiency, and theoretical grounding. | Annealing via Window Functions . | Overview | A Unified Framework for Learning Rate Annealing . | From Heuristics to Principles: Annealing as Signal Processing | The Simulated Annealing Analogy: Deriving First-Order Dynamics | The Time-Normalized Relative Rate \\(\\bar{\\gamma}(t)\\): A Universal Metric for Schedule Behavior | The Three-Stage Constraint: A Principled Approach to Exploration and Exploitation | . | Analysis of Canonical and Alternative Window Functions . | Deconstructing Baseline Schedules: Raised-Cosine and Linear Decay | A Design Space of Alternatives: Parametric Exponential, Polynomial, and Sigmoid Functions | The Impact of Shaping Parameters on First-Order Dynamics and Acceptance Collapse | Decoupling from Training Duration: The Role of the Kronecker Input Sequence | . | Empirical Validation Across Diverse Architectures and Datasets . | Experimental Design and Methodology | Performance Analysis: When First-Order Dynamics Align | The Cost of Violation: Quantifying Performance Degradation | Generalizability and Flexibility | . | Theoretical Underpinnings and Broader Implications . | Beyond Heuristics: The Connection to Chebyshev Acceleration | Practical Implications for Schedule Design and Selection | Concluding Remarks | . | . | . ",
    "url": "/summary#annealing-via-window-functions",
    
    "relUrl": "/summary#annealing-via-window-functions"
  },"24": {
    "doc": "Summary",
    "title": "Overview",
    "content": "The present paper contributes to the field of deep learning optimization by reframing the design of learning-rate annealing schedules from an empirical art into a principled, analytical science rooted in classical signal processing. Its central thesis posits that the empirical success of popular schedules, such as cosine annealing, is not attributable to their specific functional form but rather to their underlying first-order dynamic properties. The research makes three primary contributions. First, it introduces a unified theoretical framework that interprets a broad family of learning-rate schedules as signal-shaping window functions. The behavior of these functions is characterized by a novel, dimensionless metric: the time-normalized relative rate of change, denoted as \\(\\bar{\\gamma}(t)\\). This metric provides a universal basis for comparing the dynamic behavior of disparate schedules. Second, the paper derives and proposes a set of three-stage, piecewise constraints on the profile of \\(\\bar{\\gamma}(t)\\). These constraints are designed to enforce a controlled and balanced transition from high-exploration dynamics in the early stages of training to high-exploitation dynamics in the late stages, providing a principled foundation for schedule design. Third, the framework is used to synthesize a variety of novel, computationally efficient, and tunable learning-rate schedules. The authors demonstrate that by matching the first-order dynamics prescribed by the framework, these alternative schedules can achieve performance on par with, and sometimes superior to, established baselines. The paper‚Äôs theoretical claims are substantiated through extensive empirical validation. Across a diverse set of experiments involving GPT-2 and ResNet-18 models on language modeling and image classification tasks, the results consistently demonstrate that the performance of an annealing schedule is dictated by its adherence to the proposed first-order dynamic constraints. Schedules that satisfy the three-stage conditions on \\(\\bar{\\gamma}(t)\\) are shown to be effectively interchangeable, converging to statistically identical performance plateaus. Conversely, schedules deliberately designed to violate these conditions exhibit predictably poorer performance, characterized by slower convergence and higher final loss or error rates. This robust validation establishes the framework not only as a powerful explanatory tool but also as a practical guide for the design and selection of effective learning-rate schedules. ",
    "url": "/summary#overview",
    
    "relUrl": "/summary#overview"
  },"25": {
    "doc": "Summary",
    "title": "A Unified Framework for Learning Rate Annealing",
    "content": "The paper‚Äôs central innovation is the development of a cohesive theoretical framework that demystifies the behavior of learning-rate annealing schedules. By moving beyond empirical observation, it establishes a set of first principles that govern schedule effectiveness, grounded in analogies to signal processing and simulated annealing. From Heuristics to Principles: Annealing as Signal Processing . The research begins by addressing a fundamental, yet largely unanswered, question in deep learning optimization: why does cosine annealing work so effectively? Despite its widespread adoption and consistent empirical success, the mechanisms underlying its performance have remained poorly understood. The paper‚Äôs foundational step is to re-contextualize this problem by drawing a powerful analogy to the well-established field of signal processing. It proposes that learning-rate schedules should not be viewed as arbitrary mathematical functions but as classical signal-shaping ‚Äúwindow functions,‚Äù a concept with a rich theoretical background. This reframing is a critical intellectual leap. By identifying cosine annealing as mathematically equivalent to a half-period, second-order raised-cosine (or Hann) window, the authors unlock a new analytical vocabulary. Concepts such as tapering, spectral properties, and time-domain dynamics, which are standard in signal processing, can now be applied to understand the behavior of learning rates during training. This cross-disciplinary approach provides a new, more powerful lens through which to view all schedules. It shifts the focus from the static ‚Äúshape‚Äù of the function to its dynamic properties over the training interval, ultimately leading to the discovery that the schedule‚Äôs rate of change, not its specific formula, is the key determinant of its success. The Simulated Annealing Analogy: Deriving First-Order Dynamics . To build a formal model, the framework draws upon the principles of simulated annealing, a classic optimization concept. In physical annealing, the rate at which temperature is lowered (the ‚Äúcooling schedule‚Äù) is paramount to achieving a low-energy, highly-ordered final state; cooling too quickly or too slowly results in a defective structure. The paper posits that the learning rate schedule, \\(\\digamma(t)\\), plays an analogous role to this cooling function in stochastic gradient descent. This analogy is formalized by introducing a \\(\\delta\\)-dependent acceptance function, \\(\\Phi(t) = e^{-\\delta\\theta(t)}\\), where \\(\\theta(t) = \\digamma(t)^{-1}\\) and \\(\\delta\\) represents the maximum absolute change in the learning cost function per iteration, which is shown to be bounded under standard assumptions (given in the paper‚Äôs Appendix A). This function mathematically captures the intuitive goal of the training process: to transition smoothly from a state of high exploration (high acceptance of large parameter updates, facilitated by a large \\(\\digamma(t)\\)) to a state of high exploitation (low acceptance, allowing for convergence to a local minimum with small updates from a small \\(\\digamma(t)\\)). This step is pivotal as it forges a direct mathematical link between the abstract exploration-exploitation trade-off and the concrete, measurable properties of the learning rate schedule \\(\\digamma(t)\\). The Time-Normalized Relative Rate \\(\\bar{\\gamma}(t)\\): A Universal Metric for Schedule Behavior . The framework‚Äôs core analytical tool is derived by imposing a simple, physically motivated constraint on the acceptance function: it should not decay faster than intended between successive iterations. This is expressed as \\(\\Phi(t+1) \\ge \\beta(t)\\Phi(t)\\), which prevents a greedy, premature collapse into a suboptimal minimum. From this inequality, the authors derive the key variable of the paper: the time-normalized relative rate of change, defined for \\(t&gt;0\\) as: . \\[\\overline{\\gamma}(t) = \\frac{1 - \\digamma(t)}{t\\digamma(t)}\\] This quantity, \\(\\overline{\\gamma}(t)\\), is presented as a dimensionless metric, a crucial property that allows it to serve as a universal comparator of schedule dynamics. By normalizing the instantaneous rate of change by both the current schedule value \\(\\digamma(t)\\) and the elapsed time \\(t\\), \\(\\overline{\\gamma}(t)\\) captures the intrinsic dynamic behavior of any schedule, independent of its specific functional form or total duration. This metric can be interpreted as a measure of ‚Äúoptimization pressure.‚Äù The numerator, \\(1 - \\digamma(t)\\), represents the total decay the schedule has undergone from its starting value of \\(1\\). The denominator, \\(t\\digamma(t)\\), can be seen as a time-weighted measure of the current learning capacity. The ratio, \\(\\overline{\\gamma}(t)\\), therefore quantifies the aggressiveness of the decay relative to the current state of the schedule. A low \\(\\overline{\\gamma}(t)\\) signifies that the decay has been gradual, maintaining a high degree of ‚Äúexploration pressure.‚Äù Conversely, a high \\(\\overline{\\gamma}(t)\\) indicates that the decay has been rapid, signaling a decisive shift toward ‚Äúexploitation pressure.‚Äù This physical intuition explains why controlling the profile of \\(\\overline{\\gamma}(t)\\) over time is fundamental to managing the training process effectively. The Three-Stage Constraint: A Principled Approach to Exploration and Exploitation . Based on the concept of controlled optimization pressure, the paper proposes a set of prescriptive, piecewise constraints on the profile of \\(\\overline{\\gamma}(t)\\) over the training interval \\(\\tau\\). These constraints define what the authors term favorable first-order dynamics. | Early-to-mid stage (\\(0 \\le t \\le \\tau/2\\)): The framework requires \\(\\overline{\\gamma}(t)\\) to be uniformly small, specifically bounded such that \\(\\digamma(t) \\ge 1/2\\) (for a conservative choice of a slack variable). This constraint acts as a safeguard against premature decay, forcing the optimizer to maintain a high learning rate and engage in sustained exploration during the critical first half of training. | Late stage (\\(t \\ge \\tau-1\\)): To ensure effective convergence, the constraint \\(\\overline{\\gamma}(t) \\ge 1\\) is imposed, which corresponds to a learning rate \\(\\digamma(t) \\le 1/\\tau\\). This pushes the optimizer into a high-exploitation regime, enabling fine-grained search around a potential minimum. | Mid-to-late stage (\\(\\tau/2 \\le t &lt; \\tau\\)): A smooth transition between the early and late stage regimes is enforced to prevent sharp, potentially destabilizing changes in the learning rate dynamics. | . Together, these three stages define a controlled, monotonic profile for \\(\\overline{\\gamma}(t)\\): it should start low and relatively flat, then rise smoothly and accelerate towards the end of training. These conditions are not arbitrary bounds; they represent a principled recipe for applying the right amount of optimization pressure at the right time. The paper provides a compelling visual proof in Figure 1, which shows that the \\(\\overline{\\gamma}(t)\\) profiles of both standard cosine and linear decay schedules naturally adhere to this three-stage template, offering a novel and powerful explanation for their long-observed empirical success. ",
    "url": "/summary#a-unified-framework-for-learning-rate-annealing",
    
    "relUrl": "/summary#a-unified-framework-for-learning-rate-annealing"
  },"26": {
    "doc": "Summary",
    "title": "Analysis of Canonical and Alternative Window Functions",
    "content": "The theoretical framework provides a powerful lens for both analyzing existing schedules and generating novel, effective alternatives. This section examines how the principles of first-order dynamics apply to canonical schedules and explores the design space of new functions synthesized using the framework. Deconstructing Baseline Schedules: Raised-Cosine and Linear Decay . The paper first applies its framework to the two most common learning rate schedules: raised-cosine (cosine annealing) and linear decay. The raised-cosine schedule is defined as: . \\[\\digamma(t) = \\mathcal{R}(t)[l + (1-l)\\cos^2(0.5\\pi x(t))],\\] where \\(x(t) = t/\\tau\\) and \\(l\\) is the minimum output value, typically \\(0\\). The linear decay function is simply . \\[\\digamma(t) = \\mathcal{R}(t)(1-x(t)).\\] While these functions have very different mathematical forms: one is trigonometric, and the other is a polynomial; the analysis of their corresponding \\(\\overline{\\gamma}(t)\\) profiles reveals a striking similarity. As visualized in Figure 1 of the paper, both schedules produce a \\(\\overline{\\gamma}(t)\\) curve that remains low and nearly constant for the first half of the training interval before rising smoothly in the second half. This demonstrates that both functions, despite their differences, inherently satisfy the three-stage constraint for favorable first-order dynamics. This finding serves as the first key piece of evidence for the paper‚Äôs central thesis: the shared dynamic behavior, not the specific function, is the source of their effectiveness. A Design Space of Alternatives: Parametric Exponential, Polynomial, and Sigmoid Functions . Having established that the framework can explain existing schedules, the paper then demonstrates its generative power by introducing three new families of tunable, computationally efficient window functions : . | \\(\\beta\\)-parametric exponential function: Defined as \\(\\digamma(t) = \\mathcal{R}(t)(\\beta^{x(t)} - \\beta) / (1 - \\beta)\\), this function smoothly interpolates between linear decay (as \\(\\beta \\rightarrow 1\\)) and a sharp exponential decay (as \\(\\beta \\rightarrow 0\\)). | Simple polynomial functions: A general form \\(\\digamma(t) = \\mathcal{R}(t)(1 - x(t)^i)^n\\) is proposed as a computationally cheaper alternative to the raised-cosine. For instance, with \\(n=2\\) and \\(i \\approx 1.8\\), it closely mimics the shape of the cosine schedule. | Logistic sigmoid function: A shifted and scaled logistic function, \\(\\digamma(t) = \\mathcal{R}(t)\\delta_h^{-1}[(1 + b^{2i(x(t)-0.5)})^{-1} - \\underline{h}]\\), offers another way to approximate the desired decay profile, with a parameter \\(i\\) controlling the steepness of the transition. | . These functions are not presented as inherently superior to the baselines but as proof of concept. They illustrate that one can start with the target dynamic profile: the three-stage constraints on \\(\\overline{\\gamma}(t)\\); and then engineer multiple, distinct, and practical functions that successfully implement it. The Impact of Shaping Parameters on First-Order Dynamics and Acceptance Collapse . A crucial part of the analysis involves examining how the shaping parameters of these new function families (i.e., \\(\\beta\\) for the exponential, \\(i\\) for the polynomial, and \\(i\\) for the sigmoid) influence the \\(\\overline{\\gamma}(t)\\) profile and, consequently, training stability. The paper introduces the concept of acceptance collapse a phenomenon where an improperly configured schedule causes the acceptance function \\(\\Phi(t)\\)to plummet to zero prematurely, effectively halting meaningful exploration and trapping the optimizer. The analysis, supported by Figures 2, 3, and 4, demonstrates a clear causal chain: . | A poor choice of a shaping parameter (e.g., a very small \\(\\beta\\), a tiny or very large \\(i\\)) leads to a \\(\\overline{\\gamma}(t)\\) profile that severely violates the three-stage constraints. | This malformed \\(\\overline{\\gamma}(t)\\) profile results in a premature or abrupt acceptance collapse. | This collapse in acceptance leads to poor training outcomes, as later validated by the experiments. | . This analysis reveals that the framework does more than just describe schedules. It provides a safe operating area for their design. The design recommendations in Appendix B explicitly define the ranges of the shaping parameters that ensure the schedule‚Äôs dynamics remain compliant (e.g., \\(\\beta \\approx 1\\), moderate \\(i\\) for polynomials between \\(1\\) and \\(10\\), small-to-moderate \\(i &lt; 6\\) for logistics). This transforms the task of schedule tuning from a black-box hyperparameter search into a more principled process of selecting parameters that keep the schedule‚Äôs dynamics within the prescribed safe bounds. The following table summarizes these characteristics. | Window Function Family | Mathematical Form \\(\\digamma(t)\\) | Shaping Parameter(s) | Characteristic \\(\\overline{\\gamma}(t)\\) Behavior | Recommended Safe Parameter Range | . | \\(\\beta\\)-Parametric Exponential | \\(\\mathcal{R}(t)\\frac{\\beta^{x(t)} - \\beta}{1 - \\beta}\\) | \\(\\beta \\in (0, 1)\\) | As \\(\\beta \\rightarrow 0\\), \\(\\overline{\\gamma}(t)\\) spikes early, violating the early-stage constraint. As \\(\\beta \\rightarrow 1\\), it approaches linear decay. | \\(\\beta \\approx 1\\) | . | Simple Polynomial | \\(\\mathcal{R}(t)\\left(1 - x(t)^i\\right)^n\\) | \\(i &gt; 0, \\; n &gt; 0\\) | As \\(i \\rightarrow 0\\), decay is too rapid (early collapse). As \\(i \\rightarrow \\infty\\), decay is too delayed (late collapse). | \\(1 &lt; i &lt; 10\\) (for \\(n = 2\\)) | . | Logistic Sigmoid | Scaled &amp; Shifted Sigmoid | \\(i &gt; 0\\) (slope) | As \\(i\\) becomes large, the transition sharpens, violating the mid‚Äìlate stage smoothness constraint. | \\(i &lt; 6\\) (prevent sharp transitions around mid-inflection) | . Decoupling from Training Duration: The Role of the Kronecker Input Sequence . In a subtle but significant contribution, the paper addresses a practical limitation of most schedules: their definition relies on a pre-defined total number of training iterations, \\(\\tau\\), through the input variable \\(x(t) = t/\\tau\\). To overcome this, the authors propose replacing this uniformly spaced sequence with a quasi-random Kronecker sequence: . \\[x(t) = \\phi t \\pmod 1, \\quad \\text{where } \\phi = 0.5(-1 + \\sqrt{5})\\] This sequence, related to the golden ratio, has the property of populating the unit interval \\([0, 1)\\) uniformly with low discrepancy, but without any reference to a total length \\(\\tau\\). This allows for the construction of a learning rate schedule that is fundamentally decoupled from the training budget. While the resulting schedule is non-monotonic, a sorted version can be precomputed and used as a lookup table. This innovation offers greater flexibility for training scenarios where the total number of iterations is unknown or variable. ",
    "url": "/summary#analysis-of-canonical-and-alternative-window-functions",
    
    "relUrl": "/summary#analysis-of-canonical-and-alternative-window-functions"
  },"27": {
    "doc": "Summary",
    "title": "Empirical Validation Across Diverse Architectures and Datasets",
    "content": "The paper‚Äôs theoretical framework is subjected to rigorous empirical testing to validate its core claims: the interchangeability of schedules with favorable dynamics and the performance degradation caused by violating those dynamics. Experimental Design and Methodology . The experimental design is comprehensive, aiming to establish the generalizability of the findings across different tasks, model architectures, and scales. | Models and Datasets: The tests include a 30M and 124M parameter GPT-2 model for language modeling on the character-level Shakespeare and WikiText datasets, and a ResNet-18 model for image classification on CIFAR-10. | Schedules Under Test: The experiments compare two standard baselines (\\(\\digamma_C(t)\\): raised-cosine; \\(\\digamma_L(t)\\): linear decay) against a suite of the proposed alternatives. This suite is strategically divided into ‚Äúcompliant‚Äù schedules that satisfy the three-stage conditions (e.g., \\(\\digamma_{P2}(t)\\), \\(\\digamma_{S4}(t)\\), \\(\\digamma_{E0.99}(t)\\)) and violating schedules designed as negative controls (\\(\\digamma_{E0.05}(t)\\), which violates the early-stage condition, and \\(\\digamma_{S40}(t)\\), which violates the mid-late stage condition). | Training Configuration: All experiments were conducted on a single GPU, with results averaged over up to 10 independent runs to ensure statistical robustness. Standard optimization practices like momentum were used consistently across all runs. This diverse and well-controlled setup provides a strong foundation for evaluating the practical impact of the proposed framework. | . Performance Analysis: When First-Order Dynamics Align . The central experimental result of the paper is the remarkable consistency in performance among all schedules that exhibit favorable first-order dynamics. | Quantitative Results: The data presented in Tables 1 and 2 show that across all model-dataset combinations, the final test loss (for GPT-2) or test error rate (for ResNet-18) achieved by the compliant alternative schedules is statistically indistinguishable from, or in some cases slightly better than, the performance of the standard cosine and linear baselines. For example, on CIFAR-10, the linear decay baseline (\\(\\digamma_L(t)\\)) achieved a test error of \\(0.0403 \\pm 0.0001\\), while the compliant alternatives \\(\\digamma_{S4}(t)\\) and \\(\\digamma_{E0.99}(t)\\) achieved nearly identical errors of \\(0.0405 \\pm 0.0007\\) and \\(0.0402 \\pm 0.0007\\), respectively. | Qualitative Results: Figures 5 and 6 provide a powerful visual confirmation of this finding. The convergence curves for all compliant schedules (both baselines and alternatives) are tightly clustered, especially in the critical late stages of training. They follow nearly identical trajectories and converge to the same performance plateau. | . This body of evidence strongly supports the paper‚Äôs core thesis of interchangeability. It demonstrates that the specific mathematical identity of a schedule is secondary; what truly governs performance is its adherence to the underlying principles of controlled first-order dynamics captured by the \\(\\overline{\\gamma}(t)\\) profile. The Cost of Violation: Quantifying Performance Degradation . The experiments including the violating schedules, \\(\\digamma_{E0.05}(t)\\) and $$\\digamma_{S40}(t), serve as a critical control group. By demonstrating that a deliberate violation of the framework‚Äôs principles leads to a predictable and consistent degradation in performance, the authors establish a strong causal link between favorable dynamics and successful training. The results are unambiguous. In every experiment, these two schedules performed significantly worse than their compliant counterparts. | On the Shakespeare dataset, the baseline cosine schedule achieved a test loss of \\(0.164\\), while the violating schedules \\(\\digamma_{E0.05}(t)\\) and \\(\\digamma_{S40}(t)\\) plateaued at much higher losses of \\(0.178\\) and \\(0.262\\), respectively. | Similarly, on CIFAR-10, the baseline test error was around \\(4.0-4.1\\%\\), whereas the violating schedules yielded higher error rates of \\(4.3\\%\\) (\\(\\digamma_{E0.05}(t)\\)) and \\(4.17\\%\\) (\\(\\digamma_{S40}(t)\\)). | The convergence plots in Figures 5 and 6 visually depict this gap, showing the curves for the violating schedules leveling off at a visibly higher loss/error than the tightly clustered group of compliant schedules. | . This consistent underperformance of schedules with malformed \\(\\overline{\\gamma}(t)\\) profiles provides compelling evidence that the three-stage constraints are not merely correlational but are indeed predictive of a schedule‚Äôs effectiveness. | Experiment | Baseline Performance (Mean ¬± SE) | Compliant Alternatives‚Äô Performance Range | Violating \\(F_{E0.05}(t)\\) Performance | Violating \\(F_{S40}(t)\\) Performance | . | GPT‚Äë2 30M Shakespeare (Test Loss) | 0.159¬†‚Äì¬†0.164 | 0.159¬†‚Äì¬†0.162 | 0.178¬†¬±¬†0.002 | 0.262¬†¬±¬†0.004 | . | GPT‚Äë2 124M WikiText (Test Loss) | 2.744¬†‚Äì¬†2.901 | 2.746¬†‚Äì¬†2.903 | 2.763¬†¬±¬†0.011 | 3.022¬†¬±¬†0.003 | . | ResNet‚Äë18 CIFAR‚Äë10 (Test Error) | 0.0403¬†‚Äì¬†0.0410 | 0.0395¬†‚Äì¬†0.0409 | 0.0430¬†¬±¬†0.0010 | 0.0417¬†¬±¬†0.0009 | . | GPT‚Äë2 124M Shakespeare Full‚ÄëPeriod (Test Loss) | 0.1017¬†‚Äì¬†0.1028 | 0.1021¬†‚Äì¬†0.1029 | N/A | N/A | . Generalizability and Flexibility . The appendices extend the experimental validation to demonstrate the framework‚Äôs robustness and flexibility. Experiments are conducted with a larger 124M parameter GPT-2 model and with different schedule configurations, including full-period (warmup + cooldown) and variable coverage (a constant-rate phase followed by decay) schedules. The results, detailed in Appendix E and summarized in Figures 10-13, show that the core findings remain consistent. Compliant schedules continue to perform interchangeably and outperform violating ones, even at a larger model scale and with more complex configurations. Notably, the full-period schedules are shown to achieve final performance comparable to their half-period (cooldown-only) counterparts, albeit with higher initial loss due to the warmup phase. This demonstrates a key aspect of the framework‚Äôs utility: it unifies not just different decay shapes but also different training paradigms. Concepts like ‚Äúwarmup‚Äù and ‚Äúconstant-plus-cooldown‚Äù can be understood as applications of the same underlying windowing principle, simply applied over different sub-intervals of the training process. This is shown theoretically in Appendix D, where simple transformations on the input sequence \\(x(t)\\) are used to generate these more complex schedule shapes from a base window. The following table synthesizes the final performance across key experiments, starkly illustrating the consistent performance gap between compliant and violating schedules. ",
    "url": "/summary#empirical-validation-across-diverse-architectures-and-datasets",
    
    "relUrl": "/summary#empirical-validation-across-diverse-architectures-and-datasets"
  },"28": {
    "doc": "Summary",
    "title": "Theoretical Underpinnings and Broader Implications",
    "content": "The paper concludes by connecting its empirical framework to deeper principles in classical optimization theory and outlining the practical implications for the machine learning community. Beyond Heuristics: The Connection to Chebyshev Acceleration . Perhaps the most profound theoretical result is presented in Appendix C, which establishes a formal link between the raised-cosine schedule and the theory of accelerated optimization methods. The derivation shows that for the simplified case of optimizing a convex-quadratic function, the learning rate schedule that minimizes the worst-case error bound is derived from Chebyshev polynomials. The resulting optimal learning rate has a functional form that explicitly incorporates the raised-cosine window function. This result is significant because it elevates the cosine schedule from a well-performing heuristic to a function with deep theoretical roots in approximation theory and optimal control. It provides a principled, mathematical justification for why this particular shape is so effective, at least in this idealized setting. This finding serves as a powerful bridge between the theory of convex optimization and the practice of non-convex deep learning. While deep learning problems are non-convex, the fact that a schedule provably optimal in the convex world is also empirically superior in the non-convex world suggests that its underlying dynamics are fundamentally sound. The paper‚Äôs framework, by focusing on the first-order dynamics (\\(\\overline{\\gamma}(t)\\)) of this optimal schedule, successfully isolates its ‚Äúactive ingredient,‚Äù allowing this effective dynamic to be replicated in other, more computationally convenient functions. Practical Implications for Schedule Design and Selection . The overarching conclusion of the research is that window functions with favorable first-order dynamics can be used interchangeably, providing practitioners with newfound flexibility and a principled basis for design. This translates the paper‚Äôs theoretical and empirical findings into actionable advice. | Flexibility: Practitioners are no longer bound to a single best schedule like cosine annealing or linear decay. They can now confidently choose from a family of dynamically equivalent schedules, selecting one based on other practical criteria such as computational simplicity (e.g., polynomials) or ease of tuning. | Principled Design: The framework provides the tools to design and validate novel schedules. Instead of relying on trial and error, a designer can now engineer a function and verify its potential effectiveness by simply plotting its \\(\\overline{\\gamma}(t)\\) profile and checking for compliance with the three-stage constraints. | Informed Tuning: The analysis of shaping parameters and the concept of a safe operating area provide concrete guidance for hyperparameter tuning, making the process more systematic and less of a black art. | . Concluding Remarks . In conclusion, the paper makes a substantial contribution by developing a unified, principled framework that successfully explains, generalizes, and generates effective learning-rate schedules. Its primary strength lies in its novel synthesis of ideas from signal processing, simulated annealing, and classical optimization theory to create a powerful new lens for analyzing a critical component of deep learning training. The theoretical claims are backed by strong, consistent, and generalizable empirical evidence across multiple domains. The research effectively transforms the design of learning rate schedules from an art into a science. By identifying the controlled, time-normalized relative rate of change as the key determinant of a schedule‚Äôs success, it provides both deep theoretical insight and clear, practical guidance. As suggested by the authors, future work could extend this framework to explore adaptive mechanisms for the cost-change bound \\(\\delta\\) or investigate the role of higher-order dynamic constraints. Nevertheless, this work stands as a significant step forward in building a more rigorous and fundamental understanding of the optimization dynamics that underpin modern machine learning. ",
    "url": "/summary#theoretical-underpinnings-and-broader-implications",
    
    "relUrl": "/summary#theoretical-underpinnings-and-broader-implications"
  },"29": {
    "doc": "Summary",
    "title": "Summary",
    "content": " ",
    "url": "/summary",
    
    "relUrl": "/summary"
  }
}
