{"0": {
    "doc": "About Me",
    "title": "About Me",
    "content": "I am Oluwasegun, a PhD candidate in Artificial Intelligence and Electrical &amp; Computer Engineering at Oregon State University, where I study stochastic gradient learning. Stochastic gradient learning can be considered as one of the most important algorithms in the world, behind the success of deep learning. As an early-career researcher, I am currently studying why training deep neural networks via the stochastic gradient algorithm succeeds in practice. While classic optimization theory motivates, they do not provide exact fundamental guarantees for the deep learning setting. My current research shows that a key factor is the presence of fundamental signal processing elements found in almost all practical stochastic gradient learning variants. Long‚Äëterm vision: . My long-term goal is to establish myself as a leader in the development and formalization of the learning algorithms that drive artificial intelligence. I am deeply motivated by the challenge of uncovering and closing the gap on the fundamental dynamics that underpin successful learning algorithms, particularly those powering deep learning. Aiming to deepen our understanding of why certain algorithms succeed and how they can be improved, I seek to advance research that bridges theoretical insights with practical performance. Through collaboration and innovation, I look to develop new approaches that are both theoretically grounded and reliably effective in solving complex, real-world problems. üì¨ Let‚Äôs connect: If you‚Äôre interested in research collaboration, applied AI projects, feel free to reach out via LinkedIn or Email. ",
    "url": "/about",
    
    "relUrl": "/about"
  },"1": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "AutoSGM",
    "content": "Connecting the dots ‚Ä¶ HB, NAG, Adam. Preliminary versions of this work was presented at ICASSP 2024. Page created: Sep 11 2025 at 12:00 AM . | AutoSGM . | üåÄ The Core Update Rule | üß© Unifying PHB, NAG, and Adam | üéØ Lowpass Regularization | üìä Some Empirical Results . | 1. VIT on CIFAR10. | 2. ResNet18 on CIFAR10. | 3. GPT-2 on Shakespeare-char: | 4. GPT-2 on WikiText-103. | . | üèÅ Conclusion | . | . Momentum-based stochastic gradient methods such as Polyak‚Äôs Heavy Ball (PHB), Nesterov‚Äôs Accelerated Gradient (NAG), and Adam dominate deep learning optimization. They are often treated as separate algorithms, but in our recent work, we show they are all special cases of a single signal-processing (DSP) structure. The framework that allows us to do this is called the Automatic Stochastic Gradient Method (AutoSGM) framework. AutoSGM reframes these stochastic gradient optimizers through the lens of a first-order lowpass filter applied to the stochastic gradient, and the existence of an optimal iteration-dependent learning rate choice. The AutoSGM framework reveals: . | the first-order filtering mechanics behind what has been called momentum. | that we can derive an optimal, iteration-dependent learning rate choice that involves moment estimation. | that the smoothing effect of the first-order filter is a lowpass regularization of the loss surface. | . All algebraic operations are sample-by-sample (elementwise) unless otherwise stated. The shorthand notation \\((t,i)\\) denotes the \\(i\\)-th element of a vector at iteration \\(t\\). ",
    "url": "/asgm.html#autosgm",
    
    "relUrl": "/asgm.html#autosgm"
  },"2": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "üåÄ The Core Update Rule",
    "content": "The classic stochastic gradient method (SGM) updates parameters as: . \\[\\mathbf{w}(t+1,i) = \\mathbf{w}(t,i) - \\alpha(t,i) \\, \\mathbf{g}(t,i)\\] where: . | \\(\\mathbf{g}(t,i) = \\nabla f(\\mathbf{w}(t,i))\\) is an unbiased stochastic gradient component, | \\(\\alpha(t,i)\\) denotes the learning rate at iteration \\(t\\), determined via a selected oracle function. | . In AutoSGM, we replace the stochastic gradient with a smoothed version: . \\[\\mathbf{w}(t+1,i) = \\mathbf{w}(t,i) - \\alpha(t,i) \\, H_{\\beta,\\gamma}(\\mathbf{g}(t,i))\\] Here, \\(H_{\\beta,\\gamma}\\) is a first-order filter with transfer function: . \\[H(z) = \\eta \\, \\frac{1 - \\gamma z^{-1}}{1 - \\beta z^{-1}}, \\quad 0 \\le \\beta &lt; 1, \\ \\gamma &lt; \\beta\\] The time (iteration)-domain realization is: . \\[\\mathbf{v}(t,i) = \\beta\\,\\mathbf{v}(t-1,i) + \\eta\\,(\\mathbf{g}(t,i) - \\gamma\\,\\mathbf{g}(t-1,i))\\] See the learning dynamics of the stochastic gradient update in this framework. Also, see smoothing is not averaging for how the filter called momentum is generally not an exponential moving average, EMA. Using an extremely simple problem setup, we show in Momentum: a principled signal-processing framework that the first-order smoothing filter commonly called momentum is a principled signal processing operation‚úÖ, and in particular derive Nesterov‚Äôs Accelerated Gradient (NAG) from first principles as a point in the filter design space where we set \\(\\gamma=\\tfrac{\\beta}{1+\\beta}\\). ",
    "url": "/asgm.html#-the-core-update-rule",
    
    "relUrl": "/asgm.html#-the-core-update-rule"
  },"3": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "üß© Unifying PHB, NAG, and Adam",
    "content": "Choosing \\(\\beta, \\gamma, \\alpha(t,i)\\) appropriately, the AutoSGM framework recovers . | Algorithm | \\(\\beta\\) | \\(\\gamma\\) | \\(\\eta\\) | \\(\\alpha(t,i)\\) | . | Basic | \\(0\\) | \\(0\\) | \\(0\\) | \\(\\mu \\digamma(t)\\) | . | PHB | \\(‚úì\\) | \\(0\\) | \\(1\\) | \\(\\mu \\digamma(t)\\) | . | NAG | \\(‚úì\\) | \\({\\beta}/{(1+\\beta)}\\) | \\((1+\\beta)\\) | \\(\\mu \\digamma(t)\\) | . | Adam | \\(‚úì\\) | \\(0\\) | \\(1-\\beta\\) | \\({\\mu} \\digamma(t) \\cdot{\\mathbf{d}(t,i)}^{-1}\\) | . ",
    "url": "/asgm.html#-unifying-phb-nag-and-adam",
    
    "relUrl": "/asgm.html#-unifying-phb-nag-and-adam"
  },"4": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "üéØ Lowpass Regularization",
    "content": "Incorporating momentum is known to practically help stabilize learning dynamics and avoid shallow local minima (Haykin, 2008). In the paper, we use the impulse response of the filter to show that smoothing the gradient (also called momentum) is approximately equivalent to smoothing the loss surface: . This Lowpass regularization due to smoothing the gradient reflects the stabilized training effect of: . | reduced noise in the gradient updates, | improved convergence to flatter local minima, | . often observed. ",
    "url": "/asgm.html#-lowpass-regularization",
    
    "relUrl": "/asgm.html#-lowpass-regularization"
  },"5": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "üìä Some Empirical Results",
    "content": "Using Adam as a fixed-numerator baseline for the learning rate, we tested the AutoSGM framework using a derived iteration-dependent partial correlation numerator alternative on CIFAR-10 image classification (ViT, ResNet) and language modeling (GPT-2 on WikiText and Shakespeare): . | Tuning the filter‚Äôs zero \\(\\gamma\\) improved performance in most cases. | Iteration-dependent learning rate numerator (circled dots) outperformed fixed numerator (squared dots). | . 1. VIT on CIFAR10. | | . 2. ResNet18 on CIFAR10. | | . 3. GPT-2 on Shakespeare-char: . | | . 4. GPT-2 on WikiText-103. | | . ",
    "url": "/asgm.html#-some-empirical-results",
    
    "relUrl": "/asgm.html#-some-empirical-results"
  },"6": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "üèÅ Conclusion",
    "content": "AutoSGM offers a unified, interpretable, and tunable framework for what has traditionally been referred to as momentum-based optimization. We can operate PHB, NAG, and Adam as points in the AutoSGM parameter space. Overall AutoSGM is a foundational framework for studying stochastic gradient algorithms, enabling systematic separation of filter design, automatic learning-rate function choices and the non-unique implementations present in current methods. üí° Takeaway: If you have been switching between Adam, NAG, and PHB, you might not need to. They are all part of the same family. AutoSGM gives you the structure or map. | Haykin, S. (2008). Neural Networks and Learning Machines (3rd edition). Pearson. | . ",
    "url": "/asgm.html#-conclusion",
    
    "relUrl": "/asgm.html#-conclusion"
  },"7": {
    "doc": "AutoSGM: Unifying Momentum Methods",
    "title": "AutoSGM: Unifying Momentum Methods",
    "content": " ",
    "url": "/asgm.html",
    
    "relUrl": "/asgm.html"
  },"8": {
    "doc": "Conjugated Directions",
    "title": "Conjugate Gradient as AutoSGM",
    "content": "Solving a Linear Time-Invariant System. Page created: Sep 11 2025 at 12:00 AM . Oluwasegun Somefun. ‚ÄúConjugate Gradient as AutoSGM.‚Äù AutoSGM Framework, 2025. Please cite this page if you use information from these notes for your work, research, or anything that requires academic or formal citation. | Conjugate Gradient as AutoSGM | . We show here that the AutoSGM framework captures what has been called Conjugate Gradient Method (Hestenes &amp; Stiefel, 1952; Polyak, 1969; Shewchuk, 1994), and goes further by generalizing it. Recall, that AutoSGM reframes stochastic gradient optimizers through the lens of a first-order lowpass filter applied to a stochastic gradient, and the existence of an optimal iteration-dependent learning rate choice. These two elements are exactly found in the standard conjugate gradient method, with the filter‚Äôs zero parameter turned off. Formally, at each iteration \\(t\\), the parameter vector \\(\\mathbf{w}[t]\\) is updated via the gradient \\(\\mathbf{g}[t]\\) passed through a first-order lowpass filter . \\[\\boxed{ \\begin{aligned} \\statez{\\mathbf{q}[t]} = \\filter{\\beta}\\,\\cdot \\statez{\\mathbf{q}[t-1]} + \\input{\\mathbf{g}[t]}\\\\ \\input{\\mathbf{v}[t]} = \\statez{\\mathbf{q}[t]} - \\filter{\\gamma}\\,\\cdot \\statez{\\mathbf{q}[t-1]} \\\\ \\end{aligned} }\\] \\[\\boxed{ \\begin{aligned} \\state{\\mathbf{w}[t+1]} = \\state{\\mathbf{w}[t]} - \\gain{\\alpha[t]} \\cdot \\input{\\mathbf{v}[t]}\\\\ \\state{\\mathbf{\\varepsilon}[t+1]} = \\state{\\mathbf{\\varepsilon}[t]} - \\gain{\\alpha[t]} \\cdot \\input{\\mathbf{v}[t]} \\end{aligned} }\\] We will now assume that the filter parameters are possibly iteration-dependent, \\(\\filter{\\lvert{\\beta}[t]\\rvert} &lt; 1, \\filter{\\gamma}[t] \\ne 1\\). \\[\\boxed{ \\begin{aligned} \\statez{\\mathbf{q}[t]} = \\filter{\\beta[t-1]}\\,\\cdot \\statez{\\mathbf{q}[t-1]} + \\input{\\mathbf{g}[t]}\\\\ \\input{\\mathbf{v}[t]} = \\statez{\\mathbf{q}[t]} - \\filter{\\gamma[t-1]}\\,\\cdot \\statez{\\mathbf{q}[t-1]} \\\\ \\state{\\mathbf{w}[t+1]} = \\state{\\mathbf{w}[t]} - \\gain{\\alpha[t]} \\cdot \\input{\\mathbf{v}[t]}\\\\ \\end{aligned} }\\] . The learning algorithm is designed to minimize the objective . \\[f\\big(\\state{\\mathbf{w}[t+1]}\\big) = f\\big(\\state{\\mathbf{w}[t]}\\big) + \\state{\\mathbf{\\varepsilon}[t+1]}^\\intercal \\mathbf{R}\\,\\state{\\mathbf{\\varepsilon}[t+1]} + \\mathbf{n}[t+1]^\\intercal \\state{\\mathbf{\\varepsilon}[t+1]}\\] where \\(\\mathbf{R}\\) is an \\(n\\times n\\) symmetric positive definite (SPD) matrix, and \\(\\mathbf{n}[t] \\sim \\mathcal{N}(0, \\sigma^2), 0 \\le \\sigma^2 &lt; \\infty\\). Given \\(\\mathbf{n}[t]\\), \\(\\delta\\mathbf{n}[t] \\sim \\mathcal{N}(-\\mathbf{n}[t], \\sigma^2)\\) models the white noise process . \\[\\boxed{ \\begin{aligned} \\mathbf{n}[t+1]= \\mathbf{n}[t] + \\delta\\mathbf{n}[t] \\end{aligned}. }\\] Let \\(\\mathbf{b} = \\mathbf{R} \\state{\\mathbf{w}^\\ast}\\), then . \\[\\boxed{ \\begin{aligned} \\input{\\mathbf{g}[t]} = \\mathbf{R} \\state{\\mathbf{\\varepsilon}[t]} + \\mathbf{n}[t] = \\mathbf{R} \\state{\\mathbf{w}[t]} - \\mathbf{b} + \\mathbf{n}[t] \\end{aligned}. }\\] Therefore, by linearity, we have the gradient generating system . \\[\\boxed{ \\begin{aligned} \\input{\\mathbf{g}[t+1]} = \\mathbf{R} \\state{\\mathbf{\\varepsilon}[t+1]} = \\input{\\mathbf{g}[t]} - \\gain{\\alpha[t]}\\cdot\\mathbf{R}\\input{\\mathbf{v}[t]} + \\delta\\mathbf{n}[t] \\end{aligned}. }\\] The classic method assumes \\(\\sigma^2 = 0\\), and solves \\(\\mathbf{R} \\state{\\mathbf{w}[t]} = \\mathbf{b}\\). In exact arithmetic, the algorithm converges in at most \\(n\\) iterations for the linear time-invariant system described by matrix \\(\\mathbf{R}\\). Using the matrix \\(\\mathbf{R}\\)-orthogonality or conjugacy conditions: . \\[\\boxed{ \\begin{aligned} -\\input{\\mathbf{g}[t+1]}^\\intercal \\input{\\mathbf{v}[t]} = 0.\\\\ \\statez{\\mathbf{q}[t+1]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]} = 0. \\\\ \\input{\\mathbf{v}[t+1]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]} = 0. \\end{aligned} }\\] where \\(\\input{\\mathbf{g}[t+1]}^\\intercal \\input{\\mathbf{v}[t]} = \\state{\\mathbf{\\varepsilon}[t+1]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}\\) . After removing any extraneous scaling constants, for the simplest case \\(\\sigma^2 = 0\\), we will arive at . \\[\\boxed{ \\begin{aligned} \\gain{\\alpha[t]} = \\frac{\\input{\\mathbf{g}[t]}^\\intercal \\input{\\mathbf{v}[t]}}{\\input{\\mathbf{v}[t]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}} = \\frac{\\input{\\mathbf{g}[t]}^\\intercal \\input{\\mathbf{g}[t]}}{\\input{\\mathbf{v}[t]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}} \\end{aligned}, }\\] \\[\\boxed{ \\begin{aligned} \\filter{\\beta[t]} = \\frac{-\\input{\\mathbf{g}[t+1]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]}}{\\statez{\\mathbf{q}[t]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]}} \\end{aligned}, }\\] \\[\\boxed{ \\begin{aligned} \\filter{\\gamma[t]} = \\filter{\\beta[t]} + \\frac{\\input{\\mathbf{g}[t+1]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}}{\\statez{\\mathbf{q}[t]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}} = \\filter{\\beta[t]} - \\frac{\\input{\\mathbf{g}[t+1]}^\\intercal \\input{\\mathbf{g}[t+1]}}{\\input{\\mathbf{g}[t]}^\\intercal \\input{\\mathbf{g}[t]}} \\end{aligned}. }\\] . | The learning rate satisfies the stationarity condition of the objective function, which is an \\(\\mathbf{R}\\)-orthogonality condition between the next parameter error and the current filter‚Äôs output. | The two filter parameters are designed to enforce the \\(\\mathbf{R}\\)-orthogonality conditions of successive states and outputs of the first-order filter. | . More generally, . \\[\\boxed{ \\begin{aligned} \\gain{\\alpha[t]} = \\frac{\\big(\\input{\\mathbf{g}[t]} + \\delta\\mathbf{n}[t]\\big)^\\intercal \\input{\\mathbf{v}[t]}}{\\input{\\mathbf{v}[t]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}} \\end{aligned}, }\\] \\[\\boxed{ \\begin{aligned} \\filter{\\beta[t]} = \\frac{-\\input{\\mathbf{g}[t+1]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]}}{\\statez{\\mathbf{q}[t]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]}} \\end{aligned}, }\\] \\[\\boxed{ \\begin{aligned} \\filter{\\gamma[t]} = \\filter{\\beta[t]} - \\frac{\\input{\\mathbf{g}[t+1]}^\\intercal \\big(\\input{\\mathbf{g}[t+1]} - \\delta\\mathbf{n}[t]\\big)}{\\input{\\mathbf{g}[t]}^\\intercal \\big(\\input{\\mathbf{g}[t]} + \\delta\\mathbf{n}[t]\\big)} \\end{aligned}. }\\] . A Pseudocode for this AutoSGM following the prevalent style of presenting the conjugate gradient algorithm is: . \\[\\boxed{ \\begin{aligned} \\input{\\hbox{INITIALIZE}}\\\\ \\epsilon \\approx 0\\\\ \\state{\\mathbf{w}[0]} = \\mathbf{0}\\\\ \\input{\\mathbf{g}[0]} = \\mathbf{R} \\state{\\mathbf{w}[0]} - \\mathbf{b} + \\mathbf{n}[0]\\\\ \\statez{\\mathbf{q}[0]} = \\mathbf{0} + \\input{\\mathbf{g}[0]}\\\\ \\input{\\mathbf{v}[0]} = \\statez{\\mathbf{q}[0]} - \\mathbf{0} \\\\ t = 0\\\\ \\input{\\hbox{LOOP}}\\\\ \\gain{\\alpha[t]} = \\frac{\\input{\\mathbf{g}[t]}^\\intercal \\input{\\mathbf{v}[t]}}{\\input{\\mathbf{v}[t]}^\\intercal \\mathbf{R}\\input{\\mathbf{v}[t]}}\\\\ \\state{\\mathbf{w}[t+1]} = \\state{\\mathbf{w}[t]} - \\gain{\\alpha[t]} \\cdot \\input{\\mathbf{v}[t]}\\\\ \\\\ \\input{\\hbox{Generate Gradient}}\\\\ \\input{\\mathbf{g}[t+1]} = \\input{\\mathbf{g}[t]} - \\gain{\\alpha[t]}\\cdot\\mathbf{R}\\input{\\mathbf{v}[t]}\\\\ \\input{\\hbox{or}}\\\\ \\input{\\mathbf{g}[t+1]} = \\mathbf{R} \\state{\\mathbf{w}[t+1]} - \\mathbf{b}\\\\ \\input{\\hbox{STOP, if }} \\input{\\mathbf{g}[t+1]}^\\intercal \\input{\\mathbf{g}[t+1]} &lt; \\epsilon^2 \\\\ \\\\ \\filter{\\beta[t]} = \\frac{-\\input{\\mathbf{g}[t+1]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]}}{\\statez{\\mathbf{q}[t]}^\\intercal \\mathbf{R}\\statez{\\mathbf{q}[t]}}\\\\ \\filter{\\gamma[t]} = \\filter{\\beta[t]} - \\frac{\\input{\\mathbf{g}[t+1]}^\\intercal \\input{\\mathbf{g}[t+1]}}{\\input{\\mathbf{g}[t]}^\\intercal \\input{\\mathbf{g}[t]}}\\\\ \\\\ \\statez{\\mathbf{q}[t+1]} = \\filter{\\beta[t]}\\,\\cdot \\statez{\\mathbf{q}[t]} + \\input{\\mathbf{g}[t+1]}\\\\ \\input{\\mathbf{v}[t+1]} = \\statez{\\mathbf{q}[t+1]} - \\filter{\\gamma[t]}\\,\\cdot \\statez{\\mathbf{q}[t]} \\\\ t = t+1\\\\ \\input{\\hbox{END LOOP}} \\end{aligned} }\\] . | Hestenes, M. R., &amp; Stiefel, E. (1952). Methods of Conjugate Gradients for Solving Linear Systems. Journal of Research of the National Bureau of Standards, 49(6), 409. https://doi.org/10.6028/jres.049.044 | Polyak, B. T. (1969). The Conjugate Gradient Method in Extremal Problems. USSR Computational Mathematics and Mathematical Physics, 9(4), 94‚Äì112. https://doi.org/10.1016/0041-5553(69)90035-4 | Shewchuk, J. R. (1994). An Introduction to the Conjugate Gradient Method without the Agonizing Pain. School of Computer Science, Carnegie Mellon University. | . ",
    "url": "/asgm_cjg#conjugate-gradient-as-autosgm",
    
    "relUrl": "/asgm_cjg#conjugate-gradient-as-autosgm"
  },"9": {
    "doc": "Conjugated Directions",
    "title": "Conjugated Directions",
    "content": " ",
    "url": "/asgm_cjg",
    
    "relUrl": "/asgm_cjg"
  },"10": {
    "doc": "Smooth Learning Dynamics",
    "title": "Stochastic Gradient Learning Dynamics",
    "content": "Linear Time (Iteration) Varying System. Oluwasegun Somefun. ‚ÄúSmooth Learning Dynamics.‚Äù AutoSGM Framework, 2025. Please cite this page if you use information from these notes for your work, research, or anything that requires academic or formal citation. | Stochastic Gradient Learning Dynamics . | Weight decay | A Proximal Subproblem | Stability Conditions and Transient Behavior | . | . The AutoSGM framework exposes the exact update trajectory of each trainable parameter in a gradient-generating system like a deep neural network via the stochastic gradient algorithm under a lowpass filter (momentum) and iteration-dependent learning-rate oracle as the dynamics of a first-order linear time (iteration) varying (LTV) filter. This LTV description makes it possible to apply linear systems, control and signal‚Äëprocessing tools to reason about stability, transient response, noise attenuation and steady-state convergence tradeoffs. Formally, at each iteration \\(t\\), a single parameter \\(\\mathbf{w}[t, i]\\) update via its gradient component \\(\\mathbf{g}[t, i]\\) follows the first-order linear filter trajectory . \\[\\boxed{\\state{\\Delta\\mathbf{w}[t+1, i]} = \\filter{\\beta}\\,\\gain{r[t, i]}\\cdot\\state{\\Delta\\mathbf{w}[t,i]} + \\filter{\\eta}\\,\\gain{\\alpha[t,i]}\\cdot \\input{\\mathbf{e}[t,i]}},\\] . Weight decay . Depending on the weight-decay mechanism, the generated input trajectory for what has been called decoupled weight-decay . \\[\\input{ \\mathbf{e}[t, i]} = \\bigl(\\filter{\\gamma}\\,\\statez{\\mathbf{g}[t-1, i]} - \\statez{\\mathbf{g}[t, i]} \\bigr) + \\gainx{\\rho}\\,\\filter{\\eta^{-1}}\\bigl(\\filter{\\beta} \\,\\statex{\\tilde{\\mathbf{w}}[t-1, i]} - \\statex{\\tilde{\\mathbf{w}}[t, i]}\\bigr),\\] is exactly equivalent to the standard weight-decay . \\[\\input{\\mathbf{e}[t, i]} = \\bigl(\\filter{\\gamma}\\,\\statez{\\mathbf{g}[t-1, i]} - \\statez{\\mathbf{g}[t, i]} \\bigr) + \\gainx{\\rho^{\\prime}}\\bigl(\\filter{\\gamma} \\,\\statex{\\mathbf{w}[t-1, i]} - \\statex{\\mathbf{w}[t, i]} \\bigr),\\] when \\(\\gainx{\\rho^\\prime} = \\gainx{\\rho}\\,\\filter{\\eta^{-1}}\\), \\(\\filter{\\beta}=\\filter{\\gamma}\\), and \\(\\statex{\\tilde{\\mathbf{w}}[t, i]} = \\statex{\\mathbf{w}[t, i]}\\). Finally, by integrating the LTV filter, the actual parameter update is recovered . \\(\\boxed{\\statex{\\mathbf{w}[t+1, i]} = \\statex{\\mathbf{w}[t,i]} + \\state{\\Delta \\mathbf{w}[t+1,i]}}\\), . where . | \\(\\gain{\\alpha[t,i]} = {\\gain{\\mu}}\\,\\gain{\\digamma[t}] \\cdot \\frac{\\gain{\\mathbf{a}[t,i]}}{\\gain{\\mathbf{d}[t,i]}}\\) is an iteration-dependent learning rate oracle, essentially composed of a trust-region constant \\(0 &lt; \\gain{\\mu} &lt; 1\\), a window function \\(0 \\le \\gain{\\digamma[t]} \\le 1\\) as the learning-rate schedule, together with the oracle‚Äôs numerator and denominator functions denoted respectively as \\(\\gain{\\mathbf{a}[t,i]}\\), \\(\\gain{\\mathbf{d}[t,i]}\\). | \\(\\gain{r[t,i]} = \\gain{\\alpha[t,i]}/\\gain{\\alpha[t-1,i]}\\) is the learning rate ratio, | \\(\\filter{\\beta}\\) is the lowpass filter‚Äôs pole parameter selected for stability \\(0 \\le \\filter{\\beta} &lt; 1\\), | \\(\\filter{\\gamma}\\) is the lowpass filter‚Äôs zero parameter, selected such that \\(\\filter{\\gamma} &lt; \\filter{\\beta}\\), | \\(\\filter{\\eta}\\) is a constant selected as \\((1-\\filter{\\beta})/(1-\\filter{\\gamma})\\) such the steady-state (DC) gain of the lowpass filter is unity, | \\(\\gainx{\\rho, \\rho^\\prime} \\ge 0\\) denote small weight-decay constants, can be selected relative to \\(\\eta\\), | \\(\\statex{\\tilde{\\mathbf{w}}[t, i]} = \\gain{\\mathbf{d}[t,i]}\\,\\statex{\\mathbf{w}[t, i]}\\) is the scaled parameter via the learning-rate‚Äôs denominator. | . Under the mild assumptions of local smoothness of the loss function generating the gradient, and bounded gradient moments, the trajectory input \\(\\input{\\mathbf{e}[t, i]}\\) is bounded. A Proximal Subproblem . Any first‚Äëorder linear filter like . \\[\\boxed{\\state{\\Delta\\mathbf{w}[t+1, i]} = \\filter{\\beta}\\,\\gain{r[t, i]}\\cdot\\state{\\Delta\\mathbf{w}[t,i]} + \\filter{\\eta}\\,\\gain{\\alpha[t,i]}\\cdot \\input{\\mathbf{e}[t,i]}}\\] can be written as the solution of a proximal subproblem. This first‚Äëorder linear filter is the explicit closed‚Äëform optimal solution to a regularized weighted least squares objective: that balances the next trajectory step \\(\\Delta\\mathbf{w}[t+1,i]\\) between aligned with \\(\\mathbf{e}[t,i]\\), and being close to a weighted version of the previous trajectory step \\(\\Delta\\mathbf{w}[t,i]\\) . \\[\\Delta\\mathbf{w}[t+1,i] = \\arg\\min_{\\Delta} Q\\bigl(\\Delta\\bigr)\\] \\[Q\\bigl(\\Delta\\bigr)= \\;-\\; \\eta\\,\\Delta \\cdot \\mathbf{e}[t,i] \\;+\\; \\frac{1}{2\\,\\alpha[t,i]}\\, \\Bigl(\\Delta - \\beta\\, r[t,i]\\cdot\\Delta\\mathbf{w}[t,i]\\Bigr)^2\\] Rewriting as a proximal operator, . \\[\\Delta\\mathbf{w}[t+1,i] = \\mathbf{prox}_{\\gain{\\alpha[t,i]},\\:\\eta\\,\\Delta \\cdot\\mathbf{e}[t,i]}\\big(\\beta\\, r[t,i]\\cdot\\Delta\\mathbf{w}[t,i]\\big)\\] Despite the underlying deep learning loss being non‚Äëconvex, this per-iteration subproblem is strongly convex in \\(\\Delta\\mathbf{w}[t,i]\\) with modulus \\(\\alpha[t,i]^{-1}\\), and driven by \\(\\mathbf{e}[t,i]\\). Apart from unification, this LTV filter and its resulting convex quadratic subproblem formulation, concretely highlights . | Lowpass regularization: The smooth path of parameter changes empirically enhances the robustness and generalization of solutions. | Stability: Each parameter-change is the solution of a strongly convex quadratic subproblem. Provided the learning rate at each iteration is finite, this lowpass filter ensures bounded, well‚Äëdefined and stable parameter changes. | . This makes the design of such practical stochastic graient learning algorithms, more principled, analyzable, and unifying across several methods. ",
    "url": "/learning_dynamics#stochastic-gradient-learning-dynamics",
    
    "relUrl": "/learning_dynamics#stochastic-gradient-learning-dynamics"
  },"11": {
    "doc": "Smooth Learning Dynamics",
    "title": "Stability Conditions and Transient Behavior",
    "content": "As stated, the overall filtered SGM dynamics is \\(\\boxed{\\state{\\Delta\\mathbf{w}[t+1, i]} = \\filter{\\beta}\\,\\gain{r[t, i]}\\cdot\\state{\\Delta\\mathbf{w}[t,i]} + \\filter{\\eta}\\,\\gain{\\alpha[t,i]}\\cdot \\input{\\mathbf{e}[t,i]}}\\) . | The lowpass pole \\(\\filter{\\beta}\\) and learning-rate ratio \\(r[t,i]\\) shape both the the low-frequency properties of the filter (response to slow-changing inputs), and exponential stability margin of the system \\(\\lvert \\filter{\\beta^{-1}}\\gain{ r[t,i]} \\rvert &lt; 1\\). | . Together, \\(\\filter{\\beta}\\) and \\(\\gain{\\alpha[t,i]}\\) shape how quickly and smoothly the learning dynamics settle into steady-state. If selected properly, they ensure bounded and convergent behavior over time. For BIBO stability, and uniform exponential stability (boundedness and asymptotic behavior of the trajectory solution): the necessary and sufficient conditions are: \\(0 &lt; \\filter{\\beta} &lt; 1\\), and \\(\\sup_t \\gain{\\alpha[t,i]} &lt; \\infty\\). Ensures a bounded input \\(\\input{\\mathbf{e}[t,i]}\\) leads to a bounded trajectory output \\(\\state{\\Delta\\mathbf{w}[t+1, i]}\\). In addition, if \\(0 &lt; \\filter{\\beta} &lt; 1\\) to improve exponential stability, we need \\(\\gain{\\digamma[t]} \\to 0\\) as \\(t \\to \\infty\\), hich imply both \\(\\gain{r[t]} \\to 0\\) and \\(\\gain{\\alpha[t]} \\to 0\\). This explains why learning-rate annealing is important. ",
    "url": "/learning_dynamics#stability-conditions-and-transient-behavior",
    
    "relUrl": "/learning_dynamics#stability-conditions-and-transient-behavior"
  },"12": {
    "doc": "Smooth Learning Dynamics",
    "title": "Smooth Learning Dynamics",
    "content": " ",
    "url": "/learning_dynamics",
    
    "relUrl": "/learning_dynamics"
  },"13": {
    "doc": "Trust-region Optimal Learning rates",
    "title": "Optimal Learning Rate Functions",
    "content": "Minimizing Trust-region Subproblems. Page created: Sep 11 2025 at 12:00 AM . Oluwasegun Somefun. ‚ÄúAutoSGM: Trust-region Optimal Learning rates.‚Äù AutoSGM Framework, 2025. Please cite this page if you use information from these notes for your work, research, or anything that requires academic or formal citation. | Optimal Learning Rate Functions . | Notes | Optimal Iteration-Dependent Learning Rate Oracles . | Per-coordinate Setup | Matrix Setup | with Smoothed Gradients . | Per-coordinate Setup | Matrix Setup | Matrix Inverse Square-root (Realizations) . | Full Eigenvalue Decomposition (EVD) | Newton-Schulz Iteration | . | . | . | . | . AutoSGM reframes stochastic gradient algorithms used in deep learning optimization through the lens of a first-order lowpass filter applied to a stochastic gradient, and the existence of an optimal iteration-dependent learning rate choice. We show here that the automatic learning rate perspective in the AutoSGM framework captures several stochastic gradient learning variants that have been viewed as preconditioning methods. ",
    "url": "/asgm_trolrs#optimal-learning-rate-functions",
    
    "relUrl": "/asgm_trolrs#optimal-learning-rate-functions"
  },"14": {
    "doc": "Trust-region Optimal Learning rates",
    "title": "Notes",
    "content": "We assume both the objective function \\(f\\) and its gradient are Lipschitz continuous (Bottou et al., 2018). For mathematical convenience, to derive optimal learning rates, we restrict the objective function to a log-likelihood objective function \\(f=\\ln p(\\mathbf{w})\\). Let \\(\\mathbb{E}\\) denote expectation with respect to a model distribution \\(p(\\mathbf{w})\\). The expected gradient of this function is zero w.r.t \\(\\mathbf{w}\\) (Moon &amp; Stirling, 2000; Van Trees et al., 2013). In practice, many widely used training objectives admit log‚Äëlikelihood interpretations but differ from this simplified model. Define \\(\\mathbf{\\varepsilon}[t] = \\mathbf{w}[t] - {\\mathbf{w}}^\\star\\) as the parameter error, the gap between current weight and a local optimum. In practice, we can realize the expectations in the derived learning rate functions using time-averages via exponential moving averages (EMA) with long-term memory (Diniz, 2020; Haykin, 2014). ",
    "url": "/asgm_trolrs#notes",
    
    "relUrl": "/asgm_trolrs#notes"
  },"15": {
    "doc": "Trust-region Optimal Learning rates",
    "title": "Optimal Iteration-Dependent Learning Rate Oracles",
    "content": "Per-coordinate Setup . Let \\(n&gt;1\\), \\(\\alpha[t], \\mathbf{w}[t], \\mathbf{g}[t], \\Delta[t+1] \\in \\mathbb{R}^{n \\times 1}\\) . For each coordinate \\(i\\), . \\[\\boxed{ {\\mathbf{w}[t+1, i]} = {\\mathbf{w}[t,i]} + {\\Delta[t+1,i]} }\\] \\[\\boxed{ \\step }\\] Let \\(0 &lt; \\mu \\le 1\\) be a trust-region penalty constant that enforces the per-iteration subproblem . \\[\\boxed{ \\trobjsca = \\stepmom + \\mu\\,\\stepcorrng }\\] Since . \\(\\frac{d^2\\trobjsca}{d{}\\alpha[t,i]^2} = \\dengmom\\), . the subproblem defined by \\(\\trobjsca\\) is strongly convex in \\(\\alpha[t,i]\\). Let the normalized gradient be \\(\\ngrad = \\frac{\\mathbf{g}[t,i]}{\\dengmomsqrt}\\), where \\(\\numngradcorr=1\\). Minimizing \\(\\trobjsca\\) w.r.t \\(\\alpha[t,i]\\) gives . $$ \\boxed{ \\begin{align}\\label{lrmom} \\alpha[t,i] = \\mu\\,\\frac{\\numngradcorr}{\\dengmomsqrt} = \\mu\\,\\frac{1}{\\dengmomsqrt} \\end{align} } $$ Realizing this learning rate involves estimating the gradient‚Äôs moment at each iteration. More generally, . \\[\\boxed{ \\trobjsca = \\stepmom + \\mu\\,\\stepcorru }\\] So, instead of \\(\\mathbf{u}[t,i] \\triangleq \\ngrad\\) as above, let \\(\\mathbf{u}[t,i] \\triangleq \\mathbf{w}[t,i]\\) . Minimizing \\(\\trobjsca\\) w.r.t \\(\\alpha[t,i]\\) gives . $$ \\begin{align} \\label{lrparcor} \\boxed{ \\alpha[t,i] = \\mu\\,\\frac{\\numwgcorr}{\\dengmomsqrt} } \\end{align} $$ Realizing this learning rate involves estimating the gradient‚Äôs moment, and the weight-gradient partial correlation at each iteration. It turns out that minimizing the mean squared parameter error \\(\\mathbb{E}[\\mathbf{\\varepsilon}^2[t+1]]\\) yields this same iteration-dependent optimal learning rate with \\(\\mu=1\\). The derived optimal learning rates for the two trust-region subproblems are of a general iteration-dependent learning-rate oracle, in the form . \\[\\alpha[t,i] = \\mu\\,\\Phi[t,i], \\quad \\Phi[t,i] = \\frac{\\mathbf{a}[t,i]}{\\mathbf{d}[t,i]}.\\] In general, we can replace the trust-region constant \\(\\mu\\) with an iterative form \\(0 \\le \\mu[t] \\le 1\\), where \\(\\mu[t]=\\mu\\,\\digamma[t]\\), and \\(0\\le \\digamma[t] \\le 1\\). \\[\\boxed{ \\begin{aligned} \\mathbf{w}[t+1,i] = \\mathbf{w}[t,i] -\\alpha[t,i]\\,\\mathbf{g}[t,i] \\end{aligned} }\\] $$ \\begin{aligned} &amp;\\mathbf{m}[t, i] = \\dengmom\\\\ &amp;\\mathbf{d}[t, i] = {\\mathbf{m}[t]}^{\\frac{1}{2}}\\\\ &amp;\\ngrad = {\\mathbf{g}[t,i]}/{\\mathbf{d}[t, i]} \\\\ &amp;\\boxed{ \\mathbf{a}[t, i] =\\begin{cases} &amp; 1 \\\\ &amp; \\numngradcorr \\\\ &amp; \\numwgcorr \\end{cases} }\\\\ &amp;\\mathbf{w}[t+1,i] = \\mathbf{w}[t,i] - \\mu[t]\\, \\mathbf{a}[t, i] \\,\\ngrad \\end{aligned} $$ . Matrix Setup . Now, in this setup, let \\(n, m&gt;1\\) we assume \\(\\mathbf{w}[t], \\mathbf{g}[t], \\Delta[t+1] \\in \\mathbb{R}^{n \\times m}\\), while \\(\\alpha[t] \\in \\mathbb{R}^{n \\times n}\\) and symmetric. \\[\\boxed{ \\matstep }\\] \\[\\boxed{ \\trobjmat = \\gmatstepmom + \\mu\\,\\gmatstepcorrng }\\] Rewriting, . \\[\\boxed{ \\trobjmat = \\matstepmom + \\mu\\,\\matstepcorrng }\\] Since . \\(\\frac{d^2\\mkern1pt\\trobjmat}{d\\mkern1pt\\alpha[t]^2} = \\matdengmom \\ge 0\\). Let the normalized gradient be \\(\\nmatgrad = {\\matdengmomsqrt}{\\mathbf{g}[t]}\\), where \\(\\matngradcorr=\\mathbf{I}_{n\\times n}\\). Minimizing \\(\\trobjmat\\) w.r.t \\(\\alpha[t]\\) gives . $$ \\boxed{ \\alpha[t] = \\mu\\mkern1pt{\\matngradcorr}\\mkern1pt{\\matdengmomsqrt} = \\mu\\,{\\matdengmomsqrt} } $$ . Similarly, minimizing . \\[\\boxed{ \\trobjmat = \\matstepmom + \\mu\\,\\matstepcorru }\\] where, instead of \\(\\mathbf{u}[t] \\triangleq \\nmatgrad\\) as above, we have \\(\\mathbf{u}[t] \\triangleq \\mathbf{w}[t]\\) . leads to . $$ \\boxed{ \\alpha[t] = \\mu\\mkern1pt{\\matwngradcorr}\\mkern1pt{\\matdengmomsqrt} } $$ . The derived optimal learning rate matrix for the two trust-region subproblems are of a general iteration-dependent learning-rate oracle, in the form . \\[\\alpha[t] = \\mu\\,\\Phi[t], \\quad \\Phi[t] = {\\mathbf{a}[t]}\\mkern1mu{\\bar{\\mathbf{d}}[t]} \\in \\mathbb{R}^{n \\times n}.\\] . \\[\\boxed{ \\begin{aligned} \\mathbf{w}[t+1] = \\mathbf{w}[t] -\\alpha[t]\\,\\mathbf{g}[t] \\end{aligned} }\\] $$ \\begin{aligned} &amp;\\mathbf{m}[t] = \\matdengmom\\\\ &amp;\\bar{\\mathbf{d}}[t] = \\mathbf{m}[t]^{\\text{-}\\frac{1}{2}}\\\\ &amp;\\nmatgrad = \\bar{\\mathbf{d}}[t] \\mathbf{g}[t,i]\\\\ &amp;\\boxed{ \\mathbf{a}[t] = \\begin{cases} &amp; 1 \\\\ &amp; \\matngradcorr \\\\ &amp; \\matwngradcorr \\end{cases} }\\\\ &amp;\\mathbf{w}[t+1] = \\mathbf{w}[t] - \\mu[t]\\,\\mathbf{a}[t]\\,\\nmatgrad \\end{aligned} $$ . with Smoothed Gradients . Per-coordinate Setup . Replace the stochastic gradient with a smoothed version: . \\[\\boxed{ \\stepv }\\] where the smoothed gradient \\(\\mathbf{v}[t,i] = \\fof\\{\\mathbf{g}[t,i]\\}\\), and \\(\\fof\\) is a first-order filter with the transfer function: . \\[H(z) = \\eta \\, \\frac{1 - \\gamma z^{-1}}{1 - \\beta z^{-1}}, \\quad 0 \\le \\beta &lt; 1, \\ \\gamma &lt; \\beta\\] where \\(\\beta\\) and \\(\\gamma\\) are the tunable pole and zero parameters of the first-order filter. The multiplier \\(\\eta\\) is a function of \\(\\beta\\) and \\(\\gamma\\) chosen to ensure the mean value of both \\(\\mathbf{v}[t,i]\\) and \\(\\mathbf{g}[t,i]\\) are the same, \\(\\expv = \\expg.\\) Also, by the mean‚Äësquared input‚Äìoutput relation of an LTI system (Papoulis, 1977), with impulse response \\(h[k]\\), . \\[\\dengmomv \\propto \\zeta_{\\beta,\\gamma}\\, \\dengmom .\\] where \\(\\zeta_{\\beta,\\gamma}\\) is a variance reduction constant, whose maximum equals the energy of the LTI‚Äôs impulse response: \\(\\zeta_{\\beta,\\gamma} \\le \\sum_{k=0}^{\\infty} h[k]^2 = {\\eta^2}\\frac{1+\\gamma^2-2\\gamma\\beta}{(1-\\beta^2)}\\). Repeating the learning-rate derivation with \\(\\mathbf{v}[t,i]\\), . \\[\\boxed{ \\trobjsca = \\stepmom + \\mu\\,\\stepcorrngv }\\] yields . \\[\\alpha[t,i] = \\mu\\,\\frac{1}{\\sqrt{\\dengmomv}} = \\mu^{\\prime}\\,\\frac{1}{\\sqrt{\\dengmom}}.\\] The varaince reduction factor is a scaling constant, it can be removed by absorbing it into the trust‚Äëregion constant, with the rescaled trust‚Äëregion constant: . \\[\\mu^{\\prime} \\triangleq \\frac{\\mu}{\\sqrt{\\zeta}} \\subset (0, 1).\\] Therefore, the derived per‚Äëcoordinate learning rate retains the same functional form (after absorbing the variance‚Äëreduction factor into a \\(\\mu \\subset (0, 1)\\)): . $$ \\boxed{ \\alpha[t,i] = \\mu\\,\\frac{1}{\\sqrt{\\dengmom}} } $$ . \\[\\boxed{ \\begin{aligned} \\mathbf{w}[t+1,i] = \\mathbf{w}[t,i] -\\alpha[t,i]\\,\\mathbf{v}[t,i] \\end{aligned} }\\] $$ \\begin{aligned} &amp;\\mathbf{v}[t,i] = \\fof\\{\\mathbf{g}[t,i]\\}\\\\ &amp;\\mathbf{m}[t, i] = \\dengmom\\\\ &amp;\\mathbf{d}[t, i] = {\\mathbf{m}[t]}^{\\frac{1}{2}}\\\\ &amp;\\ngrad = {\\mathbf{g}[t,i]}/{\\mathbf{d}[t, i]} \\\\ &amp;\\ngradv = {\\mathbf{v}[t,i]}/{\\mathbf{d}[t, i]} \\\\ &amp;\\boxed{ \\mathbf{a}[t, i] =\\begin{cases} &amp; 1 \\\\ &amp; \\numngradvcorr \\\\ &amp; \\numwvcorr \\end{cases} }\\\\ &amp;\\mathbf{w}[t+1,i] = \\mathbf{w}[t,i] - \\mu[t]\\, \\mathbf{a}[t, i] \\,\\ngradv \\end{aligned} $$ . Matrix Setup . Now, in this setup, . \\[\\mathbf{v}[t] = \\fof\\{\\mathbf{g}[t]\\}\\] Let \\(n, m&gt;1\\) we assume \\(\\mathbf{w}[t], \\mathbf{g}[t], \\mathbf{v}[t], \\Delta[t+1] \\in \\mathbb{R}^{n \\times m}\\), while \\(\\alpha[t] \\in \\mathbb{R}^{n \\times n}\\) and symmetric. \\[\\boxed{ \\matstepv }\\] \\[\\boxed{ \\trobjmat = \\gmatstepmom + \\mu[t]\\,\\gmatstepcorrngv }\\] Since . \\(\\frac{d^2\\mkern1pt\\trobjmat}{d\\mkern1pt\\alpha[t]^2} = \\matdengmomv \\ge 0\\). Let the normalized smooth gradient be \\(\\nmatgradv = {\\matdenvmomsqrt}{\\mathbf{v}[t]}\\), where \\(\\matngradvcorr=\\mathbf{I}_{n\\times n}\\). Minimizing \\(\\trobjmat\\) w.r.t \\(\\alpha[t]\\) gives . $$ \\boxed{ \\alpha[t] = \\mu\\mkern1pt{\\matngradvcorr}\\mkern1pt{\\matdenvmomsqrt} = \\mu\\,{\\matdenvmomsqrt} } $$ Unlike the per‚Äëcoordinate case, which normalizes using a scalar variance, the matrix case normalization uses a full covariance matrix, which automatically removes all variance‚Äëscaling effects from smoothing. \\[\\boxed{ \\begin{aligned} \\mathbf{w}[t+1] = \\mathbf{w}[t] -\\alpha[t]\\,\\mathbf{v}[t] \\end{aligned} }\\] $$ \\begin{aligned} &amp;\\mathbf{v}[t] = \\fof\\{\\mathbf{g}[t]\\}\\\\ &amp;\\mathbf{m}[t] = \\matdengmomv\\\\ &amp;\\bar{\\mathbf{d}}[t] = \\mathbf{m}[t]^{\\text{-}\\frac{1}{2}}\\\\ &amp;\\nmatgradv = \\bar{\\mathbf{d}}[t] \\mathbf{v}[t,i]\\\\ &amp;\\boxed{ \\mathbf{a}[t] = \\begin{cases} &amp; 1 \\\\ &amp; \\matngradvcorr \\\\ &amp; \\matwngradvcorr \\end{cases} }\\\\ &amp;\\mathbf{w}[t+1] = \\mathbf{w}[t] - \\mu[t]\\,\\mathbf{a}[t]\\,\\nmatgradv \\end{aligned} $$ . Matrix Inverse Square-root (Realizations) . Let \\(\\mathbf{m}[t]\\) be a positive symmetric definite square matrix. There are several ways to realize its inverse square-root \\(\\bar{\\mathbf{d}}[t] = \\mathbf{m}[t]^{\\text{-}\\frac{1}{2}}\\). Typical cost is \\(O(n^{3})\\) per iteration. Full Eigenvalue Decomposition (EVD) . Compute the full eigenvalue decomposition \\(\\mathbf{m}[t] = \\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^T\\), then \\(\\bar{\\mathbf{d}}[t] = \\mathbf{U}\\mathbf{\\Lambda}^{-1/2}\\mathbf{U}^T\\). Slower for large \\(n\\), with overhead of eigenvectors, but most accurate and numerically stable. $$ \\boxed{ \\begin{aligned} &amp;\\text{1: Eigenvalue decomposition}\\\\ &amp;\\quad \\mathbf{U}\\,\\mathbf{\\Lambda}\\,\\mathbf{U}^T = \\mathbf{m}[t] \\quad\\text{with}\\quad \\mathbf{\\Lambda}=\\operatorname{diag}(\\lambda_1,\\ldots,\\lambda_n),\\ \\lambda_i&gt;0\\\\ &amp;\\text{2: Form inverse square root of eigenvalues}\\\\ &amp;\\quad \\mathbf{\\Lambda}^{-1/2} = \\operatorname{diag}(\\lambda_1^{-1/2},\\ldots,\\lambda_n^{-1/2})\\\\ &amp;\\text{3: Reconstruct inverse square root}\\\\ &amp;\\quad \\bar{\\mathbf{d}}[t] = \\mathbf{U}\\,\\mathbf{\\Lambda}^{-1/2}\\,\\mathbf{U}^T\\\\ &amp;\\bar{\\mathbf{d}}[t]\\approx \\mathbf{m}[t]^{-1/2} \\end{aligned} } $$ To save time, an approximate result can be obtained via randomized eigenvalue decomposition, which involve using randomized methods to approximate the eigenvalue decomposition by computing only the top \\(l\\) eigencomponents where \\(l \\ll n\\). Cost is now \\(O(n^2l)\\). Such low-rank approximations are cheaper than full EVD while maintaining reasonable accuracy for moderate \\(n\\). Newton-Schulz Iteration . For symmetric positive definite matrices, we may compute \\(\\bar{\\mathbf{d}}[t] = \\mathbf{m}[t]^{-1/2}\\), by maintaining two coupled polynomial sequences in matrix \\(\\mathbf{m}[t]\\) that share the same residual (Higham, 2008). Initialize with a normalization constant \\(c\\), \\(\\mathbf{y}_0 = \\mathbf{m}[t]/c\\) and \\(\\mathbf{z}_0 = \\mathbf{I}\\). \\[\\mathbf{y}_{k+1} = \\frac{1}{2}\\mathbf{y}_k\\,(3\\mathbf{I} - \\mathbf{z}_k\\mathbf{y}_k)\\] \\[\\mathbf{z}_{k+1} = \\frac{1}{2}(3\\mathbf{I} - \\mathbf{z}_k\\mathbf{y}_k)\\,\\mathbf{z}_k\\] where as \\(k\\to\\infty\\), \\(\\mathbf{y}_k \\to \\mathbf{m}[t]^{1/2}/\\sqrt{c}\\) (normalized) and \\(\\mathbf{z}_k \\to \\sqrt{c}\\,\\mathbf{m}[t]^{-1/2}\\) (normalized). Upon convergence, \\(\\bar{\\mathbf{d}}[t] \\approx \\mathbf{z}_k/\\sqrt{c}\\). Can monitor residual \\(\\mathbf{s}_k \\;=\\; \\mathbf{I} - \\mathbf{z}_k\\,\\mathbf{y}_k\\), and stop when \\(\\| \\mathbf{s}_k\\| &lt; \\epsilon\\). It is well known that optimized BLAS libraries on GPUs can reduce wall‚Äëclock time to \\(\\approx O(n^{2}l)\\), \\(l \\ll n\\). Cheap for small number of iterations and moderate \\(n\\) because matrix‚Äìmatrix multiplications on GPUs achieve very high throughput and parallelism. $$ \\boxed{ \\begin{aligned} &amp;c = \\|\\mathbf{m}[t]\\|\\\\ &amp;\\mathbf{y} = \\mathbf{m}[t]/c\\\\ &amp;\\mathbf{z} = \\mathbf{I}\\\\ &amp;\\textbf{for } k = 0, 1, 2, \\ldots K\\\\ &amp;\\quad \\mathbf{r} = 0.5\\,(3\\mathbf{I} - \\mathbf{z}_k\\,\\mathbf{y}_k)\\\\ &amp;\\quad \\mathbf{y} = \\mathbf{y}\\,\\mathbf{r}\\\\ &amp;\\quad \\mathbf{z} = \\mathbf{r}\\,\\mathbf{z}\\\\ &amp;\\textbf{end for}\\\\ &amp;\\bar{\\mathbf{d}}[t] \\approx \\mathbf{z}/\\sqrt{c} \\, \\end{aligned} } $$ . | Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. SIAM Review, 60(2), 223‚Äì311. | Moon, T. K., &amp; Stirling, W. C. (2000). Mathematical Methods and Algorithms for Signal Processing. Prentice Hall. | Van Trees, H. L., Bell, K. L., &amp; Tian, Z. (2013). Detection Estimation and Modulation Theory, Detection, Estimation, and Filtering Theory, Part I (2nd ed.). Wiley. | Diniz, P. S. R. (2020). Adaptive Filtering: Algorithms and Practical Implementation (5th edition). Springer International Publishing. https://doi.org/10.1007/978-3-030-29057-3 | Haykin, S. (2014). Adaptive Filter Theory (5th, intern.). Pearson. | Papoulis, A. (1977). Signal Analysis. McGraw-Hill College. | Higham, N. J. (2008). Functions of Matrices. Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898717778 | Honig, M. L., &amp; Messerschmitt, D. G. (1984). Adaptive Filters: Structures, Algorithms and Applications. Kluwer Academic Publishers. | . ",
    "url": "/asgm_trolrs#optimal-iteration-dependent-learning-rate-oracles",
    
    "relUrl": "/asgm_trolrs#optimal-iteration-dependent-learning-rate-oracles"
  },"16": {
    "doc": "Trust-region Optimal Learning rates",
    "title": "Trust-region Optimal Learning rates",
    "content": " ",
    "url": "/asgm_trolrs",
    
    "relUrl": "/asgm_trolrs"
  },"17": {
    "doc": "Home",
    "title": "Signal processing meets deep learning optimization",
    "content": "A notebook for my current research. From control theory and signal processing foundations to modern optimization methods for deep learning. About Me . AutoSGM View it on GitHub . PID View it on GitHub . | About Me | ",
    "url": "/#signal-processing-meets-deep-learning-optimization",
    
    "relUrl": "/#signal-processing-meets-deep-learning-optimization"
  },"18": {
    "doc": "Home",
    "title": "About Me",
    "content": ". --> ",
    "url": "/#about-me",
    
    "relUrl": "/#about-me"
  },"19": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"20": {
    "doc": "Momentum is not an EMA",
    "title": "Lowpass filtering versus Mean estimation via EMAs",
    "content": "How Momentum (Gradient smoothing) is not an EMA . Oluwasegun Somefun. ‚ÄúMomentum is not an EMA.‚Äù AutoSGM Framework, 2025. Please cite this page if you use information from these notes for your work, research, or anything that requires academic or formal citation. | Lowpass filtering versus Mean estimation via EMAs . | The Single‚ÄëPole Low‚ÄëPass Filter | EMA vs. Typical Lowpass filtering Regimes . | Frequency‚ÄëDomain Representation | Key Takeaway | . | . | . It is tempting to conflate the typical lowpass filtering operation of smoothing gradients, commonly called momentum with an exponential moving average (EMA) of gradients, since both involve recursive exponential smoothing. However, the two mechanisms are fundamentally different modes of the same first-order filter. Thinking in signal‚Äëprocessing terms makes the distinction clear. An EMA is just an operating point in the single-pole lowpass filter (\\(\\beta \\approx 1, \\gamma=0\\)) . \\[H(z) = \\eta\\,\\frac{1 - \\gamma z^{-1}}{1 - \\beta z^{-1}},\\] . ",
    "url": "/lpf_not_ema#lowpass-filtering-versus-mean-estimation-via-emas",
    
    "relUrl": "/lpf_not_ema#lowpass-filtering-versus-mean-estimation-via-emas"
  },"21": {
    "doc": "Momentum is not an EMA",
    "title": "The Single‚ÄëPole Low‚ÄëPass Filter",
    "content": "Consider a first-order filter with no zero, \\(\\gamma=0\\), only a single pole \\(\\beta\\). The filter reduces to a single‚Äëpole IIR (infinite impulse recursive) filter recursion: . \\[x_{t+1} = \\beta x_t + (1-\\beta) u_t,\\] where \\(u_t\\) is the input and \\(x_t\\) is the filtered output. | This is a causal linear filter with impulse response \\(h[t] = (1-\\beta)\\beta^t, \\quad t \\geq 0,\\) i.e. an exponentially decaying weighting of past inputs. | The update direction is therefore the cumulative contribution of past inputs, exponentially weighted by \\(\\lvert \\beta \\rvert &lt; 1\\). | . ",
    "url": "/lpf_not_ema#the-singlepole-lowpass-filter",
    
    "relUrl": "/lpf_not_ema#the-singlepole-lowpass-filter"
  },"22": {
    "doc": "Momentum is not an EMA",
    "title": "EMA vs. Typical Lowpass filtering Regimes",
    "content": ". | EMA regime: The output behaves like a time‚Äëaverage statistic only if \\(0.9 \\ll \\beta &lt; 1\\) and \\(\\gamma=0.\\) In this high \\(\\beta\\) regime (extreme smoothing), the filter has long-time memory, a larger lag, and is used to approximate statistical expectations under ergodicity of the input to the filter. This is why in practice, values like \\(0.99, 0.999, 0.9999\\) are effective. | Typical Lowpass filtering regime: In this regime, we are not interested in estimating expected values, but merely reducing the high-frequency noise content in a signal. A safe range for which the filter is an extremely poor estimator of expectation under ergodicity, is \\(0 &lt; \\beta \\le 0.9\\) (normal smoothing). It is simply returning its low‚Äëpass filtered version of the input. This is the typical momentum. Since we want smoothing, using \\(0.9\\) has long been used (before deep learning) as a good default value, when nothing is known about the frequency characteristics of the input signal. | . Frequency‚ÄëDomain Representation . The transfer function of the single pole filter is . \\[H(z) = (1-\\beta)\\,\\frac{1}{1 - \\beta z^{-1}},\\] on the unit circle \\(z = e^{j\\omega}\\). | Magnitude response: Low frequencies (\\(\\omega\\) nearer to \\(0\\)) pass with gain near 1. Both lower and higher frequencies are attenuated more strongly as \\(\\beta \\to 1\\). With high \\(\\beta\\) acts as a very narrow-band low‚Äëpass filter, and thus an EMA. | Phase response: The filter introduces a delay (phase lag) that grows with \\(\\beta\\). | . Key Takeaway . A single-pole lowpass filter‚Äôs interpretation as an EMA depends on its pole location (the value of \\(\\beta\\)): . | High \\(\\beta\\) regime: The pole of the filter is very close to the unit circle (edge of stability often called marginal stability). | The filter has long memory and effectively integrates over a large window of past inputs. | This makes the output track only the slowly‚Äëvarying mean of the input (under ergodic assumptions). | High‚Äëfrequency content (rapid changes, oscillations, transient spikes) is heavily attenuated, i.e. a lot of information is lost. | In estimation, this is the EMA regime: the output is treated as a proxy for a statistical expectation. | . | Low‚ÄìModerate \\(\\beta\\) regime: The pole is further inside the unit circle. | The filter has shorter memory and responds more directly to the current input. | The output is a smoothed version of the full input signal, not its mean estimate (under ergodic assumptions). | High‚Äëfrequency noise is reduced, but the underlying variations are still preserved. | In stochastic gradient learning, this is the momentum regime: the filter smooths and shapes the input trajectory rather than estimating a mean value. We call this lowpass regularization. | . | . Momentum is not an EMA. Conflating the two misses the point: it is the same lowpass filter operating in a different regime, with different intent lowpass smoothing vs. mean estimation. ",
    "url": "/lpf_not_ema#ema-vs-typical-lowpass-filtering-regimes",
    
    "relUrl": "/lpf_not_ema#ema-vs-typical-lowpass-filtering-regimes"
  },"23": {
    "doc": "Momentum is not an EMA",
    "title": "Momentum is not an EMA",
    "content": " ",
    "url": "/lpf_not_ema",
    
    "relUrl": "/lpf_not_ema"
  },"24": {
    "doc": "Learning-Rate Annealing",
    "title": "Why Cosine Annealing Works",
    "content": "and Why We Don‚Äôt Actually Need It. Work in Progress Page created: Oct 25 2025 at 12:00 AM . | Why Cosine Annealing Works . | Constant Setup | Iterative Setup | . | . Learning rate schedules are used to coax better training performance out of deep learning models, and play a pivotal role in training stability and generalization. But a fundamental question has lingered: Why is it so effective? Is there something magical about these schedule functions? . Constant Setup . Let \\(n&gt;1\\), \\(\\alpha[t], \\mathbf{w}[t], \\mathbf{g}[t], \\Delta[t+1] \\in \\mathbb{R}^{n \\times 1}\\) . For each coordinate \\(i\\), . \\[\\boxed{ {\\mathbf{w}[t+1, i]} = {\\mathbf{w}[t,i]} + {\\Delta[t+1,i]} }\\] \\[\\boxed{ \\step }\\] Let \\(0 &lt; \\mu \\le 1\\) be a trust-region penalty constant that enforces the per-iteration subproblem . \\[\\boxed{ \\trobjsca = \\stepmom + \\mu\\,\\stepcorrng }\\] Since . \\(\\frac{d^2\\trobjsca}{d{}\\alpha[t,i]^2} = \\dengmom\\), . the subproblem defined by \\(\\trobjsca\\) is strongly convex in \\(\\alpha[t,i]\\). Let the normalized gradient be \\(\\ngrad = \\frac{\\mathbf{g}[t,i]}{\\dengmomsqrt}\\), where \\(\\numngradcorr=1\\). Minimizing \\(\\trobjsca\\) w.r.t \\(\\alpha[t,i]\\) gives . $$ \\boxed{ \\begin{align}\\label{lrmom} \\alpha[t,i] = \\mu\\,\\frac{1}{\\dengmomsqrt} \\end{align} } $$ which is in the form . \\[\\alpha[t,i] = \\mu\\,\\Phi[t,i], \\quad \\Phi[t,i] = \\frac{1}{\\dengmomsqrt} .\\] . Iterative Setup . In general, to enforce the per-iteration subproblem, replace the trust-region constant \\(\\mu\\) with an iterative form \\(0 \\le \\mu[t] \\le 1\\), where . \\(\\mu[t]=\\mu\\,\\digamma[t], \\quad 0\\le \\digamma[t] \\le 1\\), . and \\(0 &lt; \\mu \\le 1\\) is the trust-region penalty constant. The trust-region objective becomes . \\[\\boxed{ \\trobjsca = \\stepmom + \\mu[t]\\,\\stepcorrng }\\] Minimizing \\(\\trobjsca\\) w.r.t \\(\\alpha[t,i]\\) gives . $$ \\boxed{ \\begin{align}\\label{lrmomt} \\alpha[t,i] = \\mu[t]\\,\\frac{1}{\\dengmomsqrt} \\end{align} } $$ which is in the form . \\[\\alpha[t,i] = \\mu[t]\\,\\Phi[t,i].\\] . . ",
    "url": "/asgm_lrwinds#why-cosine-annealing-works",
    
    "relUrl": "/asgm_lrwinds#why-cosine-annealing-works"
  },"25": {
    "doc": "Learning-Rate Annealing",
    "title": "Learning-Rate Annealing",
    "content": " ",
    "url": "/asgm_lrwinds",
    
    "relUrl": "/asgm_lrwinds"
  }
}
