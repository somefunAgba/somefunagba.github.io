<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li:not(:nth-child(2)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(2) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(1) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(1) > ul > li:nth-child(2) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(1) > ul.nav-list > li.nav-list-item:nth-child(2) > ul.nav-list { display: block; } </style> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Trust-region Optimal Learning rates | Research Notes</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Trust-region Optimal Learning rates" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Optimal Learning Rate Functions Minimizing Trust-region Subproblems." /> <meta property="og:description" content="Optimal Learning Rate Functions Minimizing Trust-region Subproblems." /> <link rel="canonical" href="http://localhost:4000/asgm_trolrs" /> <meta property="og:url" content="http://localhost:4000/asgm_trolrs" /> <meta property="og:site_name" content="Research Notes" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-09-11T00:00:00-07:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Trust-region Optimal Learning rates" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-11T00:00:00-07:00","datePublished":"2025-09-11T00:00:00-07:00","description":"Optimal Learning Rate Functions Minimizing Trust-region Subproblems.","headline":"Trust-region Optimal Learning rates","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/asgm_trolrs"},"url":"http://localhost:4000/asgm_trolrs"}</script> <!-- End Jekyll SEO tag --> <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']], processEscapes: true, tags: 'ams' // Enables equation numbering if you use \label{} and \ref{} }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] } }; </script> <!--Macros--> <div style="display: none"> $$ \newcommand{sca}[1]{\langle #1 \rangle} \newcommand{\scalong}[1]{(#1_1,\dots,#1_k)} \newcommand{\red}[1]{\textcolor{OrangeRed}{#1}} \newcommand{\blue}[1]{\textcolor{blue}{#1}} \newcommand{\green}[1]{\textcolor{OliveGreen}{#1}} \newcommand{\orange}[1]{\textcolor{orange}{#1}} \newcommand{\purple}[1]{\textcolor{purple}{#1}} \newcommand{\gray}[1]{\textcolor{gray}{#1}} \newcommand{\teal}[1]{\textcolor{teal}{#1}} \newcommand{\gold}[1]{\textcolor{gold}{#1}} \newcommand{\bluea}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\reda}[1]{\textcolor{Red}{#1}} \newcommand{\redb}[1]{\textcolor{RubineRed}{#1}} \newcommand{\greena}[1]{\textcolor{LimeGreen}{#1}} \newcommand{\golden}[1]{\textcolor{GoldenRod}{#1}} \newcommand{\filter}[1]{\green{#1}} \newcommand{\param}[1]{\purple{#1}} \newcommand{\state}[1]{\blue{#1}} \newcommand{\statex}[1]{\bluea{#1}} \newcommand{\stateu}[1]{\greena{#1}} \newcommand{\statez}[1]{\golden{#1}} \newcommand{\input}[1]{\gray{#1}} \newcommand{\gain}[1]{\red{#1}} \newcommand{\gainx}[1]{\reda{#1}} \newcommand{\trust}[1]{\teal{#1}} \newcommand{\schedule}[1]{\gold{#1}} $$ <!-- TROLRS --> $$ \newcommand{\trobjsca}{\mathcal{D}[t,i]} \newcommand{\trobjmat}{\mathcal{D}[t]} \newcommand{\Tr}{\mathrm{Tr}} \newcommand{\step}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{g}[t,i]} \newcommand{\stepv}{\Delta[t+1, i] = -\alpha[t,i]\,\mathbf{v}[t,i]} \newcommand{\matstep}{\Delta[t+1] = -\alpha[t]\,\mathbf{g}[t]} \newcommand{\matstepv}{\Delta[t+1] = -\alpha[t]\,\mathbf{v}[t]} \newcommand{\ngrad}{\bar{\mathbf{g}}[t,i]} \newcommand{\ngradv}{\bar{\mathbf{v}}[t,i]} \newcommand{\ngradsq}{\bar{\mathbf{g}}^2[t,i]} \newcommand{\ngradvsq}{\bar{\mathbf{v}}^2[t,i]} \newcommand{\nmatgrad}{\bar{\mathbf{g}}[t]} \newcommand{\nmatgradv}{\bar{\mathbf{v}}[t]} \newcommand{\fof}{\mathbb{H}_{\beta,\,\gamma}} \newcommand{\expg}{\mathbb{E}\big[\mathbf{g}[t,i]\big]} \newcommand{\expv}{\mathbb{E}\big[\mathbf{v}[t,i]\big]} $$ $$ \newcommand{\stepmom}{\mathbb{E}\big[{\Delta}^2[t+1, i]\big]} \newcommand{\matstepmom}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\Delta[t+1]\big]} \newcommand{\gmatstepmom}{\mathbb{E}\big[\Tr\big(\left<{\Delta[t+1],\Delta[t+1]}\right>\big)\big]} $$ $$ \newcommand{\stepcorrng}{\mathbb{E}\big[\Delta[t+1, i]\,\ngrad\big]} \newcommand{\stepcorrngv}{\mathbb{E}\big[\Delta[t+1, i]\,\ngradv\big]} \newcommand{\matstepcorrng}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgrad \big) \big]} \newcommand{\matstepcorrngv}{\mathbb{E}\big[\Tr\big(\Delta^\intercal[t+1]\,\nmatgradv \big) \big]} \newcommand{\gmatstepcorrng}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgrad}\right\rangle \big)\big]} \newcommand{\gmatstepcorrngv}{\mathbb{E}\big[\Tr\big(\left\langle{\Delta[t+1],\nmatgradv}\right\rangle \big)\big]} $$ $$ \newcommand{\stepcorru}{\mathbb{E}\big[\Delta[t+1, i]\,\mathbf{u}[t,i]\big]} \newcommand{\matstepcorru}{\mathbb{E}\big[\Delta^\intercal[t+1]\,\mathbf{u}[t]\big]} $$ $$ \newcommand{\numngradcorr}{\mathbb{E}\big[\ngradsq\big]} \newcommand{\numwgcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngrad\big]} \newcommand{\numngradvcorr}{\mathbb{E}\big[\ngradvsq\big]} \newcommand{\numwvcorr}{\mathbb{E}\big[\mathbf{w}[t,i]\,\ngradv\big]} \newcommand{\dengmom}{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]} \newcommand{\dengmomv}{\mathbb{E}\big[\mathbf{v}^2[t,i]\big]} \newcommand{\matdengmom}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]} \newcommand{\matdengmomv}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]} \newcommand{\dengmomsqrt}{\sqrt{\mathbb{E}\big[\mathbf{g}^2[t,i]\big]}} \newcommand{\matdengmomsqrt}{\mathbb{E}\big[\mathbf{g}[t]\mathbf{g}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matdenvmomsqrt}{\mathbb{E}\big[\mathbf{v}[t]\mathbf{v}^\intercal[t]\big]^{\text{-}\frac{1}{2}}} \newcommand{\matngradcorr}{\mathbb{E}\big[\nmatgrad\nmatgrad^{\intercal}\big]} \newcommand{\matngradvcorr}{\mathbb{E}\big[\nmatgradv\nmatgradv^{\intercal}\big]} \newcommand{\matwngradcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgrad^{\intercal}\big]} \newcommand{\matwngradvcorr}{\mathbb{E}\big[\mathbf{w}[t]\nmatgradv^{\intercal}\big]} $$ </div> <!-- Load Google Fonts --> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <!-- Copied from https://docs.mathjax.org/en/latest/web/components/combined.html --> <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using MathJax --> <!-- <script type="text/javascript" defer src="/assets/js/mathjax-script-type.js"> </script> --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"><div class="site-branding"> <span class="site-title ">Research Notes</span> <span class="site-description">Signal processing, and control in learning and optimization.</span> </div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in AutoSGM: Unifying Momentum Methods category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/asgm.html" class="nav-list-link">AutoSGM: Unifying Momentum Methods</a><ul class="nav-list"><li class="nav-list-item"><a href="/learning_dynamics" class="nav-list-link">Smooth Learning Dynamics</a></li><li class="nav-list-item"><a href="/asgm_trolrs" class="nav-list-link">Trust-region Optimal Learning rates</a></li><li class="nav-list-item"><a href="/asgm_cjg" class="nav-list-link">Conjugated Directions</a></li><li class="nav-list-item"><a href="/lpf_not_ema" class="nav-list-link">Momentum is not an EMA</a></li><li class="nav-list-item"><a href="/asgm_lrwinds" class="nav-list-link">Learning-Rate Annealing</a></li></ul></li><li class="nav-list-item"><a href="/about" class="nav-list-link">About Me</a></li></ul> </nav> <footer class="site-footer"> © 2026. <a href="/about">Oluwasegun Somefun</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Research Notes" aria-label="Search Research Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/asgm.html">AutoSGM: Unifying Momentum Methods</a></li> <li class="breadcrumb-nav-list-item"><span>Trust-region Optimal Learning rates</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 class="fs-9" id="optimal-learning-rate-functions"> <a href="#optimal-learning-rate-functions" class="anchor-heading" aria-labelledby="optimal-learning-rate-functions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimal Learning Rate Functions </h1> <p class="fs-6 fw-300">Minimizing Trust-region Subproblems.</p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2">Page created: Sep 11 2025 at 12:00 AM</p> </div> <blockquote> <p><em>Oluwasegun Somefun</em>. “<a href="https://somefunagba.github.io/asgm_trolrs.html">AutoSGM: Trust-region Optimal Learning rates</a>.” <em>AutoSGM Framework</em>, 2025.</p> </blockquote> <blockquote> <p><strong>Please cite this page</strong> if you use information from these notes for your work, research, or anything that requires academic or formal citation.</p> </blockquote><hr /> <details> <summary class="text-delta"> Table of contents </summary> <ol id="markdown-toc"> <li><a href="#optimal-learning-rate-functions" id="markdown-toc-optimal-learning-rate-functions">Optimal Learning Rate Functions</a> <ol> <li><a href="#notes" id="markdown-toc-notes">Notes</a></li> <li><a href="#optimal-iteration-dependent-learning-rate-oracles" id="markdown-toc-optimal-iteration-dependent-learning-rate-oracles">Optimal Iteration-Dependent Learning Rate Oracles</a> <ol> <li><a href="#per-coordinate-setup" id="markdown-toc-per-coordinate-setup">Per-coordinate Setup</a></li> <li><a href="#matrix-setup" id="markdown-toc-matrix-setup">Matrix Setup</a></li> <li><a href="#with-smoothed-gradients" id="markdown-toc-with-smoothed-gradients">with Smoothed Gradients</a> <ol> <li><a href="#per-coordinate-setup-1" id="markdown-toc-per-coordinate-setup-1">Per-coordinate Setup</a></li> <li><a href="#matrix-setup-1" id="markdown-toc-matrix-setup-1">Matrix Setup</a></li> <li><a href="#matrix-inverse-square-root-realizations" id="markdown-toc-matrix-inverse-square-root-realizations">Matrix Inverse Square-root (Realizations)</a> <ol> <li><a href="#full-eigenvalue-decomposition-evd" id="markdown-toc-full-eigenvalue-decomposition-evd">Full Eigenvalue Decomposition (EVD)</a></li> <li><a href="#newton-schulz-iteration" id="markdown-toc-newton-schulz-iteration">Newton-Schulz Iteration</a></li> </ol> </li> </ol> </li> </ol> </li> </ol> </li> </ol> </details><hr /> <blockquote> <p>AutoSGM reframes stochastic gradient algorithms used in deep learning optimization through the lens of a <strong>first-order lowpass filter</strong> applied to a stochastic gradient, and the <strong>existence</strong> of an <strong>optimal</strong> iteration-dependent <strong>learning rate</strong> choice.</p> </blockquote> <blockquote> <p>We show here that the automatic learning rate perspective in the AutoSGM framework captures several stochastic gradient learning variants that have been viewed as preconditioning methods.</p> </blockquote><hr /> <h2 id="notes"> <a href="#notes" class="anchor-heading" aria-labelledby="notes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Notes </h2> <p>We assume both the objective function \(f\) and its gradient are Lipschitz continuous <a class="citation" href="#bottouOptimizationMethodsLargescale2018">(Bottou et al., 2018)</a>.</p> <p>For mathematical convenience, to derive optimal learning rates, we restrict the objective function to a log-likelihood objective function \(f=\ln p(\mathbf{w})\). Let \(\mathbb{E}\) denote expectation with respect to a model distribution \(p(\mathbf{w})\). The expected gradient of this function is zero w.r.t \(\mathbf{w}\) <a class="citation" href="#moonMathematicalMethodsAlgorithms2000">(Moon &amp; Stirling, 2000; Van Trees et al., 2013)</a>.</p> <blockquote> <p>In practice, many widely used training objectives admit log‑likelihood interpretations but differ from this simplified model.</p> </blockquote> <p>Define \(\mathbf{\varepsilon}[t] = \mathbf{w}[t] - {\mathbf{w}}^\star\) as the parameter error, the gap between current weight and a local optimum.</p> <p>In practice, we can realize the expectations in the derived learning rate functions using time-averages via <strong>exponential moving averages (EMA)</strong> with long-term memory <a class="citation" href="#dinizAdaptiveFilteringAlgorithms2020">(Diniz, 2020; Haykin, 2014)</a>.</p><hr /> <h2 id="optimal-iteration-dependent-learning-rate-oracles"> <a href="#optimal-iteration-dependent-learning-rate-oracles" class="anchor-heading" aria-labelledby="optimal-iteration-dependent-learning-rate-oracles"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimal Iteration-Dependent Learning Rate Oracles </h2> <h4 id="per-coordinate-setup"> <a href="#per-coordinate-setup" class="anchor-heading" aria-labelledby="per-coordinate-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Per-coordinate Setup </h4> <p>Let \(n&gt;1\), \(\alpha[t], \mathbf{w}[t], \mathbf{g}[t], \Delta[t+1] \in \mathbb{R}^{n \times 1}\)</p> <p>For each coordinate \(i\),</p> <div class="text-center">\[\boxed{ {\mathbf{w}[t+1, i]} = {\mathbf{w}[t,i]} + {\Delta[t+1,i]} }\] </div> \[\boxed{ \step }\] <p>Let \(0 &lt; \mu \le 1\) be a trust-region penalty constant that enforces the per-iteration subproblem</p> \[\boxed{ \trobjsca = \stepmom + \mu\,\stepcorrng }\] <p>Since</p> <p class="text-center">\(\frac{d^2\trobjsca}{d{}\alpha[t,i]^2} = \dengmom\),</p> <p>the subproblem defined by \(\trobjsca\) is strongly convex in \(\alpha[t,i]\). Let the normalized gradient be \(\ngrad = \frac{\mathbf{g}[t,i]}{\dengmomsqrt}\), where \(\numngradcorr=1\).</p> <p>Minimizing \(\trobjsca\) w.r.t \(\alpha[t,i]\) gives</p> <div class="rbox"> $$ \boxed{ \begin{align}\label{lrmom} \alpha[t,i] = \mu\,\frac{\numngradcorr}{\dengmomsqrt} = \mu\,\frac{1}{\dengmomsqrt} \end{align} } $$ </div> <p>Realizing this learning rate involves estimating the gradient’s moment at each iteration.</p><hr /> <p>More generally,</p> \[\boxed{ \trobjsca = \stepmom + \mu\,\stepcorru }\] <p>So, instead of \(\mathbf{u}[t,i] \triangleq \ngrad\) as above, let \(\mathbf{u}[t,i] \triangleq \mathbf{w}[t,i]\)</p> <p>Minimizing \(\trobjsca\) w.r.t \(\alpha[t,i]\) gives</p> <div class="rbox"> $$ \begin{align} \label{lrparcor} \boxed{ \alpha[t,i] = \mu\,\frac{\numwgcorr}{\dengmomsqrt} } \end{align} $$ </div> <p>Realizing this learning rate involves estimating the gradient’s moment, and the weight-gradient partial correlation at each iteration. It turns out that minimizing the mean squared parameter error \(\mathbb{E}[\mathbf{\varepsilon}^2[t+1]]\) yields this same iteration-dependent optimal learning rate with \(\mu=1\).</p><hr /> <p>The derived optimal learning rates for the two trust-region subproblems are of a general iteration-dependent learning-rate oracle, in the form</p> \[\alpha[t,i] = \mu\,\Phi[t,i], \quad \Phi[t,i] = \frac{\mathbf{a}[t,i]}{\mathbf{d}[t,i]}.\] <p>In general, we can replace the trust-region constant \(\mu\) with an iterative form \(0 \le \mu[t] \le 1\), where \(\mu[t]=\mu\,\digamma[t]\), and \(0\le \digamma[t] \le 1\).</p><hr /> <div class="text-center">\[\boxed{ \begin{aligned} \mathbf{w}[t+1,i] = \mathbf{w}[t,i] -\alpha[t,i]\,\mathbf{g}[t,i] \end{aligned} }\] </div> <div class="bbox text-center"> $$ \begin{aligned} &amp;\mathbf{m}[t, i] = \dengmom\\ &amp;\mathbf{d}[t, i] = {\mathbf{m}[t]}^{\frac{1}{2}}\\ &amp;\ngrad = {\mathbf{g}[t,i]}/{\mathbf{d}[t, i]} \\ &amp;\boxed{ \mathbf{a}[t, i] =\begin{cases} &amp; 1 \\ &amp; \numngradcorr \\ &amp; \numwgcorr \end{cases} }\\ &amp;\mathbf{w}[t+1,i] = \mathbf{w}[t,i] - \mu[t]\, \mathbf{a}[t, i] \,\ngrad \end{aligned} $$ </div><hr /> <h4 id="matrix-setup"> <a href="#matrix-setup" class="anchor-heading" aria-labelledby="matrix-setup"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Matrix Setup </h4> <p>Now, in this setup, let \(n, m&gt;1\) we assume \(\mathbf{w}[t], \mathbf{g}[t], \Delta[t+1] \in \mathbb{R}^{n \times m}\), while \(\alpha[t] \in \mathbb{R}^{n \times n}\) and symmetric.</p> \[\boxed{ \matstep }\] \[\boxed{ \trobjmat = \gmatstepmom + \mu\,\gmatstepcorrng }\] <p>Rewriting,</p> \[\boxed{ \trobjmat = \matstepmom + \mu\,\matstepcorrng }\] <p>Since</p> <p class="text-center">\(\frac{d^2\mkern1pt\trobjmat}{d\mkern1pt\alpha[t]^2} = \matdengmom \ge 0\).</p> <p>Let the normalized gradient be \(\nmatgrad = {\matdengmomsqrt}{\mathbf{g}[t]}\), where \(\matngradcorr=\mathbf{I}_{n\times n}\).</p> <p>Minimizing \(\trobjmat\) w.r.t \(\alpha[t]\) gives</p> <div class="rbox"> $$ \boxed{ \alpha[t] = \mu\mkern1pt{\matngradcorr}\mkern1pt{\matdengmomsqrt} = \mu\,{\matdengmomsqrt} } $$ </div><hr /> <p>Similarly, minimizing</p> \[\boxed{ \trobjmat = \matstepmom + \mu\,\matstepcorru }\] <p>where, instead of \(\mathbf{u}[t] \triangleq \nmatgrad\) as above, we have \(\mathbf{u}[t] \triangleq \mathbf{w}[t]\)</p> <p>leads to</p> <div class="rbox"> $$ \boxed{ \alpha[t] = \mu\mkern1pt{\matwngradcorr}\mkern1pt{\matdengmomsqrt} } $$ </div><hr /> <p>The derived optimal learning rate matrix for the two trust-region subproblems are of a general iteration-dependent learning-rate oracle, in the form</p> \[\alpha[t] = \mu\,\Phi[t], \quad \Phi[t] = {\mathbf{a}[t]}\mkern1mu{\bar{\mathbf{d}}[t]} \in \mathbb{R}^{n \times n}.\]<hr /> <div class="text-center">\[\boxed{ \begin{aligned} \mathbf{w}[t+1] = \mathbf{w}[t] -\alpha[t]\,\mathbf{g}[t] \end{aligned} }\] </div> <div class="bbox text-center"> $$ \begin{aligned} &amp;\mathbf{m}[t] = \matdengmom\\ &amp;\bar{\mathbf{d}}[t] = \mathbf{m}[t]^{\text{-}\frac{1}{2}}\\ &amp;\nmatgrad = \bar{\mathbf{d}}[t] \mathbf{g}[t,i]\\ &amp;\boxed{ \mathbf{a}[t] = \begin{cases} &amp; 1 \\ &amp; \matngradcorr \\ &amp; \matwngradcorr \end{cases} }\\ &amp;\mathbf{w}[t+1] = \mathbf{w}[t] - \mu[t]\,\mathbf{a}[t]\,\nmatgrad \end{aligned} $$ </div><hr /> <h3 id="with-smoothed-gradients"> <a href="#with-smoothed-gradients" class="anchor-heading" aria-labelledby="with-smoothed-gradients"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> with Smoothed Gradients </h3> <h4 id="per-coordinate-setup-1"> <a href="#per-coordinate-setup-1" class="anchor-heading" aria-labelledby="per-coordinate-setup-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Per-coordinate Setup </h4> <p>Replace the stochastic gradient with a <strong>smoothed</strong> version:</p> \[\boxed{ \stepv }\] <p>where the smoothed gradient \(\mathbf{v}[t,i] = \fof\{\mathbf{g}[t,i]\}\), and \(\fof\) is a <strong>first-order filter</strong> with the transfer function:</p> \[H(z) = \eta \, \frac{1 - \gamma z^{-1}}{1 - \beta z^{-1}}, \quad 0 \le \beta &lt; 1, \ \gamma &lt; \beta\] <p>where \(\beta\) and \(\gamma\) are the tunable pole and zero parameters of the first-order filter. The multiplier \(\eta\) is a function of \(\beta\) and \(\gamma\) chosen to ensure the mean value of both \(\mathbf{v}[t,i]\) and \(\mathbf{g}[t,i]\) are the same, \(\expv = \expg.\) Also, by the mean‑squared input–output relation of an LTI system <a class="citation" href="#papoulisSignalAnalysis1977">(Papoulis, 1977)</a>, with impulse response \(h[k]\),</p> \[\dengmomv \propto \zeta_{\beta,\gamma}\, \dengmom .\] <p>where \(\zeta_{\beta,\gamma}\) is a variance reduction constant, whose maximum equals the energy of the LTI’s impulse response: \(\zeta_{\beta,\gamma} \le \sum_{k=0}^{\infty} h[k]^2 = {\eta^2}\frac{1+\gamma^2-2\gamma\beta}{(1-\beta^2)}\).</p> <p>Repeating the learning-rate derivation with \(\mathbf{v}[t,i]\),</p> \[\boxed{ \trobjsca = \stepmom + \mu\,\stepcorrngv }\] <p>yields</p> \[\alpha[t,i] = \mu\,\frac{1}{\sqrt{\dengmomv}} = \mu^{\prime}\,\frac{1}{\sqrt{\dengmom}}.\] <p>The varaince reduction factor is a scaling constant, it can be removed by absorbing it into the trust‑region constant, with the rescaled trust‑region constant:</p> \[\mu^{\prime} \triangleq \frac{\mu}{\sqrt{\zeta}} \subset (0, 1).\] <p>Therefore, the derived per‑coordinate learning rate retains the same functional form (after absorbing the variance‑reduction factor into a \(\mu \subset (0, 1)\)):</p> <div class="rbox"> $$ \boxed{ \alpha[t,i] = \mu\,\frac{1}{\sqrt{\dengmom}} } $$ </div><hr /> <div class="text-center">\[\boxed{ \begin{aligned} \mathbf{w}[t+1,i] = \mathbf{w}[t,i] -\alpha[t,i]\,\mathbf{v}[t,i] \end{aligned} }\] </div> <div class="bbox text-center"> $$ \begin{aligned} &amp;\mathbf{v}[t,i] = \fof\{\mathbf{g}[t,i]\}\\ &amp;\mathbf{m}[t, i] = \dengmom\\ &amp;\mathbf{d}[t, i] = {\mathbf{m}[t]}^{\frac{1}{2}}\\ &amp;\ngrad = {\mathbf{g}[t,i]}/{\mathbf{d}[t, i]} \\ &amp;\ngradv = {\mathbf{v}[t,i]}/{\mathbf{d}[t, i]} \\ &amp;\boxed{ \mathbf{a}[t, i] =\begin{cases} &amp; 1 \\ &amp; \numngradvcorr \\ &amp; \numwvcorr \end{cases} }\\ &amp;\mathbf{w}[t+1,i] = \mathbf{w}[t,i] - \mu[t]\, \mathbf{a}[t, i] \,\ngradv \end{aligned} $$ </div><hr /> <h4 id="matrix-setup-1"> <a href="#matrix-setup-1" class="anchor-heading" aria-labelledby="matrix-setup-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Matrix Setup </h4> <p>Now, in this setup,</p> \[\mathbf{v}[t] = \fof\{\mathbf{g}[t]\}\] <p>Let \(n, m&gt;1\) we assume \(\mathbf{w}[t], \mathbf{g}[t], \mathbf{v}[t], \Delta[t+1] \in \mathbb{R}^{n \times m}\), while \(\alpha[t] \in \mathbb{R}^{n \times n}\) and symmetric.</p> \[\boxed{ \matstepv }\] \[\boxed{ \trobjmat = \gmatstepmom + \mu[t]\,\gmatstepcorrngv }\] <p>Since</p> <p class="text-center">\(\frac{d^2\mkern1pt\trobjmat}{d\mkern1pt\alpha[t]^2} = \matdengmomv \ge 0\).</p> <p>Let the normalized smooth gradient be \(\nmatgradv = {\matdenvmomsqrt}{\mathbf{v}[t]}\), where \(\matngradvcorr=\mathbf{I}_{n\times n}\).</p> <p>Minimizing \(\trobjmat\) w.r.t \(\alpha[t]\) gives</p> <div class="rbox"> $$ \boxed{ \alpha[t] = \mu\mkern1pt{\matngradvcorr}\mkern1pt{\matdenvmomsqrt} = \mu\,{\matdenvmomsqrt} } $$ </div> <p>Unlike the per‑coordinate case, which normalizes using a scalar variance, the matrix case normalization uses a full covariance matrix, which automatically removes all variance‑scaling effects from smoothing.</p><hr /> <div class="text-center">\[\boxed{ \begin{aligned} \mathbf{w}[t+1] = \mathbf{w}[t] -\alpha[t]\,\mathbf{v}[t] \end{aligned} }\] </div> <div class="bbox text-center"> $$ \begin{aligned} &amp;\mathbf{v}[t] = \fof\{\mathbf{g}[t]\}\\ &amp;\mathbf{m}[t] = \matdengmomv\\ &amp;\bar{\mathbf{d}}[t] = \mathbf{m}[t]^{\text{-}\frac{1}{2}}\\ &amp;\nmatgradv = \bar{\mathbf{d}}[t] \mathbf{v}[t,i]\\ &amp;\boxed{ \mathbf{a}[t] = \begin{cases} &amp; 1 \\ &amp; \matngradvcorr \\ &amp; \matwngradvcorr \end{cases} }\\ &amp;\mathbf{w}[t+1] = \mathbf{w}[t] - \mu[t]\,\mathbf{a}[t]\,\nmatgradv \end{aligned} $$ </div><hr /> <h4 id="matrix-inverse-square-root-realizations"> <a href="#matrix-inverse-square-root-realizations" class="anchor-heading" aria-labelledby="matrix-inverse-square-root-realizations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Matrix Inverse Square-root (Realizations) </h4> <p>Let \(\mathbf{m}[t]\) be a positive symmetric definite square matrix.<br /> There are several ways to realize its inverse square-root \(\bar{\mathbf{d}}[t] = \mathbf{m}[t]^{\text{-}\frac{1}{2}}\). Typical cost is \(O(n^{3})\) per iteration.</p> <h5 id="full-eigenvalue-decomposition-evd"> <a href="#full-eigenvalue-decomposition-evd" class="anchor-heading" aria-labelledby="full-eigenvalue-decomposition-evd"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Full Eigenvalue Decomposition (EVD) </h5> <p>Compute the full eigenvalue decomposition \(\mathbf{m}[t] = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^T\), then \(\bar{\mathbf{d}}[t] = \mathbf{U}\mathbf{\Lambda}^{-1/2}\mathbf{U}^T\).</p> <p>Slower for large \(n\), with overhead of eigenvectors, but most accurate and numerically stable.</p> <div class="mrbox"> $$ \boxed{ \begin{aligned} &amp;\text{1: Eigenvalue decomposition}\\ &amp;\quad \mathbf{U}\,\mathbf{\Lambda}\,\mathbf{U}^T = \mathbf{m}[t] \quad\text{with}\quad \mathbf{\Lambda}=\operatorname{diag}(\lambda_1,\ldots,\lambda_n),\ \lambda_i&gt;0\\ &amp;\text{2: Form inverse square root of eigenvalues}\\ &amp;\quad \mathbf{\Lambda}^{-1/2} = \operatorname{diag}(\lambda_1^{-1/2},\ldots,\lambda_n^{-1/2})\\ &amp;\text{3: Reconstruct inverse square root}\\ &amp;\quad \bar{\mathbf{d}}[t] = \mathbf{U}\,\mathbf{\Lambda}^{-1/2}\,\mathbf{U}^T\\ &amp;\bar{\mathbf{d}}[t]\approx \mathbf{m}[t]^{-1/2} \end{aligned} } $$ </div> <p>To save time, an approximate result can be obtained via <em>randomized eigenvalue decomposition</em>, which involve using randomized methods to approximate the eigenvalue decomposition by computing only the top \(l\) eigencomponents where \(l \ll n\). Cost is now \(O(n^2l)\). Such low-rank approximations are cheaper than full EVD while maintaining reasonable accuracy for moderate \(n\).</p> <h5 id="newton-schulz-iteration"> <a href="#newton-schulz-iteration" class="anchor-heading" aria-labelledby="newton-schulz-iteration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Newton-Schulz Iteration </h5> <p>For symmetric positive definite matrices, we may compute \(\bar{\mathbf{d}}[t] = \mathbf{m}[t]^{-1/2}\), by maintaining two coupled polynomial sequences in matrix \(\mathbf{m}[t]\) that share the same residual <a class="citation" href="#highamFunctionsMatrices2008">(Higham, 2008)</a>.</p> <p>Initialize with a normalization constant \(c\), \(\mathbf{y}_0 = \mathbf{m}[t]/c\) and \(\mathbf{z}_0 = \mathbf{I}\).</p> \[\mathbf{y}_{k+1} = \frac{1}{2}\mathbf{y}_k\,(3\mathbf{I} - \mathbf{z}_k\mathbf{y}_k)\] \[\mathbf{z}_{k+1} = \frac{1}{2}(3\mathbf{I} - \mathbf{z}_k\mathbf{y}_k)\,\mathbf{z}_k\] <p>where as \(k\to\infty\), \(\mathbf{y}_k \to \mathbf{m}[t]^{1/2}/\sqrt{c}\) (normalized) and \(\mathbf{z}_k \to \sqrt{c}\,\mathbf{m}[t]^{-1/2}\) (normalized). Upon convergence, \(\bar{\mathbf{d}}[t] \approx \mathbf{z}_k/\sqrt{c}\).</p> <p>Can monitor residual \(\mathbf{s}_k \;=\; \mathbf{I} - \mathbf{z}_k\,\mathbf{y}_k\), and stop when \(\| \mathbf{s}_k\| &lt; \epsilon\). It is well known that optimized BLAS libraries on GPUs can reduce wall‑clock time to \(\approx O(n^{2}l)\), \(l \ll n\). Cheap for small number of iterations and moderate \(n\) because matrix–matrix multiplications on GPUs achieve very high throughput and parallelism.</p> <div class="mrbox text-center"> $$ \boxed{ \begin{aligned} &amp;c = \|\mathbf{m}[t]\|\\ &amp;\mathbf{y} = \mathbf{m}[t]/c\\ &amp;\mathbf{z} = \mathbf{I}\\ &amp;\textbf{for } k = 0, 1, 2, \ldots K\\ &amp;\quad \mathbf{r} = 0.5\,(3\mathbf{I} - \mathbf{z}_k\,\mathbf{y}_k)\\ &amp;\quad \mathbf{y} = \mathbf{y}\,\mathbf{r}\\ &amp;\quad \mathbf{z} = \mathbf{r}\,\mathbf{z}\\ &amp;\textbf{end for}\\ &amp;\bar{\mathbf{d}}[t] \approx \mathbf{z}/\sqrt{c} \, \end{aligned} } $$ </div><hr /> <!-- #### EMA Realizations To avoid breakdown of the factorization we shift the matrix by a small multiple of its diagonal, or the identity matrix. Track the denominator term (moment estimation): $$ \mathbf{b}(t,i) = \beta_b \,\mathbf{b}(t-1,i) + (1 - \beta_b) \,\mathbf{g}(t,i)^2, $$ and define the RMS-normalizer: $$ \mathbf{d}(t,i) = \sqrt{\frac{\mathbf{b}(t,i)}{1 - \beta_b^t}} + \epsilon $$ → bias-corrected RMS-norm with small $$ \epsilon $$ <a class="citation" href="#honigAdaptiveFiltersStructures1984">(Honig &amp; Messerschmitt, 1984)</a> to prevent division by zero. Track the numerator term: $$ \mathbf{a}(t,i) = \beta_a \,\mathbf{a}(t-1,i) + \mu \,\mathbf{w}(t,i) \,\bar{\mathbf{g}}(t,i) $$ Finally: $$ \alpha(t,i) = \digamma(t)\,\frac{\mathbf{a}(t,i)}{\mathbf{d}(t,i)}. $$ --><hr /> <ol class="bibliography"><li><span id="bottouOptimizationMethodsLargescale2018">Bottou, L., Curtis, F. E., &amp; Nocedal, J. (2018). Optimization Methods for Large-Scale Machine Learning. <i>SIAM Review</i>, <i>60</i>(2), 223–311.</span></li> <li><span id="moonMathematicalMethodsAlgorithms2000">Moon, T. K., &amp; Stirling, W. C. (2000). <i>Mathematical Methods and Algorithms for Signal Processing</i>. Prentice Hall.</span></li> <li><span id="vantreesDetectionEstimationModulation2013">Van Trees, H. L., Bell, K. L., &amp; Tian, Z. (2013). <i>Detection Estimation and Modulation Theory, Detection, Estimation, and Filtering Theory, Part I</i> (2nd ed.). Wiley.</span></li> <li><span id="dinizAdaptiveFilteringAlgorithms2020">Diniz, P. S. R. (2020). <i>Adaptive Filtering: Algorithms and Practical Implementation</i> (5th edition). Springer International Publishing. https://doi.org/10.1007/978-3-030-29057-3</span></li> <li><span id="haykinAdaptiveFilterTheory2014">Haykin, S. (2014). <i>Adaptive Filter Theory</i> (5th, intern.). Pearson.</span></li> <li><span id="papoulisSignalAnalysis1977">Papoulis, A. (1977). <i>Signal Analysis</i>. McGraw-Hill College.</span></li> <li><span id="highamFunctionsMatrices2008">Higham, N. J. (2008). <i>Functions of Matrices</i>. Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9780898717778</span></li> <li><span id="honigAdaptiveFiltersStructures1984">Honig, M. L., &amp; Messerschmitt, D. G. (1984). <i>Adaptive Filters: Structures, Algorithms and Applications</i>. Kluwer Academic Publishers.</span></li></ol><hr /> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0 mr-2"> Page last modified: <span class="d-inline-block">Dec 24 2025 at 12:00 AM</span>. </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
